{"title":"SLR: Model diagnostics","markdown":{"yaml":{"title":"SLR: Model diagnostics","subtitle":"STA 210 - Spring 2022","author":"Dr. Mine Ã‡etinkaya-Rundel","footer":"[sta210-s22.github.io/website](https://sta210-s22.github.io/website/)","logo":"images/logo.png","format":{"revealjs":{"theme":"slides.scss","multiplex":true,"transition":"fade","slide-number":true,"incremental":true,"chalkboard":true}},"editor":"visual","execute":{"freeze":"auto"}},"headingText":"| include: false","containsRefs":false,"markdown":"\n\n```{r setup}\n\nlibrary(countdown)\n\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\",\n  fig.align = \"center\"\n)\n\noptions(scipen = 100)\n```\n\n# Welcome\n\n## Computational setup\n\n```{r packages}\n#| echo: true\n#| message: false\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))\n```\n\n# Mathematical models for inference\n\n## The regression model, revisited\n\n```{r}\n#| echo: true\n\ndf_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %>%\n  kable(digits = 2)\n```\n\n## HT for the slope {.smaller}\n\n**Hypotheses:** $H_0: \\beta_1 = 0$ vs. $H_A: \\beta_1 \\ne 0$\n\n. . .\n\n**Test statistic:** Number of standard errors the estimate is away from the null\n\n$$\nT = \\frac{\\text{Estimate - Null}}{\\text{Standard error}} \\\\\n$$\n\n. . .\n\n**p-value:** Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\n$$\np-value = P(|t| > |\\text{test statistic}),\n$$\n\ncalculated from a $t$ distribution with $n - 2$ degrees of freedom\n\n## HT: Test statistic\n\n```{r}\ntidy(df_fit) %>%\n  kable(digits = 2) %>%\n  row_spec(2, background = \"#D9E3E4\")\n```\n\n$$\nt = \\frac{\\hat{\\beta}_1 - 0}{SE_{\\hat{\\beta}_1}} = \\frac{159.48 - 0}{18.17} = 8.78\n$$\n\n## HT: p-value\n\n```{r}\ntidy(df_fit) %>%\n  kable(digits = 2) %>%\n  row_spec(2, background = \"#D9E3E4\")\n```\n\n```{r}\nnormTail(L = -8.78, U = 8.78, df = nrow(duke_forest) - 2, xlim = c(-9,9), col = \"#D9E3E4\")\n```\n\n## Understanding the p-value {.smaller}\n\n| Magnitude of p-value    | Interpretation                        |\n|:------------------------|:--------------------------------------|\n| p-value \\< 0.01         | strong evidence against $H_0$         |\n| 0.01 \\< p-value \\< 0.05 | moderate evidence against $H_0$       |\n| 0.05 \\< p-value \\< 0.1  | weak evidence against $H_0$           |\n| p-value \\> 0.1          | effectively no evidence against $H_0$ |\n\n::: callout-important\nThese are general guidelines.\nThe strength of evidence depends on the context of the problem.\n:::\n\n## HT: Conclusion, in context\n\n```{r}\ntidy(df_fit) %>%\n  kable(digits = 2) %>%\n  row_spec(2, background = \"#D9E3E4\")\n```\n\n-   The data provide convincing evidence that the population slope $\\beta_1$ is different from 0.\n-   The data provide convincing evidence of a linear relationship between area and price of houses in Duke Forest.\n\n## CI for the slope\n\n$$\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n$$\n\n. . .\n\n$$\n\\hat{\\beta}_1 \\pm t^* \\times SE_{\\hat{\\beta}_1}\n$$\n\nwhere $t^*$ is calculated from a $t$ distribution with $n-2$ degrees of freedom\n\n## CI: Critical value\n\n::: columns\n::: {.column width=\"60%\"}\n```{r}\n#| echo: true\n\n# confidence level: 95%\nqt(0.975, df = nrow(duke_forest) - 2)\n\n# confidence level: 90%\nqt(0.95, df = nrow(duke_forest) - 2)\n\n# confidence level: 99%\nqt(0.995, df = nrow(duke_forest) - 2)\n```\n:::\n\n::: {.column width=\"40%\"}\n```{r}\n#| out.width: \"100%\"\n\nnormTail(M = c(-1.984984, 1.984984), df = nrow(duke_forest) - 2, col = \"#D9E3E4\")\ntext(x = 0, y = 0.04, labels = \"95%\", cex = 2, col = \"#5B888C\")\n```\n:::\n:::\n\n## 95% CI for the slope: Calculation\n\n```{r}\ntidy(df_fit) %>% \n  kable(digits = 2) %>%\n  row_spec(2, background = \"#D9E3E4\")\n```\n\n$$\\hat{\\beta}_1 = 159.48 \\hspace{15mm} t^* = 1.98 \\hspace{15mm} SE_{\\hat{\\beta}_1} = 18.17$$\n\n. . .\n\n$$\n159.48 \\pm 1.98 \\times 18.17 = (123.50, 195.46)\n$$\n\n## 95% CI for the slope: Computation {.smaller}\n\n```{r}\n#| echo: true\n\ntidy(df_fit, conf.int = TRUE, conf.level = 0.95) %>% \n  kable(digits = 2)\n```\n\n## Confidence interval for predictions {.smaller}\n\n-   Suppose we want to answer the question *\"What is the predicted sale price of a Duke Forest house that is 2,800 square feet?\"*\n-   We said reporting a single estimate for the slope is not wise, and we should report a plausible range instead\n-   Similarly, reporting a single prediction for a new value is not wise, and we should report a plausible range instead\n\n```{r}\n#| fig.width: 10\n\nx_new <- 2800\ny_hat_x_new <- predict(df_fit, new_data = tibble(area = x_new)) %>% pull()\n\nggplot(duke_forest, aes(x = area, y = price)) +\n  geom_segment(\n    x = x_new, xend = x_new, y = y_hat_x_new-600000, yend = y_hat_x_new+600000,\n    color = \"#CDDBDC\", size = 4\n  ) +\n  geom_segment(\n    x = x_new, xend = x_new, y = y_hat_x_new-400000, yend = y_hat_x_new+400000,\n    color = \"#ADC3C5\", size = 4\n  ) +\n  geom_segment(\n    x = x_new, xend = x_new, y = y_hat_x_new-200000, yend = y_hat_x_new+200000,\n    color = \"#7B9FA3\", size = 4\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#8F2D56\") +\n  geom_segment(\n    x = x_new, xend = x_new, y = 0, yend = y_hat_x_new,\n    linetype = \"dashed\", color = \"#5B888C\"\n  ) +\n  geom_segment(\n    x = 0, xend = x_new, y = y_hat_x_new, yend = y_hat_x_new,\n    linetype = \"dashed\", color = \"#5B888C\"\n  ) +\n  annotate(\"point\", x = x_new, y = y_hat_x_new, size = 2, color = \"magenta\") +\n  annotate(\"point\", x = x_new, y = y_hat_x_new, size = 5, shape = \"circle open\", color = \"#5B888C\", stroke = 2) +\n  scale_x_continuous(labels = label_number()) +\n  scale_y_continuous(labels = label_dollar(), limits = c(000000, 1500000)) +\n  labs(\n    x = \"Area (square feet)\", y = \"Sale price\",\n    title = \"Houses in Duke Forest\"\n    )\n```\n\n## Two types of predictions {.smaller}\n\n1.  Prediction for the mean: \"\"What is the average predicted sale price of Duke Forest houses that are 2,800 square feet?\"\n\n2.  Prediction for an individual observation: \"What is the predicted sale price of a Duke Forest house that is 2,800 square feet?\"\n\n. . .\n\n::: question\nWhich would you expect to be more variable?\nThe average prediction or the prediction for an individual observation?\nBased on your answer, how would you expect the widths of plausible ranges for these two predictions to compare?\n:::\n\n## Uncertainty in predictions\n\n**Confidence interval for the mean outcome:** $$\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE}_{\\hat{\\boldsymbol{\\mu}}}}$$\n\n. . .\n\n**Prediction interval for an individual observation:** $$\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE_{\\hat{y}}}}$$\n\n## Standard errors {.smaller}\n\n**Standard error of the mean outcome:** $$SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}$$\n\n. . .\n\n**Standard error of an individual outcome:** $$SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{1 + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}$$\n\n## Standard errors {.smaller}\n\n**Standard error of the mean outcome:** $$SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}$$\n\n**Standard error of an individual outcome:** $$SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\mathbf{\\color{purple}{\\Large{1}}} + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}$$\n\n## Confidence interval\n\nThe 95% **confidence interval** for the *mean* outcpme:\n\n```{r}\n#| echo: true\nnew_house <- tibble(area = 2800)\n\npredict(df_fit, new_data = new_house, type = \"conf_int\", level = 0.95)\n```\n\n```{r}\nnew_house_ci <- predict(df_fit, new_data = new_house, type = \"conf_int\", level = 0.95)\n```\n\n. . .\n\nWe are 95% confident that mean sale price of Duke Forest houses that are 2,800 square feet is between `r dollar(new_house_ci$.pred_lower)` and `r dollar(new_house_ci$.pred_upper)`.\n\n## Prediction interval\n\nThe 95% **prediction interval**for the *individual* outcome:\n\n```{r}\n#| echo: true\npredict(df_fit, new_data = new_house, type = \"pred_int\", level = 0.95)\n```\n\n```{r}\nnew_house_pi <- predict(df_fit, new_data = new_house, type = \"pred_int\", level = 0.95)\n```\n\n. . .\n\nWe are 95% confident that predicted sale price of a Duke Forest house that is 2,800 square feet is between `r dollar(new_house_pi$.pred_lower)` and `r dollar(new_house_pi$.pred_upper)`.\n\n## Comparing intervals\n\n```{r}\n#| out.width: \"100%\"\n#| fig.width: 10\n\nnew_houses <- tibble(area = seq(1000, 6500, 50))\nnew_houses_ci <- predict(df_fit, new_data = new_houses, type = \"conf_int\", level = 0.95) %>% \n  mutate(\n    area = new_houses$area,\n    type = \"Confidence interval\"\n    )\nnew_houses_pi <- predict(df_fit, new_data = new_houses, type = \"pred_int\", level = 0.95) %>% \n  mutate(\n    area = new_houses$area,\n    type = \"Prediction interval\"\n    )\nnew_houses_int <- bind_rows(new_houses_ci, new_houses_pi)\n\nggplot(duke_forest, aes(x = area, y = price)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#8F2D56\") +\n  geom_line(data = new_houses_int,\n            aes(x = area, y = .pred_lower, linetype = type, color = type),\n            size = 1) +\n  geom_line(data = new_houses_int,\n            aes(x = area, y = .pred_upper, linetype = type, color = type),\n            size = 1) +\n  scale_x_continuous(labels = label_number()) +\n  scale_y_continuous(labels = label_dollar(), limits = c(000000, 1500000)) +\n  scale_color_manual(values = c(\"#5B888C\", \"#888c5b\")) +\n  labs(\n    x = \"Area (square feet)\", y = \"Sale price\",\n    color = \"Type of interval\", linetype = \"Type of interval\",\n    title = \"Houses in Duke Forest\"\n    ) +\n  theme(\n    legend.position = c(0.2, 0.85)\n  )\n```\n\n## Extrapolation\n\n::: columns\n::: {.column width=\"45%\"}\n::: question\nCalculate the prediction interval for the sale price of a \"tiny house\" in Duke Forest that is 225 square feet.\n:::\n:::\n\n::: {.column width=\"55%\"}\n![](images/lec-7/tiny-house.jpeg){fig-alt=\"Black tiny house on wheels\" fig-align=\"center\" width=\"750\"}\n:::\n:::\n\n. . .\n\n*No, thanks!*\n\n# Model conditions\n\n## Model conditions\n\n1.  **Linearity:** There is a linear relationship between the outcome and predictor variables\n2.  **Constant variance:** The variability of the errors is equal for all values of the predictor variable, i.e. the errors are homeoscedastic\n3.  **Normality:** The errors follow a normal distribution\n4.  **Independence:** The errors are independent from each other\n\n## Linearity\n\nâœ… The residuals vs. fitted values plot should not show a random scatter of residuals (no distinguishable pattern or structure)\n\n```{r res-vs-fit}\ndf_aug <- augment(df_fit$fit)\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )\n```\n\n## Residuals vs. fitted values\n\n```{r}\n#| echo: true\n#| ref.label: \"res-vs-fit\"\n#| fig.show: \"hide\"\n```\n\n## Application exercise\n\n::: appex\nðŸ“‹ [github.com/sta210-s22/ae-3-duke-forest](https://github.com/sta210-s22/?q=ae-3-duke-forest&type=all&language=&sort=)\n:::\n\n```{r}\ncountdown(minutes = 5, font_size = \"2em\")\n```\n\n## Non-linear relationships\n\n```{r}\nset.seed(1234)\n\nn = 100\n\ndf <- tibble(\n  x = -49:50,\n  e_curved = rnorm(n, 0, 150),\n  y_curved = x^2 + e_curved,\n  e_slight_curve = sort(rbeta(n, 5, 1) * 200) + rnorm(n, 0, 5),\n  y_slight_curve = x + e_slight_curve,\n  x_fan = seq(0, 3.99, 4 / n),\n  y_fan = c(rnorm(n / 8, 3, 1), rnorm(n / 8, 3.5, 2), rnorm(n / 8, 4, 2.5), rnorm(n / 8, 4.5, 3), rnorm(n / 4, 5, 4), rnorm((n / 4) + 2, 6, 5))\n)\n```\n\n::: columns\n::: {.column width=\"50%\"}\n```{r}\n#| out.width: \"100%\"\n#| fig.asp: 1.2\n\np1 <- ggplot(df, aes(x = x, y = y_curved)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#8F2D56\") +\n  labs(\n    x = \"X\", y = \"Y\",\n    title = \"Observed data + model\"\n    )\n\ncurved_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y_curved ~ x, data = df)\n\ncurved_aug <- augment(curved_fit$fit)\n\np2 <- ggplot(curved_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  ) +\n  ylim(-2000, 2000)\n\np1 / p2 +\n  plot_annotation(title = \"Obviously curved\")\n```\n:::\n\n::: {.column width=\"50%\"}\n```{r}\n#| out.width: \"100%\"\n#| fig.asp: 1.2\n\np1 <- ggplot(df, aes(x = x, y = y_slight_curve)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#8F2D56\") +\n  labs(\n    x = \"X\", y = \"Y\",\n    title = \"Observed data + model\"\n    )\n\nslight_curve_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y_slight_curve ~ x, data = df)\n\nslight_curve_aug <- augment(slight_curve_fit$fit)\n\np2 <- ggplot(slight_curve_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )\n\np1 / p2 +\n  plot_annotation(title = \"Not so obviously curved\")\n```\n:::\n:::\n\n## Constant variance\n\nâœ… The vertical spread of the residuals should be relatively constant across the plot\n\n```{r}\n#| ref.label: \"res-vs-fit\"\n```\n\n## Non-constant variance\n\n::: columns\n::: {.column width=\"50%\"}\n```{r}\n#| out.width: \"100%\"\n\nggplot(df, aes(x = x_fan, y = y_fan)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#8F2D56\") +\n  labs(\n    x = \"X\", y = \"Y\",\n    title = \"Observed data + model\"\n    )\n```\n:::\n\n::: {.column width=\"50%\"}\n```{r}\n#| out.width: \"100%\"\n\nfan_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y_fan ~ x_fan, data = df)\n\nfan_aug <- augment(fan_fit$fit)\n\nggplot(fan_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  ) +\n  ylim(-15, 15)\n```\n:::\n:::\n\n## Normality\n\n```{r}\nggplot(df_aug, aes(x = .resid)) +\n  geom_histogram(aes(y = ..density..), binwidth = 100000, color = \"white\") +\n  geom_function(\n    #geom = \"line\",\n    fun = dnorm,\n    args = list(\n      mean = mean(df_aug$.resid), \n      sd = sd(df_aug$.resid)\n      ),\n    lwd = 2,\n    col = \"#8F2D5690\"\n  ) +\n  labs(\n    x = \"Residual\",\n    y = \"Density\",\n    title = \"Histogram of residuals\"\n  )\n```\n\n## Independence\n\n-   We can often check the independence assumption based on the context of the data and how the observations were collected\n\n-   If the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected\n\n. . .\n\nâœ… If this is a random sample of Duke Houses, the error for one house does not tell us anything about the error for another use\n\n## Recap\n\nUsed residual plots to check conditions for SLR:\n\n::: columns\n::: {.column width=\"50%\"}\n::: nonincremental\n-   Linearity\n-   Constant variance\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: nonincremental\n-   Normality\n-   Independence\n:::\n:::\n:::\n\n. . .\n\n::: question\nWhich of these conditions are required for fitting a SLR?\nWhich for simulation-based inference for the slope for an SLR?\nWhich for inference with mathematical models?\n:::\n\n```{r}\ncountdown(minutes = 3, font_size = \"2em\")\n```\n"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":true,"output-file":"lec-7.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"0.9.345","auto-stretch":true,"editor":"visual","title":"SLR: Model diagnostics","subtitle":"STA 210 - Spring 2022","author":"Dr. Mine Ã‡etinkaya-Rundel","footer":"[sta210-s22.github.io/website](https://sta210-s22.github.io/website/)","logo":"images/logo.png","theme":"slides.scss","multiplex":true,"transition":"fade","slideNumber":true,"chalkboard":true}}}}