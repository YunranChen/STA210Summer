{"title":"LR: Inference + conditions","markdown":{"yaml":{"title":"LR: Inference + conditions","subtitle":"STA 210 - Spring 2022","author":"Dr. Mine Çetinkaya-Rundel","footer":"[sta210-s22.github.io/website](https://sta210-s22.github.io/website/)","logo":"images/logo.png","format":{"revealjs":{"theme":"slides.scss","transition":"fade","slide-number":true,"incremental":true,"chalkboard":true}},"editor":"visual","execute":{"freeze":"auto","echo":true}},"headingText":"| include: false","containsRefs":false,"markdown":"\n\n```{r}\n\n# figure options\nknitr::opts_chunk$set(\n  fig.width = 8, fig.asp = 0.618, out.width = \"90%\",\n  fig.retina = 3, dpi = 300, fig.align = \"center\"\n)\n\nlibrary(countdown)\n```\n\n# Welcome\n\n## Topics\n\n::: nonincremental\n-   Bulding predictive logistic regression models\n-   Sensitivity and specificity\n-   Making classification decisions\n:::\n\n## Computational setup\n\n```{r}\n#| echo: true\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)\nlibrary(kableExtra)  # for table embellishments\nlibrary(Stat2Data)   # for empirical logit\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))\n```\n\n# Data\n\n## Risk of coronary heart disease {.smaller}\n\nThis dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts.\nWe want to examine the relationship between various health characteristics and the risk of having heart disease.\n\n-   `high_risk`:\n\n    -   1: High risk of having heart disease in next 10 years\n    -   0: Not high risk of having heart disease in next 10 years\n\n-   `age`: Age at exam time (in years)\n\n-   `education`: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\n\n-   `currentSmoker`: 0 = nonsmoker, 1 = smoker\n\n## Data prep\n\n```{r}\nheart_disease <- read_csv(here::here(\"slides\", \"data/framingham.csv\")) %>%\n  select(age, education, TenYearCHD, totChol, currentSmoker) %>%\n  drop_na() %>%\n  mutate(\n    high_risk = as.factor(TenYearCHD),\n    education = as.factor(education),\n    currentSmoker = as.factor(currentSmoker)\n  )\n\nheart_disease\n```\n\n# Inference for a model\n\n## Modeling risk of coronary heart disease\n\nFrom age and education:\n\n```{r}\nrisk_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")\n```\n\n## Model output {.smaller}\n\n```{r}\ntidy(risk_fit, conf.int = TRUE) %>% \n  kable(format = \"markdown\", digits = 3)\n```\n\n$$\n\\small{\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.385 + 0.073 ~ \\text{age} - 0.242 ~ \\text{ed2} - 0.235 ~ \\text{ed3} - 0.020 ~ \\text{ed4}}\n$$\n\n## Hypothesis test for $\\beta_j$\n\n**Hypotheses:** $H_0: \\beta_j = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_j \\neq 0$\n\n. . .\n\n**Test Statistic:** $$z = \\frac{\\hat{\\beta}_j - 0}{SE_{\\hat{\\beta}_j}}$$\n\n. . .\n\n**P-value:** $P(|Z| > |z|)$, where $Z \\sim N(0, 1)$, the Standard Normal distribution\n\n## Confidence interval for $\\beta_j$\n\nWe can calculate the .vocab\\[C% confidence interval\\] for $\\beta_j$ as the following:\n\n$$\n\\Large{\\hat{\\beta}_j \\pm z^* SE_{\\hat{\\beta}_j}}\n$$\n\nwhere $z^*$ is calculated from the $N(0,1)$ distribution\n\n. . .\n\nThis is an interval for the change in the log-odds for every one unit increase in $x_j$.\n\n## Interpretation in terms of the odds\n\nThe change in **odds** for every one unit increase in $x_j$.\n\n$$\n\\Large{e^{\\hat{\\beta}_j \\pm z^* SE_{\\hat{\\beta}_j}}}\n$$\n\n. . .\n\n**Interpretation:** We are $C\\%$ confident that for every one unit increase in $x_j$, the odds multiply by a factor of $e^{\\hat{\\beta}_j - z^* SE_{\\hat{\\beta}_j}}$ to $e^{\\hat{\\beta}_j + z^* SE_{\\hat{\\beta}_j}}$, holding all else constant.\n\n## Coefficient for `age` {.smaller}\n\n```{r}\n#| label: risk-model-age-highlight\n#| echo: false\n\ntidy(risk_fit, conf.int = TRUE) %>% \n  kable(digits = 3) %>%\n  row_spec(2, background = \"#D9E3E4\")\n```\n\n. . .\n\n**Hypotheses:**\n\n$$\nH_0: \\beta_{1} = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_{1} \\neq 0\n$$\n\n## Coefficient for `age` {.smaller}\n\n```{r}\n#| echo: false\n#| ref.label: risk-model-age-highlight\n```\n\n**Test statistic:**\n\n$$\nz = \\frac{0.0733 - 0}{0.00547} = 13.4\n$$\n\n## Coefficient for `age` {.smaller}\n\n```{r}\n#| echo: false\n#| ref.label: risk-model-age-highlight\n```\n\n**P-value:**\n\n$$\nP(|Z| > |13.4|) \\approx 0\n$$\n\n. . .\n\n```{r}\n2 * pnorm(13.4,lower.tail = FALSE)\n```\n\n## Coefficient for `age` {.smaller}\n\n```{r}\n#| echo: false\n#| ref.label: risk-model-age-highlight\n```\n\n**Conclusion:**\n\nThe p-value is very small, so we reject $H_0$.\nThe data provide sufficient evidence that age is a statistically significant predictor of whether someone is high risk of having heart disease, *after accounting for education*.\n\n# Comparing models\n\n## Log likelihood\n\n$$\n\\log L = \\sum\\limits_{i=1}^n[y_i \\log(\\hat{\\pi}_i) + (1 - y_i)\\log(1 - \\hat{\\pi}_i)]\n$$\n\n-   Measure of how well the model fits the data\n\n-   Higher values of $\\log L$ are better\n\n-   **Deviance** = $-2 \\log L$\n\n    -   $-2 \\log L$ follows a $\\chi^2$ distribution with $n - p - 1$ degrees of freedom\n\n## Comparing nested models\n\n-   Suppose there are two models:\n\n    -   Reduced Model includes predictors $x_1, \\ldots, x_q$\n    -   Full Model includes predictors $x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p$\n\n-   We want to test the hypotheses\n\n    $$\n    \\begin{aligned}\n    H_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\n    H_A&: \\text{ at least 1 }\\beta_j \\text{ is not } 0\n    \\end{aligned}\n    $$\n\n-   To do so, we will use the **Drop-in-deviance test**, also known as the Nested Likelihood Ratio test\n\n## Drop-in-deviance test\n\n**Hypotheses:**\n\n$$\n\\begin{aligned}\nH_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\nH_A&: \\text{ at least 1 }\\beta_j \\text{ is not } 0\n\\end{aligned}\n$$\n\n. . .\n\n**Test Statistic:** $$G = (-2 \\log L_{reduced}) - (-2 \\log L_{full})$$\n\n. . .\n\n**P-value:** $P(\\chi^2 > G)$, calculated using a $\\chi^2$ distribution with degrees of freedom equal to the difference in the number of parameters in the full and reduced models\n\n## $\\chi^2$ distribution\n\n```{r}\n#| echo: false\n#| fig-height: 6\n\nx <- seq(from =0, to = 10, length = 100)\n\n# Evaluate the densities\ny_1 <- dchisq(x, 1)\ny_2 <- dchisq(x,2)\ny_3 <- dchisq(x,3)\ny_4 <- dchisq(x,5)\n\n# Plot the densities\nplot(x, y_1, col = 1, type = \"l\", ylab=\"\",lwd=3, ylim = c(0, 0.5), \n     main  = \"Chi-square Distribution\")\nlines(x,y_2, col = 2,lwd=3)\nlines(x, y_3, col = 3,lwd=3)\nlines(x, y_4, col = 4,lwd=3)\n\n# Add the legend\nlegend(\"topright\",\n       c(\"df = 1\", \"df = 2 \", \"df = 3\", \"df = 5\"), \n       col = c(1, 2, 3, 4), lty = 1)\n```\n\n## Model with age and education {.smaller}\n\n```{r}\n#| echo: false\nrisk_fit_reduced <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")\n\nrisk_fit_full <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age + education + currentSmoker, \n      data = heart_disease, family = \"binomial\")\n```\n\n::: question\nShould we add `currentSmoker` to this model?\n:::\n\n```{r}\n#| echo: false\n\ntidy(risk_fit_reduced, conf.int = T) %>% \n  kable(digits = 3)\n```\n\n## Should we add `currentSmoker` to the model?\n\nFirst model, reduced:\n\n```{r}\n#| eval: false\n\nrisk_fit_reduced <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")\n```\n\n<br>\n\nSecond model, full:\n\n```{r}\n#| eval: false\n#| code-line-numbers: \"2\"\n\nrisk_fit_full <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age + education + currentSmoker, \n      data = heart_disease, family = \"binomial\")\n```\n\n## Should we add `currentSmoker` to the model?\n\nCalculate deviance for each model:\n\n```{r}\n(dev_reduced <- glance(risk_fit_reduced)$deviance)\n\n(dev_full <- glance(risk_fit_full)$deviance)\n```\n\n. . .\n\nDrop-in-deviance test statistic:\n\n```{r}\n(test_stat <- dev_reduced - dev_full)\n```\n\n## Should we add `currentSmoker` to the model?\n\nCalculate the p-value using a `pchisq()`, with degrees of freedom equal to the number of new model terms in the second model:\n\n```{r}\npchisq(test_stat, 1, lower.tail = FALSE) \n```\n\n. . .\n\n**Conclusion:** The p-value is very small, so we reject $H_0$.\nThe data provide sufficient evidence that the coefficient of `currentSmoker` is not equal to 0.\nTherefore, we should add it to the model.\n\n## Drop-in-Deviance test in R\n\n-   We can use the **`anova`** function to conduct this test\n\n-   Add **`test = \"Chisq\"`** to conduct the drop-in-deviance test\n\n. . .\n\n```{r}\nanova(risk_fit_reduced$fit, risk_fit_full$fit, test = \"Chisq\") %>%\n  tidy()\n```\n\n## Model selection\n\nUse AIC or BIC for model selection\n\n$$\n\\begin{align}\n&AIC = - 2 * \\log L - \\color{purple}{n\\log(n)}+ 2(p +1)\\\\[5pt]\n&BIC =- 2 * \\log L - \\color{purple}{n\\log(n)} + log(n)\\times(p+1)\n\\end{align}\n$$\n\n## AIC from the `glance()` function\n\nLet's look at the AIC for the model that includes `age`, `education`, and `currentSmoker`\n\n```{r}\nglance(risk_fit_full)$AIC\n```\n\n. . .\n\n**Calculating AIC**\n\n```{r}\n- 2 * glance(risk_fit_full)$logLik + 2 * (5 + 1)\n```\n\n## Comparing the models using AIC\n\nLet's compare the full and reduced models using AIC.\n\n```{r}\nglance(risk_fit_reduced)$AIC\nglance(risk_fit_full)$AIC\n```\n\n::: question\nBased on AIC, which model would you choose?\n:::\n\n## Comparing the models using BIC\n\nLet's compare the full and reduced models using BIC\n\n```{r echo = T}\nglance(risk_fit_reduced)$BIC\nglance(risk_fit_full)$BIC\n```\n\n::: question\nBased on BIC, which model would you choose?\n:::\n\n# Conditions\n\n## The model {.smaller}\n\nLet's predict `high_risk` from age, total cholesterol, and whether the patient is a current smoker:\n\n```{r}\nrisk_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age + totChol + currentSmoker, \n      data = heart_disease, family = \"binomial\")\n\ntidy(risk_fit, conf.int = TRUE) %>% \n  kable(digits = 3)\n```\n\n## Conditions for logistic regression\n\n1.  **Linearity:** The log-odds have a linear relationship with the predictors.\n\n2.  **Randomness:** The data were obtained from a random process\n\n3.  **Independence:** The observations are independent from one another.\n\n## Empirical logit\n\nThe **empirical logit** is the log of the observed odds:\n\n$$\n\\text{logit}(\\hat{p}) = \\log\\Big(\\frac{\\hat{p}}{1 - \\hat{p}}\\Big) = \\log\\Big(\\frac{\\# \\text{Yes}}{\\# \\text{No}}\\Big)\n$$\n\n## Calculating empirical logit (categorical predictor)\n\nIf the predictor is categorical, we can calculate the empirical logit for each level of the predictor.\n\n```{r}\nheart_disease %>%\n  count(currentSmoker, high_risk) %>%\n  group_by(currentSmoker) %>%\n  mutate(prop = n/sum(n)) %>%\n  filter(high_risk == \"1\") %>%\n  mutate(emp_logit = log(prop/(1-prop)))\n```\n\n## Calculating empirical logit (quantitative predictor)\n\n1.  Divide the range of the predictor into intervals with approximately equal number of cases.\n    (If you have enough observations, use 5 - 10 intervals.)\n\n2.  Calculate the mean value of the predictor in each interval.\n\n3.  Compute the empirical logit for each interval.\n\n. . .\n\nThen, create a plot of the empirical logit versus the mean value of the predictor in each interval.\n\n## Empirical logit plot in R (quantitative predictor)\n\n```{r}\nemplogitplot1(high_risk ~ age, \n              data = heart_disease, \n              ngroups = 10)\n```\n\n## Empirical logit plot in R (interactions)\n\n```{r}\nemplogitplot2(high_risk ~ age + currentSmoker, data = heart_disease, \n              ngroups = 10, \n              putlegend = \"bottomright\")\n```\n\n## Checking linearity\n\n::: columns\n::: {.column width=\"50%\"}\n```{r}\nemplogitplot1(high_risk ~ age, \n              data = heart_disease, \n              ngroups = 10)\n```\n:::\n\n::: {.column width=\"50%\"}\n```{r}\nemplogitplot1(high_risk ~ totChol, \n              data = heart_disease, \n              ngroups = 10)\n```\n:::\n:::\n\n. . .\n\n`r emo::ji(\"white_check_mark\")` The linearity condition is satisfied.\nThere is a linear relationship between the empirical logit and the predictor variables.\n\n## Checking randomness {.smaller}\n\nWe can check the randomness condition based on the context of the data and how the observations were collected.\n\n-   Was the sample randomly selected?\n-   If the sample was not randomly selected, ask whether there is reason to believe the observations in the sample differ systematically from the population of interest.\n\n. . .\n\n`r emo::ji(\"white_check_mark\")` The randomness condition is satisfied.\nWe do not have reason to believe that the participants in this study differ systematically from adults in the U.S. in regards to health characteristics and risk of heart disease.\n\n## Checking independence\n\n- We can check the independence condition based on the context of the data and how the observations were collected.\n- Independence is most often violated if the data were collected over time or there is a strong spatial relationship between the observations.\n\n. . .\n\n`r emo::ji(\"white_check_mark\")` The independence condition is satisfied.\nIt is reasonable to conclude that the participants' health characteristics are independent of one another.\n"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":true,"output-file":"lec-22.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"0.9.345","auto-stretch":true,"editor":"visual","title":"LR: Inference + conditions","subtitle":"STA 210 - Spring 2022","author":"Dr. Mine Çetinkaya-Rundel","footer":"[sta210-s22.github.io/website](https://sta210-s22.github.io/website/)","logo":"images/logo.png","theme":"slides.scss","transition":"fade","slideNumber":true,"chalkboard":true}}}}