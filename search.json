[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 210: Regression Analysis",
    "section": "",
    "text": "W\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nSlides\nAE\nLab\nHW\nExam\nProject\n\n\n\n\n1\nWed, May 11\nWelcome to STA 210\n📖\n🖥️\n📋\n\n\n\n\n\n\n\n\nLab 1 - Meet the toolkit\n\n🖥️\n\n💻\n\n\n\n\n\n\nThur, May 12\nSimple linear regression (SLR)\n\n🖥️\n\n\n\n\n\n\n\n*\nFri, May 13\nSLR: Model fitting in R with tidymodels\n\n🖥️\n📋\n\n\n\n\n\n\n\n\nReleased: HW 5\n\n\n\n\n✍️\n\n\n\n\n\n\nDue: Lab 1 + AE 0\n\n\n\n💻 🗝️\n\n\n\n\n\n*\nSun, May 15\nDue: AE 1\n\n\n\n\n\n\n\n\n\n2\nMon, May 16\nSLR: Prediction + model evaluation\n📖\n🖥️\n📋\n\n\n\n\n\n\n\n\nLab: Intro to HW5 + AE2 (continued)\n\n\n📋\n\n✍️\n\n\n\n\n\n\nReleased: HW 1\n\n\n\n\n✍️\n\n\n\n\n\nTue, May 17\nSLR: Simulation-based inference\n\n🖥️\n\n\n\n\n\n\n\n*\nWed, May 18\nSLR: Mathematical models for inference\n\n🖥️\n\n\n\n\n\n\n\n\n\nLab 2 - College scorecard\n\n\n\n💻\n\n\n\n\n\n\n\nDue: HW 1 + AE 2\n\n\n\n\n✍️ 🗝️\n\n\n\n\n\nThur, May 19\nSLR: Model diagnostics\n\n🖥️\n📋\n\n\n\n\n\n\n*\nFri, May 20\nExam 1 review\n\n🖥️\n📋\n\n\n\n\n\n\n\n\nReleased: Exam 1\n\n\n\n\n\n✅\n\n\n\n\n\nDue: Lab 2 + AE 3\n\n\n\n💻 🗝️\n\n\n\n\n\n*\nSun, May 22\nDue: AE 4\n\n\n\n\n\n\n\n\n\n3*\nMon, May 23\nMultiple linear regression (MLR)\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab: Project topic ideas\n\n\n\n\n\n\n📂\n\n\n\n\nDue: Exam 1\n\n\n\n\n\n✅🗝️\n\n\n\n\nTue, May 24\nMLR: Types of predictors\n\n🖥️\n\n\n\n\n\n\n\n\nWed, May 25\nMLR: Types of predictors\n\n🖥️\n\n\n\n\n\n\n\n\n\nReleased: HW 2\n\n\n\n\n✍️\n\n\n\n\n\n\nLab 3 - Coffee ratings\n\n\n\n💻\n\n\n\n\n\n\nThur, May 26\nMLR: Model comparison\n\n🖥️\n\n\n\n\n\n\n\n*\nFri, May 27\nMLR: Feature engineering\n\n🖥️\n📋\n\n\n\n\n\n\n\n\nDue: Lab 3\n\n\n\n💻 🗝️\n\n\n\n\n\n*\nSun, May 29\nDue: HW 2 + Project topic ideas + AE 5\n\n\n\n\n✍️ 🗝️\n\n📂\n\n\n4\nMon, May 30\nMemorial Day holiday\n📖\n\n\n\n\n\n\n\n\n\nTue, May 31\nMLR: Cross validation\n\n🖥️\n📋\n\n\n\n\n\n\n*\nWed, June 1\nMLR: Inference\n\n🖥️\n📋\n\n\n\n\n\n\n\n\nLab: Lab 4 + project proposals\n\n\n\n💻\n\n\n📂\n\n\n\n\nDue: AE 6\n\n\n\n\n\n\n\n\n\n\nThur, June 2\nMLR: Inference conditions + multicollinearity\n\n🖥️\n\n\n\n\n\n\n\n*\nFri, June 3\nExam 2 review\n\n🖥️\n📋\n\n\n\n\n\n\n\n\nReleased: Exam 2\n\n\n\n\n\n✅\n\n\n\n\n\nDue: Lab 4 + AE 8\n\n\n\n💻 🗝️\n\n\n\n\n\n*\nSun, June 5\nDue: AE 7\n\n\n\n\n\n\n\n\n\n5*\nMon, June 6\nLogistic regression (LR)\n📖\n🖥️\n📋\n\n\n\n\n\n\n\n\nLab: Work on project proposals\n\n\n\n\n\n\n📂\n\n\n\n\nDue: Exam 2\n\n\n\n\n\n✅\n\n\n\n\nTue, June 7\nProbabilities, odds, and odds ratios\n\n🖥️\n\n\n\n\n\n\n\n*\nWed, June 8\nLR: Prediction / classification\n\n🖥️\n\n\n\n\n\n\n\n\n\nLab 5: General Social Survey+ project drafts\n\n\n\n💻\n\n\n📂\n\n\n\n\nDue: Project proposals + AE 9\n\n\n\n\n\n\n📂\n\n\n\n\nReleased: HW 3\n\n\n\n\n✍️\n\n\n\n\n\nThur, June 9\nLR: Model inference + comparison + conditions\n\n🖥️\n📋\n\n\n\n\n\n\n*\nFri, June 10\nMultinomial Logistic Regression (MultiLR)\n\n🖥️\n\n\n\n\n\n\n\n\n\nDue: Lab 5 + AE 10\n\n\n\n💻 🗝️\n\n\n\n\n\n*\nSun, June 12\nDue: HW 3\n\n\n\n\n✍️ 🗝️\n\n📂\n\n\n6*\nMon, June 13\nMultiLR: Prediction + inferential models\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nWork on project drafts\n\n\n\n\n\n\n📂\n\n\n\n\nDue: HW 5\n\n\n\n\n✍️\n\n\n\n\n\n\nReleased: HW 4\n\n\n\n\n✍️\n\n\n\n\n\nTue, June 14\nMultiLR: Predictive models\n\n🖥️\n📋\n\n\n\n\n\n\n*\nWed, June 15\nData Wrangling\n\n🖥️\n\n\n\n\n\n\n\n\n\nLab 6: Why Many Americans Don’t Vote\n\n\n\n💻\n\n\n\n\n\n\n\nDue: HW 4+project drafts\n\n\n\n\n✍️ 🗝️\n\n📂\n\n\n\nThur, June 16\nExam 3 review+Work on Project peer review of drafts\n\n🖥️\n📋\n\n\n\n📂\n\n\n*\nFri, June 17\nWrap-up\n\n🖥️\n\n\n\n\n\n\n\n\n\nDue: Lab 6 +AE 11\n\n\n\n💻 🗝️\n\n\n\n\n\n\n\nReleased: Exam 3\n\n\n\n\n\n✅\n\n\n\n*\nSun, June 19\nDue: AE 12\n\n\n\n\n\n\n\n\n\n7*\nMon, June 20\nJuneteenth holiday. No class\n📖\n\n\n\n\n\n\n\n\n\n\nDue: Exam 3\n\n\n\n\n\n✅\n\n\n\n\nTue, June 21\nReading day\n\n\n\n\n\n\n\n\n\n*\nWed, June 22\nDue: Video presentation + repo\n\n\n\n\n\n\n📂\n\n\n\n\nDue: Video comments\n\n\n\n\n\n\n📂\n\n\n\n\nDue: Project write-up\n\n\n\n\n\n\n📂\n\n\n\nThur, June 23\nFinal grades"
  },
  {
    "objectID": "slides/lec-22.html#topics",
    "href": "slides/lec-22.html#topics",
    "title": "LR: Inference + conditions",
    "section": "Topics",
    "text": "Topics\n\n\nBuilding predictive logistic regression models\nSensitivity and specificity\nMaking classification decisions"
  },
  {
    "objectID": "slides/lec-22.html#computational-setup",
    "href": "slides/lec-22.html#computational-setup",
    "title": "LR: Inference + conditions",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)\nlibrary(kableExtra)  # for table embellishments\nlibrary(Stat2Data)   # for empirical logit\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-22.html#risk-of-coronary-heart-disease",
    "href": "slides/lec-22.html#risk-of-coronary-heart-disease",
    "title": "LR: Inference + conditions",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)\neducation: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\ncurrentSmoker: 0 = nonsmoker, 1 = smoker"
  },
  {
    "objectID": "slides/lec-22.html#data-prep",
    "href": "slides/lec-22.html#data-prep",
    "title": "LR: Inference + conditions",
    "section": "Data prep",
    "text": "Data prep\n\nheart_disease <- read_csv(here::here(\"slides\", \"data/framingham.csv\")) %>%\n  select(age, education, TenYearCHD, totChol, currentSmoker) %>%\n  drop_na() %>%\n  mutate(\n    high_risk = as.factor(TenYearCHD),\n    education = as.factor(education),\n    currentSmoker = as.factor(currentSmoker)\n  )\n\nheart_disease\n\n# A tibble: 4,086 × 6\n     age education TenYearCHD totChol currentSmoker high_risk\n   <dbl> <fct>          <dbl>   <dbl> <fct>         <fct>    \n 1    39 4                  0     195 0             0        \n 2    46 2                  0     250 0             0        \n 3    48 1                  0     245 1             0        \n 4    61 3                  1     225 1             1        \n 5    46 3                  0     285 1             0        \n 6    43 2                  0     228 0             0        \n 7    63 1                  1     205 0             1        \n 8    45 2                  0     313 1             0        \n 9    52 1                  0     260 0             0        \n10    43 1                  0     225 1             0        \n# … with 4,076 more rows"
  },
  {
    "objectID": "slides/lec-22.html#modeling-risk-of-coronary-heart-disease",
    "href": "slides/lec-22.html#modeling-risk-of-coronary-heart-disease",
    "title": "LR: Inference + conditions",
    "section": "Modeling risk of coronary heart disease",
    "text": "Modeling risk of coronary heart disease\nFrom age and education:\n\nrisk_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")"
  },
  {
    "objectID": "slides/lec-22.html#model-output",
    "href": "slides/lec-22.html#model-output",
    "title": "LR: Inference + conditions",
    "section": "Model output",
    "text": "Model output\n\ntidy(risk_fit, conf.int = TRUE) %>% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\\[\n\\small{\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.385 + 0.073 ~ \\text{age} - 0.242 ~ \\text{ed2} - 0.235 ~ \\text{ed3} - 0.020 ~ \\text{ed4}}\n\\]"
  },
  {
    "objectID": "slides/lec-22.html#hypothesis-test-for-beta_j",
    "href": "slides/lec-22.html#hypothesis-test-for-beta_j",
    "title": "LR: Inference + conditions",
    "section": "Hypothesis test for \\(\\beta_j\\)",
    "text": "Hypothesis test for \\(\\beta_j\\)\nHypotheses: \\(H_0: \\beta_j = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_j \\neq 0\\)\n\nTest Statistic: \\[z = \\frac{\\hat{\\beta}_j - 0}{SE_{\\hat{\\beta}_j}}\\]\n\n\nP-value: \\(P(|Z| > |z|)\\), where \\(Z \\sim N(0, 1)\\), the Standard Normal distribution"
  },
  {
    "objectID": "slides/lec-22.html#confidence-interval-for-beta_j",
    "href": "slides/lec-22.html#confidence-interval-for-beta_j",
    "title": "LR: Inference + conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\nWe can calculate the .vocab[C% confidence interval] for \\(\\beta_j\\) as the following:\n\\[\n\\Large{\\hat{\\beta}_j \\pm z^* SE_{\\hat{\\beta}_j}}\n\\]\nwhere \\(z^*\\) is calculated from the \\(N(0,1)\\) distribution\n\nThis is an interval for the change in the log-odds for every one unit increase in \\(x_j\\)."
  },
  {
    "objectID": "slides/lec-22.html#interpretation-in-terms-of-the-odds",
    "href": "slides/lec-22.html#interpretation-in-terms-of-the-odds",
    "title": "LR: Inference + conditions",
    "section": "Interpretation in terms of the odds",
    "text": "Interpretation in terms of the odds\nThe change in odds for every one unit increase in \\(x_j\\).\n\\[\n\\Large{e^{\\hat{\\beta}_j \\pm z^* SE_{\\hat{\\beta}_j}}}\n\\]\n\nInterpretation: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), the odds multiply by a factor of \\(e^{\\hat{\\beta}_j - z^* SE_{\\hat{\\beta}_j}}\\) to \\(e^{\\hat{\\beta}_j + z^* SE_{\\hat{\\beta}_j}}\\), holding all else constant."
  },
  {
    "objectID": "slides/lec-22.html#coefficient-for-age",
    "href": "slides/lec-22.html#coefficient-for-age",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n  \n \n\n  \n    (Intercept) \n    -5.508 \n    0.311 \n    -17.692 \n    0.000 \n    -6.125 \n    -4.904 \n  \n  \n    age \n    0.076 \n    0.006 \n    13.648 \n    0.000 \n    0.065 \n    0.087 \n  \n  \n    education2 \n    -0.245 \n    0.113 \n    -2.172 \n    0.030 \n    -0.469 \n    -0.026 \n  \n  \n    education3 \n    -0.236 \n    0.135 \n    -1.753 \n    0.080 \n    -0.504 \n    0.024 \n  \n  \n    education4 \n    -0.024 \n    0.150 \n    -0.161 \n    0.872 \n    -0.323 \n    0.264 \n  \n\n\n\n\n\n\nHypotheses:\n\\[\nH_0: \\beta_{1} = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_{1} \\neq 0\n\\]"
  },
  {
    "objectID": "slides/lec-22.html#coefficient-for-age-1",
    "href": "slides/lec-22.html#coefficient-for-age-1",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n  \n \n\n  \n    (Intercept) \n    -5.508 \n    0.311 \n    -17.692 \n    0.000 \n    -6.125 \n    -4.904 \n  \n  \n    age \n    0.076 \n    0.006 \n    13.648 \n    0.000 \n    0.065 \n    0.087 \n  \n  \n    education2 \n    -0.245 \n    0.113 \n    -2.172 \n    0.030 \n    -0.469 \n    -0.026 \n  \n  \n    education3 \n    -0.236 \n    0.135 \n    -1.753 \n    0.080 \n    -0.504 \n    0.024 \n  \n  \n    education4 \n    -0.024 \n    0.150 \n    -0.161 \n    0.872 \n    -0.323 \n    0.264 \n  \n\n\n\n\n\nTest statistic:\n\\[\nz = \\frac{0.0733 - 0}{0.00547} = 13.4\n\\]"
  },
  {
    "objectID": "slides/lec-22.html#coefficient-for-age-2",
    "href": "slides/lec-22.html#coefficient-for-age-2",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n  \n \n\n  \n    (Intercept) \n    -5.508 \n    0.311 \n    -17.692 \n    0.000 \n    -6.125 \n    -4.904 \n  \n  \n    age \n    0.076 \n    0.006 \n    13.648 \n    0.000 \n    0.065 \n    0.087 \n  \n  \n    education2 \n    -0.245 \n    0.113 \n    -2.172 \n    0.030 \n    -0.469 \n    -0.026 \n  \n  \n    education3 \n    -0.236 \n    0.135 \n    -1.753 \n    0.080 \n    -0.504 \n    0.024 \n  \n  \n    education4 \n    -0.024 \n    0.150 \n    -0.161 \n    0.872 \n    -0.323 \n    0.264 \n  \n\n\n\n\n\nP-value:\n\\[\nP(|Z| > |13.4|) \\approx 0\n\\]\n\n\n2 * pnorm(13.4,lower.tail = FALSE)\n\n[1] 6.046315e-41"
  },
  {
    "objectID": "slides/lec-22.html#coefficient-for-age-3",
    "href": "slides/lec-22.html#coefficient-for-age-3",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n  \n \n\n  \n    (Intercept) \n    -5.508 \n    0.311 \n    -17.692 \n    0.000 \n    -6.125 \n    -4.904 \n  \n  \n    age \n    0.076 \n    0.006 \n    13.648 \n    0.000 \n    0.065 \n    0.087 \n  \n  \n    education2 \n    -0.245 \n    0.113 \n    -2.172 \n    0.030 \n    -0.469 \n    -0.026 \n  \n  \n    education3 \n    -0.236 \n    0.135 \n    -1.753 \n    0.080 \n    -0.504 \n    0.024 \n  \n  \n    education4 \n    -0.024 \n    0.150 \n    -0.161 \n    0.872 \n    -0.323 \n    0.264 \n  \n\n\n\n\n\nConclusion:\nThe p-value is very small, so we reject \\(H_0\\). The data provide sufficient evidence that age is a statistically significant predictor of whether someone is high risk of having heart disease, after accounting for education."
  },
  {
    "objectID": "slides/lec-22.html#log-likelihood",
    "href": "slides/lec-22.html#log-likelihood",
    "title": "LR: Inference + conditions",
    "section": "Log likelihood",
    "text": "Log likelihood\n\\[\n\\log L = \\sum\\limits_{i=1}^n[y_i \\log(\\hat{\\pi}_i) + (1 - y_i)\\log(1 - \\hat{\\pi}_i)]\n\\]\n\nMeasure of how well the model fits the data\nHigher values of \\(\\log L\\) are better\nDeviance = \\(-2 \\log L\\)\n\n\\(-2 \\log L\\) follows a \\(\\chi^2\\) distribution with \\(n - p - 1\\) degrees of freedom"
  },
  {
    "objectID": "slides/lec-22.html#comparing-nested-models",
    "href": "slides/lec-22.html#comparing-nested-models",
    "title": "LR: Inference + conditions",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced Model includes predictors \\(x_1, \\ldots, x_q\\)\nFull Model includes predictors \\(x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p\\)\n\nWe want to test the hypotheses\n\\[\n\\begin{aligned}\nH_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\nH_A&: \\text{ at least 1 }\\beta_j \\text{ is not } 0\n\\end{aligned}\n\\]\nTo do so, we will use the Drop-in-deviance test, also known as the Nested Likelihood Ratio test"
  },
  {
    "objectID": "slides/lec-22.html#drop-in-deviance-test",
    "href": "slides/lec-22.html#drop-in-deviance-test",
    "title": "LR: Inference + conditions",
    "section": "Drop-in-deviance test",
    "text": "Drop-in-deviance test\nHypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\nH_A&: \\text{ at least 1 }\\beta_j \\text{ is not } 0\n\\end{aligned}\n\\]\n\nTest Statistic: \\[G = (-2 \\log L_{reduced}) - (-2 \\log L_{full})\\]\n\n\nP-value: \\(P(\\chi^2 > G)\\), calculated using a \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in the number of parameters in the full and reduced models"
  },
  {
    "objectID": "slides/lec-22.html#chi2-distribution",
    "href": "slides/lec-22.html#chi2-distribution",
    "title": "LR: Inference + conditions",
    "section": "\\(\\chi^2\\) distribution",
    "text": "\\(\\chi^2\\) distribution"
  },
  {
    "objectID": "slides/lec-22.html#model-with-age-and-education",
    "href": "slides/lec-22.html#model-with-age-and-education",
    "title": "LR: Inference + conditions",
    "section": "Model with age and education",
    "text": "Model with age and education\n\n\n\n\nShould we add currentSmoker to this model?\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n  \n \n\n  \n    (Intercept) \n    -5.508 \n    0.311 \n    -17.692 \n    0.000 \n    -6.125 \n    -4.904 \n  \n  \n    age \n    0.076 \n    0.006 \n    13.648 \n    0.000 \n    0.065 \n    0.087 \n  \n  \n    education2 \n    -0.245 \n    0.113 \n    -2.172 \n    0.030 \n    -0.469 \n    -0.026 \n  \n  \n    education3 \n    -0.236 \n    0.135 \n    -1.753 \n    0.080 \n    -0.504 \n    0.024 \n  \n  \n    education4 \n    -0.024 \n    0.150 \n    -0.161 \n    0.872 \n    -0.323 \n    0.264"
  },
  {
    "objectID": "slides/lec-22.html#should-we-add-currentsmoker-to-the-model",
    "href": "slides/lec-22.html#should-we-add-currentsmoker-to-the-model",
    "title": "LR: Inference + conditions",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nFirst model, reduced:\n\nrisk_fit_reduced <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")\n\n\nSecond model, full:\n\nrisk_fit_full <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age + education + currentSmoker, \n      data = heart_disease, family = \"binomial\")"
  },
  {
    "objectID": "slides/lec-22.html#should-we-add-currentsmoker-to-the-model-1",
    "href": "slides/lec-22.html#should-we-add-currentsmoker-to-the-model-1",
    "title": "LR: Inference + conditions",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nCalculate deviance for each model:\n\n(dev_reduced <- glance(risk_fit_reduced)$deviance)\n\n[1] 3244.187\n\n(dev_full <- glance(risk_fit_full)$deviance)\n\n[1] 3221.901\n\n\n\nDrop-in-deviance test statistic:\n\n(test_stat <- dev_reduced - dev_full)\n\n[1] 22.2863"
  },
  {
    "objectID": "slides/lec-22.html#should-we-add-currentsmoker-to-the-model-2",
    "href": "slides/lec-22.html#should-we-add-currentsmoker-to-the-model-2",
    "title": "LR: Inference + conditions",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nCalculate the p-value using a pchisq(), with degrees of freedom equal to the number of new model terms in the second model:\n\npchisq(test_stat, 1, lower.tail = FALSE) \n\n[1] 2.348761e-06\n\n\n\nConclusion: The p-value is very small, so we reject \\(H_0\\). The data provide sufficient evidence that the coefficient of currentSmoker is not equal to 0. Therefore, we should add it to the model."
  },
  {
    "objectID": "slides/lec-22.html#drop-in-deviance-test-in-r",
    "href": "slides/lec-22.html#drop-in-deviance-test-in-r",
    "title": "LR: Inference + conditions",
    "section": "Drop-in-Deviance test in R",
    "text": "Drop-in-Deviance test in R\n\nWe can use the anova function to conduct this test\nAdd test = \"Chisq\" to conduct the drop-in-deviance test\n\n\n\nanova(risk_fit_reduced$fit, risk_fit_full$fit, test = \"Chisq\") %>%\n  tidy()\n\n# A tibble: 2 × 5\n  Resid..Df Resid..Dev    df Deviance     p.value\n      <dbl>      <dbl> <dbl>    <dbl>       <dbl>\n1      4081      3244.    NA     NA   NA         \n2      4080      3222.     1     22.3  0.00000235"
  },
  {
    "objectID": "slides/lec-22.html#model-selection",
    "href": "slides/lec-22.html#model-selection",
    "title": "LR: Inference + conditions",
    "section": "Model selection",
    "text": "Model selection\nUse AIC or BIC for model selection\n\\[\n\\begin{align}\n&AIC = - 2 * \\log L - \\color{purple}{n\\log(n)}+ 2(p +1)\\\\[5pt]\n&BIC =- 2 * \\log L - \\color{purple}{n\\log(n)} + log(n)\\times(p+1)\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lec-22.html#aic-from-the-glance-function",
    "href": "slides/lec-22.html#aic-from-the-glance-function",
    "title": "LR: Inference + conditions",
    "section": "AIC from the glance() function",
    "text": "AIC from the glance() function\nLet’s look at the AIC for the model that includes age, education, and currentSmoker\n\nglance(risk_fit_full)$AIC\n\n[1] 3233.901\n\n\n\nCalculating AIC\n\n- 2 * glance(risk_fit_full)$logLik + 2 * (5 + 1)\n\n[1] 3233.901"
  },
  {
    "objectID": "slides/lec-22.html#comparing-the-models-using-aic",
    "href": "slides/lec-22.html#comparing-the-models-using-aic",
    "title": "LR: Inference + conditions",
    "section": "Comparing the models using AIC",
    "text": "Comparing the models using AIC\nLet’s compare the full and reduced models using AIC.\n\nglance(risk_fit_reduced)$AIC\n\n[1] 3254.187\n\nglance(risk_fit_full)$AIC\n\n[1] 3233.901\n\n\n\nBased on AIC, which model would you choose?"
  },
  {
    "objectID": "slides/lec-22.html#comparing-the-models-using-bic",
    "href": "slides/lec-22.html#comparing-the-models-using-bic",
    "title": "LR: Inference + conditions",
    "section": "Comparing the models using BIC",
    "text": "Comparing the models using BIC\nLet’s compare the full and reduced models using BIC\n\nglance(risk_fit_reduced)$BIC\n\n[1] 3285.764\n\nglance(risk_fit_full)$BIC\n\n[1] 3271.793\n\n\n\nBased on BIC, which model would you choose?"
  },
  {
    "objectID": "slides/lec-22.html#the-model",
    "href": "slides/lec-22.html#the-model",
    "title": "LR: Inference + conditions",
    "section": "The model",
    "text": "The model\nLet’s predict high_risk from age, total cholesterol, and whether the patient is a current smoker:\n\nrisk_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age + totChol + currentSmoker, \n      data = heart_disease, family = \"binomial\")\n\ntidy(risk_fit, conf.int = TRUE) %>% \n  kable(digits = 3)\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n  \n \n\n  \n    (Intercept) \n    -6.673 \n    0.378 \n    -17.647 \n    0.000 \n    -7.423 \n    -5.940 \n  \n  \n    age \n    0.082 \n    0.006 \n    14.344 \n    0.000 \n    0.071 \n    0.094 \n  \n  \n    totChol \n    0.002 \n    0.001 \n    1.940 \n    0.052 \n    0.000 \n    0.004 \n  \n  \n    currentSmoker1 \n    0.443 \n    0.094 \n    4.733 \n    0.000 \n    0.260 \n    0.627"
  },
  {
    "objectID": "slides/lec-22.html#conditions-for-logistic-regression",
    "href": "slides/lec-22.html#conditions-for-logistic-regression",
    "title": "LR: Inference + conditions",
    "section": "Conditions for logistic regression",
    "text": "Conditions for logistic regression\n\nLinearity: The log-odds have a linear relationship with the predictors.\nRandomness: The data were obtained from a random process\nIndependence: The observations are independent from one another."
  },
  {
    "objectID": "slides/lec-22.html#empirical-logit",
    "href": "slides/lec-22.html#empirical-logit",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit",
    "text": "Empirical logit\nThe empirical logit is the log of the observed odds:\n\\[\n\\text{logit}(\\hat{p}) = \\log\\Big(\\frac{\\hat{p}}{1 - \\hat{p}}\\Big) = \\log\\Big(\\frac{\\# \\text{Yes}}{\\# \\text{No}}\\Big)\n\\]"
  },
  {
    "objectID": "slides/lec-22.html#calculating-empirical-logit-categorical-predictor",
    "href": "slides/lec-22.html#calculating-empirical-logit-categorical-predictor",
    "title": "LR: Inference + conditions",
    "section": "Calculating empirical logit (categorical predictor)",
    "text": "Calculating empirical logit (categorical predictor)\nIf the predictor is categorical, we can calculate the empirical logit for each level of the predictor.\n\nheart_disease %>%\n  count(currentSmoker, high_risk) %>%\n  group_by(currentSmoker) %>%\n  mutate(prop = n/sum(n)) %>%\n  filter(high_risk == \"1\") %>%\n  mutate(emp_logit = log(prop/(1-prop)))\n\n# A tibble: 2 × 5\n# Groups:   currentSmoker [2]\n  currentSmoker high_risk     n  prop emp_logit\n  <fct>         <fct>     <int> <dbl>     <dbl>\n1 0             1           301 0.145     -1.77\n2 1             1           318 0.158     -1.67"
  },
  {
    "objectID": "slides/lec-22.html#calculating-empirical-logit-quantitative-predictor",
    "href": "slides/lec-22.html#calculating-empirical-logit-quantitative-predictor",
    "title": "LR: Inference + conditions",
    "section": "Calculating empirical logit (quantitative predictor)",
    "text": "Calculating empirical logit (quantitative predictor)\n\nDivide the range of the predictor into intervals with approximately equal number of cases. (If you have enough observations, use 5 - 10 intervals.)\nCalculate the mean value of the predictor in each interval.\nCompute the empirical logit for each interval.\n\n\nThen, create a plot of the empirical logit versus the mean value of the predictor in each interval."
  },
  {
    "objectID": "slides/lec-22.html#empirical-logit-plot-in-r-quantitative-predictor",
    "href": "slides/lec-22.html#empirical-logit-plot-in-r-quantitative-predictor",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit plot in R (quantitative predictor)",
    "text": "Empirical logit plot in R (quantitative predictor)\n\nemplogitplot1(high_risk ~ age, \n              data = heart_disease, \n              ngroups = 10)"
  },
  {
    "objectID": "slides/lec-22.html#empirical-logit-plot-in-r-interactions",
    "href": "slides/lec-22.html#empirical-logit-plot-in-r-interactions",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit plot in R (interactions)",
    "text": "Empirical logit plot in R (interactions)\n\nemplogitplot2(high_risk ~ age + currentSmoker, data = heart_disease, \n              ngroups = 10, \n              putlegend = \"bottomright\")"
  },
  {
    "objectID": "slides/lec-22.html#checking-linearity",
    "href": "slides/lec-22.html#checking-linearity",
    "title": "LR: Inference + conditions",
    "section": "Checking linearity",
    "text": "Checking linearity\n\n\n\nemplogitplot1(high_risk ~ age, \n              data = heart_disease, \n              ngroups = 10)\n\n\n\n\n\n\n\n\n\n\nemplogitplot1(high_risk ~ totChol, \n              data = heart_disease, \n              ngroups = 10)\n\n\n\n\n\n\n\n\n\n\n\n✅ The linearity condition is satisfied. There is a linear relationship between the empirical logit and the predictor variables."
  },
  {
    "objectID": "slides/lec-22.html#checking-randomness",
    "href": "slides/lec-22.html#checking-randomness",
    "title": "LR: Inference + conditions",
    "section": "Checking randomness",
    "text": "Checking randomness\nWe can check the randomness condition based on the context of the data and how the observations were collected.\n\nWas the sample randomly selected?\nIf the sample was not randomly selected, ask whether there is reason to believe the observations in the sample differ systematically from the population of interest.\n\n\n✅ The randomness condition is satisfied. We do not have reason to believe that the participants in this study differ systematically from adults in the U.S. in regards to health characteristics and risk of heart disease."
  },
  {
    "objectID": "slides/lec-22.html#checking-independence",
    "href": "slides/lec-22.html#checking-independence",
    "title": "LR: Inference + conditions",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can check the independence condition based on the context of the data and how the observations were collected.\nIndependence is most often violated if the data were collected over time or there is a strong spatial relationship between the observations.\n\n\n✅ The independence condition is satisfied. It is reasonable to conclude that the participants’ health characteristics are independent of one another.\n\n\n\nsta210-s22.github.io/website"
  },
  {
    "objectID": "slides/lec-4.html#questions-from-last-week",
    "href": "slides/lec-4.html#questions-from-last-week",
    "title": "SLR: Prediction + model evaluation",
    "section": "Questions from last week",
    "text": "Questions from last week\n\nIn YAML, set format: pdf instead of pdf_format\nset_engine: what alternatives? can use show_engines(\"linear_reg\")\nIn R code chunk, can set #| message: false and #| warning: false\nTwo teams, check github!\nGood news: set up Github Classroom! You can find your own repo and directly clone and edit! No need to fork anymore."
  },
  {
    "objectID": "slides/lec-4.html#computational-setup",
    "href": "slides/lec-4.html#computational-setup",
    "title": "SLR: Prediction + model evaluation",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/lec-4.html#data-source",
    "href": "slides/lec-4.html#data-source",
    "title": "SLR: Prediction + model evaluation",
    "section": "Data source",
    "text": "Data source\n\nThe data come from usdata::county_2019\nThese data have been compiled from the 2019 American Community Survey"
  },
  {
    "objectID": "slides/lec-4.html#uninsurance-rate",
    "href": "slides/lec-4.html#uninsurance-rate",
    "title": "SLR: Prediction + model evaluation",
    "section": "Uninsurance rate",
    "text": "Uninsurance rate"
  },
  {
    "objectID": "slides/lec-4.html#high-school-graduation-rate",
    "href": "slides/lec-4.html#high-school-graduation-rate",
    "title": "SLR: Prediction + model evaluation",
    "section": "High school graduation rate",
    "text": "High school graduation rate"
  },
  {
    "objectID": "slides/lec-4.html#examining-the-relationship",
    "href": "slides/lec-4.html#examining-the-relationship",
    "title": "SLR: Prediction + model evaluation",
    "section": "Examining the relationship",
    "text": "Examining the relationship\n\nThe NC Labor and Economic Analysis Division (LEAD), which “administers and collects data, conducts research, and publishes information on the state’s economy, labor force, educational, and workforce-related issues”.\nSuppose that an analyst working for LEAD is interested in the relationship between uninsurance and high school graduation rates in NC counties.\n\n\n\nWhat type of visualization should the analyst make to examine the relationship between these two variables?"
  },
  {
    "objectID": "slides/lec-4.html#data-prep",
    "href": "slides/lec-4.html#data-prep",
    "title": "SLR: Prediction + model evaluation",
    "section": "Data prep",
    "text": "Data prep\n\ncounty_2019_nc <- county_2019 %>%\n  as_tibble() %>%\n  filter(state == \"North Carolina\") %>%\n  select(name, hs_grad, uninsured)\n\ncounty_2019_nc\n\n# A tibble: 100 × 3\n   name             hs_grad uninsured\n   <chr>              <dbl>     <dbl>\n 1 Alamance County     86.3      11.2\n 2 Alexander County    82.4       8.9\n 3 Alleghany County    77.5      11.3\n 4 Anson County        80.7      11.1\n 5 Ashe County         85.1      12.6\n 6 Avery County        83.6      15.9\n 7 Beaufort County     87.7      12  \n 8 Bertie County       78.4      11.9\n 9 Bladen County       81.3      12.9\n10 Brunswick County    91.3       9.8\n# … with 90 more rows"
  },
  {
    "objectID": "slides/lec-4.html#uninsurance-vs.-hs-graduation-rates",
    "href": "slides/lec-4.html#uninsurance-vs.-hs-graduation-rates",
    "title": "SLR: Prediction + model evaluation",
    "section": "Uninsurance vs. HS graduation rates",
    "text": "Uninsurance vs. HS graduation rates\n\n\nCode\nggplot(county_2019_nc,\n       aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  ) +\n  geom_point(data = county_2019_nc %>% filter(name == \"Durham County\"), aes(x = hs_grad, y = uninsured), shape = \"circle open\", color = \"#8F2D56\", size = 4, stroke = 2) +\n  geom_text(data = county_2019_nc %>% filter(name == \"Durham County\"), aes(x = hs_grad, y = uninsured, label = name), color = \"#8F2D56\", fontface = \"bold\", nudge_y = 3, nudge_x = 2)"
  },
  {
    "objectID": "slides/lec-4.html#modeling-the-relationship",
    "href": "slides/lec-4.html#modeling-the-relationship",
    "title": "SLR: Prediction + model evaluation",
    "section": "Modeling the relationship",
    "text": "Modeling the relationship\n\n\nCode\nggplot(county_2019_nc, aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#8F2D56\") +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  )"
  },
  {
    "objectID": "slides/lec-4.html#fitting-the-model",
    "href": "slides/lec-4.html#fitting-the-model",
    "title": "SLR: Prediction + model evaluation",
    "section": "Fitting the model",
    "text": "Fitting the model\nWith fit():\n\nnc_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(uninsured ~ hs_grad, data = county_2019_nc)\n\ntidy(nc_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   33.9      3.99        8.50 2.12e-13\n2 hs_grad       -0.262    0.0468     -5.61 1.88e- 7"
  },
  {
    "objectID": "slides/lec-4.html#augmenting-the-data",
    "href": "slides/lec-4.html#augmenting-the-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Augmenting the data",
    "text": "Augmenting the data\nWith augment() to add columns for predicted values (.fitted), residuals (.resid), etc.:\n\nnc_aug <- augment(nc_fit$fit)\nnc_aug\n\n# A tibble: 100 × 8\n   uninsured hs_grad .fitted  .resid   .hat .sigma    .cooksd .std.resid\n       <dbl>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>      <dbl>      <dbl>\n 1      11.2    86.3   11.3  -0.0633 0.0107   2.10 0.00000501    -0.0305\n 2       8.9    82.4   12.3  -3.39   0.0138   2.07 0.0186        -1.63  \n 3      11.3    77.5   13.6  -2.27   0.0393   2.09 0.0252        -1.11  \n 4      11.1    80.7   12.7  -1.63   0.0199   2.09 0.00633       -0.790 \n 5      12.6    85.1   11.6   1.02   0.0100   2.10 0.00122        0.492 \n 6      15.9    83.6   12.0   3.93   0.0112   2.06 0.0203         1.89  \n 7      12      87.7   10.9   1.10   0.0133   2.10 0.00191        0.532 \n 8      11.9    78.4   13.3  -1.44   0.0328   2.09 0.00830       -0.700 \n 9      12.9    81.3   12.6   0.324  0.0174   2.10 0.000218       0.157 \n10       9.8    91.3    9.95 -0.151  0.0291   2.10 0.0000806     -0.0734\n# … with 90 more rows"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-the-model-i",
    "href": "slides/lec-4.html#visualizing-the-model-i",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model I",
    "text": "Visualizing the model I\n\n\n\n\nBlack circles: Observed values (y = uninsured)"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-the-model-ii",
    "href": "slides/lec-4.html#visualizing-the-model-ii",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model II",
    "text": "Visualizing the model II\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-the-model-iii",
    "href": "slides/lec-4.html#visualizing-the-model-iii",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model III",
    "text": "Visualizing the model III\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\nMaroon triangles: Predicted values (y = .fitted)"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-the-model-iv",
    "href": "slides/lec-4.html#visualizing-the-model-iv",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model IV",
    "text": "Visualizing the model IV\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\nMaroon triangles: Predicted values (y = .fitted)\nGray dashed lines: Residuals"
  },
  {
    "objectID": "slides/lec-4.html#evaluating-the-model-fit",
    "href": "slides/lec-4.html#evaluating-the-model-fit",
    "title": "SLR: Prediction + model evaluation",
    "section": "Evaluating the model fit",
    "text": "Evaluating the model fit\n\nHow can we evaluate whether the model for predicting uninsurance rate from high school graduation rate for NC counties is a good fit?"
  },
  {
    "objectID": "slides/lec-4.html#two-statistics",
    "href": "slides/lec-4.html#two-statistics",
    "title": "SLR: Prediction + model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/lec-4.html#r-squared",
    "href": "slides/lec-4.html#r-squared",
    "title": "SLR: Prediction + model evaluation",
    "section": "R-squared",
    "text": "R-squared\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nUnitless (Having no units of measurement; such as a ratio or percentage of two numbers which have the same units.)\nCalculate with rsq():\n\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.243"
  },
  {
    "objectID": "slides/lec-4.html#interpreting-r-squared",
    "href": "slides/lec-4.html#interpreting-r-squared",
    "title": "SLR: Prediction + model evaluation",
    "section": "Interpreting R-squared",
    "text": "Interpreting R-squared\n\n\n\n\n🗳️ Vote\nThe \\(R^2\\) of the model for predicting uninsurance rate from high school graduation rate for NC counties is 24.3%. Which of the following is the correct interpretation of this value?\n\n\nHigh school graduation rates correctly predict 24.3% of uninsurance rates in NC counties.\n24.3% of the variability in uninsurance rates in NC counties can be explained by high school graduation rates.\n24.3% of the variability in high school graduation rates in NC counties can be explained by uninsurance rates.\n24.3% of the time uninsurance rates in NC counties can be predicted by high school graduation rates."
  },
  {
    "objectID": "slides/lec-4.html#alternative-approach-for-r-squared",
    "href": "slides/lec-4.html#alternative-approach-for-r-squared",
    "title": "SLR: Prediction + model evaluation",
    "section": "Alternative approach for R-squared",
    "text": "Alternative approach for R-squared\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(nc_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.243         0.235  2.09      31.5 0.000000188     1  -214.  435.  443.\n# … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\nglance(nc_fit)$r.squared\n\n[1] 0.2430694"
  },
  {
    "objectID": "slides/lec-4.html#rmse",
    "href": "slides/lec-4.html#rmse",
    "title": "SLR: Prediction + model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the outcome variable\nCalculate with rmse():\n\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        2.07\n\n\nThe value of RMSE is not very meaningful on its own, but it’s useful for comparing across models (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/lec-4.html#obtaining-r-squared-and-rmse",
    "href": "slides/lec-4.html#obtaining-r-squared-and-rmse",
    "title": "SLR: Prediction + model evaluation",
    "section": "Obtaining R-squared and RMSE",
    "text": "Obtaining R-squared and RMSE\n\nUse rsq() and rmse(), respectively\n\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n\nFirst argument: data frame containing truth and estimate columns\nSecond argument: name of the column containing truth (observed outcome)\nThird argument: name of the column containing estimate (predicted outcome)"
  },
  {
    "objectID": "slides/lec-4.html#purpose-of-model-evaluation",
    "href": "slides/lec-4.html#purpose-of-model-evaluation",
    "title": "SLR: Prediction + model evaluation",
    "section": "Purpose of model evaluation",
    "text": "Purpose of model evaluation\n\n\\(R^2\\) tells us how our model is doing to predict the data we already have\nBut generally we are interested in prediction for a new observation, not for one that is already in our sample, i.e. out-of-sample prediction\nWe have a couple ways of simulating out-of-sample prediction before actually getting new data to evaluate the performance of our models"
  },
  {
    "objectID": "slides/lec-4.html#spending-our-data",
    "href": "slides/lec-4.html#spending-our-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Spending our data",
    "text": "Spending our data\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available leaves us with no other data to assess our choices\nWe can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we’ve done so far)"
  },
  {
    "objectID": "slides/lec-4.html#simulation-data-splitting",
    "href": "slides/lec-4.html#simulation-data-splitting",
    "title": "SLR: Prediction + model evaluation",
    "section": "Simulation: data splitting",
    "text": "Simulation: data splitting\n\n\n\n\nTake a random sample of 10% of the data and set aside (testing data)\nFit a model on the remaining 90% of the data (training data)\nUse the coefficients from this model to make predictions for the testing data\nRepeat 10 times"
  },
  {
    "objectID": "slides/lec-4.html#predictive-performance",
    "href": "slides/lec-4.html#predictive-performance",
    "title": "SLR: Prediction + model evaluation",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\n\nHow consistent are the predictions for different testing datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs. in the edges?"
  },
  {
    "objectID": "slides/lec-4.html#bootstrapping-our-data",
    "href": "slides/lec-4.html#bootstrapping-our-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Bootstrapping our data",
    "text": "Bootstrapping our data\n\nThe idea behind bootstrapping is that if a given observation exists in a sample, there may be more like it in the population\nWith bootstrapping, we simulate resampling from the population by resampling from the sample we observed\nBootstrap samples are the sampled with replacement from the original sample and same size as the original sample\n\nFor example, if our sample consists of the observations {A, B, C}, bootstrap samples could be {A, A, B}, {A, C, A}, {B, C, C}, {A, B, C}, etc."
  },
  {
    "objectID": "slides/lec-4.html#simulation-bootstrapping",
    "href": "slides/lec-4.html#simulation-bootstrapping",
    "title": "SLR: Prediction + model evaluation",
    "section": "Simulation: bootstrapping",
    "text": "Simulation: bootstrapping\n\n\n\n\nTake a bootstrap sample – sample with replacement from the original data, same size as the original data\nFit model to the sample and make predictions for that sample\nRepeat many times"
  },
  {
    "objectID": "slides/lec-4.html#predictive-performance-1",
    "href": "slides/lec-4.html#predictive-performance-1",
    "title": "SLR: Prediction + model evaluation",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\n\nHow consistent are the predictions for different bootstrap datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs. in the edges?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-26.html#topics",
    "href": "slides/lec-26.html#topics",
    "title": "Data Wrangling",
    "section": "Topics",
    "text": "Topics\n\nData Cleaning/Wrangling\nCourse Evaluation (10-15min)"
  },
  {
    "objectID": "slides/lec-26.html#project-peer-review-20-min",
    "href": "slides/lec-26.html#project-peer-review-20-min",
    "title": "Data Cleaning",
    "section": "Project Peer Review (20 min)",
    "text": "Project Peer Review (20 min)"
  },
  {
    "objectID": "slides/lec-26.html#data-cleaning-using-tidyverse",
    "href": "slides/lec-26.html#data-cleaning-using-tidyverse",
    "title": "Data Wrangling",
    "section": "Data Cleaning using tidyverse",
    "text": "Data Cleaning using tidyverse\n\nRaw data to understanding, insight, and knowledge\nWorkflow for real-world data analysis"
  },
  {
    "objectID": "slides/lec-26.html#packages-in-tidyverse",
    "href": "slides/lec-26.html#packages-in-tidyverse",
    "title": "Data Wrangling",
    "section": "Packages in Tidyverse",
    "text": "Packages in Tidyverse"
  },
  {
    "objectID": "slides/lec-26.html#focus-on-data-wrangling",
    "href": "slides/lec-26.html#focus-on-data-wrangling",
    "title": "Data Wrangling",
    "section": "Focus on data wrangling",
    "text": "Focus on data wrangling\n\n\nData import (readr, tibble)\nTidy data (tidyr)\nWrangle (dplyr, stringr, lubridate,janitor)\n\n\n\nlibrary(tidyverse)\nlibrary(cowplot)"
  },
  {
    "objectID": "slides/lec-26.html#data-import-using-readr",
    "href": "slides/lec-26.html#data-import-using-readr",
    "title": "Data Wrangling",
    "section": "Data import using readr",
    "text": "Data import using readr\n\nread_csv, …"
  },
  {
    "objectID": "slides/lec-26.html#extract-the-certain-type-of-data",
    "href": "slides/lec-26.html#extract-the-certain-type-of-data",
    "title": "Data Wrangling",
    "section": "Extract the certain type of data",
    "text": "Extract the certain type of data\nreadr::parse_*: parse the characters/numbers only"
  },
  {
    "objectID": "slides/lec-26.html#function-parse-in-pkg-readr",
    "href": "slides/lec-26.html#function-parse-in-pkg-readr",
    "title": "Data Wrangling",
    "section": "function parse in pkg readr",
    "text": "function parse in pkg readr\n\nparse_number(\"$100\")\n\n[1] 100\n\nparse_number(\"20%\")\n\n[1] 20\n\nparse_number(\"It cost $123.45\")\n\n[1] 123.45\n\n# Used in America\nparse_number(\"$123,456,789\")\n\n[1] 123456789\n\n# Used in many parts of Europe\nparse_number(\"123.456.789\", locale = locale(grouping_mark = \".\"))\n\n[1] 123456789\n\n# Used in Switzerland\nparse_number(\"123'456'789\", locale = locale(grouping_mark = \"'\"))\n\n[1] 123456789"
  },
  {
    "objectID": "slides/lec-26.html#function-parse-in-pkg-readr-1",
    "href": "slides/lec-26.html#function-parse-in-pkg-readr-1",
    "title": "Data Wrangling",
    "section": "function parse in pkg readr",
    "text": "function parse in pkg readr\n\nx1 <- \"El Ni\\xf1o was particularly bad this year\"\nx2 <- \"\\x82\\xb1\\x82\\xf1\\x82\\xc9\\x82\\xbf\\x82\\xcd\"\n\nx1\n\n[1] \"El Ni\\xf1o was particularly bad this year\"\n\nx2\n\n[1] \"\\x82\\xb1\\x82\\xf1\\x82ɂ\\xbf\\x82\\xcd\"\n\nparse_character(x1, locale = locale(encoding = \"Latin1\"))\n\n[1] \"El Niño was particularly bad this year\"\n\nparse_character(x2, locale = locale(encoding = \"Shift-JIS\"))\n\n[1] \"こんにちは\""
  },
  {
    "objectID": "slides/lec-26.html#function-parse-in-pkg-readr-2",
    "href": "slides/lec-26.html#function-parse-in-pkg-readr-2",
    "title": "Data Wrangling",
    "section": "function parse in pkg readr",
    "text": "function parse in pkg readr\n\nparse_date(\"1 janvier 2015\", \"%d %B %Y\", locale = locale(\"fr\"))\n\n[1] \"2015-01-01\"\n\nparse_date(\"01/02/15\", \"%m/%d/%y\")\n\n[1] \"2015-01-02\"\n\nparse_date(\"01/02/15\", \"%d/%m/%y\")\n\n[1] \"2015-02-01\"\n\nparse_date(\"01/02/15\", \"%y/%m/%d\")\n\n[1] \"2001-02-15\"\n\n\n\nparse_datetime(\"2010-10-01T2010\")\n\n[1] \"2010-10-01 20:10:00 UTC\"\n\nlibrary(hms)\nparse_time(\"01:10 am\")\n\n01:10:00"
  },
  {
    "objectID": "slides/lec-26.html#other-packages-for-data-importing",
    "href": "slides/lec-26.html#other-packages-for-data-importing",
    "title": "Data Wrangling",
    "section": "Other packages for data importing",
    "text": "Other packages for data importing\n\nPackage haven: SPSS, Stata, SAS file\nPackage readxl: Excel file .xls, .xlsx\nPackage jsonlite/htmltab: json, html\nuse as_tibble to coerce a data frame to a tibble"
  },
  {
    "objectID": "slides/lec-26.html#janitor-package-can-help-with-cleaning-names",
    "href": "slides/lec-26.html#janitor-package-can-help-with-cleaning-names",
    "title": "Data Wrangling",
    "section": "janitor package can help with cleaning names",
    "text": "janitor package can help with cleaning names\n\nclean_names,remove_empty_cols,remove_empty_rows"
  },
  {
    "objectID": "slides/lec-26.html#janitor-package",
    "href": "slides/lec-26.html#janitor-package",
    "title": "Data Wrangling",
    "section": "janitor package",
    "text": "janitor package\n\nclean_names,remove_empty_cols,remove_empty_rows"
  },
  {
    "objectID": "slides/lec-26.html#tidy-data-1",
    "href": "slides/lec-26.html#tidy-data-1",
    "title": "Data Wrangling",
    "section": "Tidy data",
    "text": "Tidy data"
  },
  {
    "objectID": "slides/lec-26.html#tidy-data-2",
    "href": "slides/lec-26.html#tidy-data-2",
    "title": "Data Wrangling",
    "section": "Tidy data",
    "text": "Tidy data"
  },
  {
    "objectID": "slides/lec-26.html#tidy-data-3",
    "href": "slides/lec-26.html#tidy-data-3",
    "title": "Data Wrangling",
    "section": "Tidy data",
    "text": "Tidy data"
  },
  {
    "objectID": "slides/lec-26.html#tidy-data-4",
    "href": "slides/lec-26.html#tidy-data-4",
    "title": "Data Wrangling",
    "section": "Tidy data",
    "text": "Tidy data"
  },
  {
    "objectID": "slides/lec-26.html#tidy-data-5",
    "href": "slides/lec-26.html#tidy-data-5",
    "title": "Data Wrangling",
    "section": "Tidy data",
    "text": "Tidy data"
  },
  {
    "objectID": "slides/lec-26.html#tidy-data-6",
    "href": "slides/lec-26.html#tidy-data-6",
    "title": "Data Wrangling",
    "section": "Tidy data",
    "text": "Tidy data"
  },
  {
    "objectID": "slides/lec-26.html#pivot_longer-function-in-tidyr-pkg",
    "href": "slides/lec-26.html#pivot_longer-function-in-tidyr-pkg",
    "title": "Data Wrangling",
    "section": "pivot_longer function in tidyr pkg",
    "text": "pivot_longer function in tidyr pkg\n\npivot_longer: from wide to long\n\n\n\ntable4 %>% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")"
  },
  {
    "objectID": "slides/lec-26.html#unite-function-in-tidyr-pkg",
    "href": "slides/lec-26.html#unite-function-in-tidyr-pkg",
    "title": "Data Wrangling",
    "section": "unite function in tidyr pkg",
    "text": "unite function in tidyr pkg\n\nunite: from 2+ column to 1 column\n\n\n\ntable6 %>% \n  unite(\"year\", century, year, sep = \"\")"
  },
  {
    "objectID": "slides/lec-26.html#join-multiple-datasets",
    "href": "slides/lec-26.html#join-multiple-datasets",
    "title": "Data Wrangling",
    "section": "join multiple datasets",
    "text": "join multiple datasets\nR pkg nycflights13 provide multiple relational datasets:\n\nflights connects to planes via a single variable, tailnum.\nflights connects to airlines through the carrier variable.\nflights connects to airports in two ways: via the origin and dest variables.\nflights connects to weather via origin (the location), and year, month, day and hour (the time)"
  },
  {
    "objectID": "slides/lec-26.html#relational-datasets-in-nycflights13",
    "href": "slides/lec-26.html#relational-datasets-in-nycflights13",
    "title": "Data Wrangling",
    "section": "Relational datasets in nycflights13",
    "text": "Relational datasets in nycflights13"
  },
  {
    "objectID": "slides/lec-26.html#inner_join",
    "href": "slides/lec-26.html#inner_join",
    "title": "Data Wrangling",
    "section": "inner_join",
    "text": "inner_join\n\ninner_join(x,y,by=\"key\")"
  },
  {
    "objectID": "slides/lec-26.html#join",
    "href": "slides/lec-26.html#join",
    "title": "Data Wrangling",
    "section": "*_join",
    "text": "*_join\nleft_join, right_join, full_join"
  },
  {
    "objectID": "slides/lec-26.html#data-wrangling-with-dplyr",
    "href": "slides/lec-26.html#data-wrangling-with-dplyr",
    "title": "Data Wrangling",
    "section": "Data wrangling with dplyr",
    "text": "Data wrangling with dplyr\n\nslice(): pick rows using indexes\nfilter(): keep rows satisfying your condition\nselect(): select variables obtain a tibble\npull(): grab a column as a vector\nrelocate(): relocate a variable\narrange(): reorder rows\nrename(): rename a variable\nmutate(): add columns\ngroup_by()%>%summarize(): summary statistics for different groups\ncount(): count the frequency\ndistinct(): keep unique rows\nfunctions within mutate(): across(), if_else(), case_when()\nfunctions for selecting variables: starts_with(), ends_with(), contains(), matches(), everything()"
  },
  {
    "objectID": "slides/lec-26.html#case_when",
    "href": "slides/lec-26.html#case_when",
    "title": "Data Wrangling",
    "section": "case_when",
    "text": "case_when"
  },
  {
    "objectID": "slides/lec-26.html#practice",
    "href": "slides/lec-26.html#practice",
    "title": "Data Wrangling",
    "section": "Practice",
    "text": "Practice\n\nlibrary(\"palmerpenguins\")\n\n\nKeep the chinstrap and gentoo penguins, living in Dream and Biscoe Islands.\nGet first 100 observation\nOnly keep columns from species to flipper_length_mm, and sex and year\nRename sex as gender\nMove gender right after island, move numeric variables after factor variables\nAdd a new column to identify each observation\nTransfer island as character\nAdd a new variable called bill_ratio which is the ratio of bill length to bill depth\nObtain the mean and standard deviation of body mass of different species\nFor different species, obtain the mean of variables ending with mm\nProvide the distribution of different species of penguins living in different island across time"
  },
  {
    "objectID": "slides/lec-26.html#practice-1",
    "href": "slides/lec-26.html#practice-1",
    "title": "Data Wrangling",
    "section": "Practice",
    "text": "Practice\nTo penguins, add a new column size_bin that contains:\n\n“large” if body mass is greater than 4500 g\n“medium” if body mass is greater than 3000 g, and less than or equal to 4500 g\n“small” if body mass is less than or equal to 3000 g"
  },
  {
    "objectID": "slides/lec-26.html#deal-with-different-types-of-variables-1",
    "href": "slides/lec-26.html#deal-with-different-types-of-variables-1",
    "title": "Data Wrangling",
    "section": "Deal with different types of variables",
    "text": "Deal with different types of variables\n\nstringr for strings\nforcats for factors\nlubridate for dates and times"
  },
  {
    "objectID": "slides/lec-26.html#stringr-for-strings-1",
    "href": "slides/lec-26.html#stringr-for-strings-1",
    "title": "Data Wrangling",
    "section": "stringr for strings",
    "text": "stringr for strings\n\nCheatsheets for stringr"
  },
  {
    "objectID": "slides/lec-26.html#useful-functions-in-stringr",
    "href": "slides/lec-26.html#useful-functions-in-stringr",
    "title": "Data Wrangling",
    "section": "Useful functions in stringr",
    "text": "Useful functions in stringr\nIf your raw data has numbers as variable names, you may consider to add characters in front of it.\n\ntable4 %>% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n\nstr_c: join multiple strings to a single string\n\nstr_c(c(\"x\", \"y\", \"z\"), collapse = \", \")\n\n[1] \"x, y, z\"\n\nstr_c(\"x\",1:3) \n\n[1] \"x1\" \"x2\" \"x3\""
  },
  {
    "objectID": "slides/lec-26.html#useful-functions-in-stringr-1",
    "href": "slides/lec-26.html#useful-functions-in-stringr-1",
    "title": "Data Wrangling",
    "section": "Useful functions in stringr",
    "text": "Useful functions in stringr\nTo obtain nice variable names. - Can use janitor::clean_names - Or str_to_upper, str_to_lower, str_to_title\n\n\ncolnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\ncolnames(penguins)%>%\n  str_to_title()\n\n[1] \"Species\"           \"Island\"            \"Bill_length_mm\"   \n[4] \"Bill_depth_mm\"     \"Flipper_length_mm\" \"Body_mass_g\"      \n[7] \"Sex\"               \"Year\""
  },
  {
    "objectID": "slides/lec-26.html#useful-functions-in-stringr-2",
    "href": "slides/lec-26.html#useful-functions-in-stringr-2",
    "title": "Data Wrangling",
    "section": "Useful functions in stringr",
    "text": "Useful functions in stringr\n\nstr_detect(): Return TRUE/FALSE for strings satisfying the pattern\npattern: regular expressions (CheatSheets)\nstr_replace, str_replace_all(multiple replacements)\nText mining: Useful if your have text in the survey, and you want to extract important features from text\n\n\n\nx <- c(\"apple\", \"banana\", \"pear\")\nstr_detect(x, \"[aeiou]$\")\n\n[1]  TRUE  TRUE FALSE\n\nstr_replace(x, \"[aeiou]\", \"-\")\n\n[1] \"-pple\"  \"b-nana\" \"p-ar\"  \n\nx <- c(\"1 house\", \"2 cars\", \"3 people\")\nstr_replace_all(x, c(\"1\" = \"one\", \"2\" = \"two\", \"3\" = \"three\"))\n\n[1] \"one house\"    \"two cars\"     \"three people\""
  },
  {
    "objectID": "slides/lec-26.html#forcats-for-factors-1",
    "href": "slides/lec-26.html#forcats-for-factors-1",
    "title": "Data Wrangling",
    "section": "forcats for factors",
    "text": "forcats for factors\n\nfct_infreq: order levels by frequency\nfct_rev: reverse the order of levels\nfct_reorder,fct_reorder2: order according to other variables\nfct_relevel: reorder manually\nfct_collapse: combine levels\nUseful for visualization in ggplot."
  },
  {
    "objectID": "slides/lec-26.html#fct_infreq-and-fct_rev",
    "href": "slides/lec-26.html#fct_infreq-and-fct_rev",
    "title": "Data Wrangling",
    "section": "fct_infreq and fct_rev",
    "text": "fct_infreq and fct_rev\n\np1=penguins %>%\n  ggplot(aes(species)) + geom_bar()\np2=penguins %>%\n  mutate(species=species %>% fct_infreq() %>% fct_rev()) %>%\n  ggplot(aes(species)) + geom_bar()\nplot_grid(p1,p2)"
  },
  {
    "objectID": "slides/lec-26.html#examples-for-fct_reorder",
    "href": "slides/lec-26.html#examples-for-fct_reorder",
    "title": "Data Wrangling",
    "section": "Examples for fct_reorder",
    "text": "Examples for fct_reorder\n\nlibrary(cowplot)\npeng_summary=penguins %>%\n  group_by(species)%>%\n  summarise(\n    bill_length_mean=mean(bill_length_mm, na.rm=T),\n    bill_depth_mean=mean(bill_depth_mm, na.rm=T))\np1=ggplot(peng_summary,aes(bill_length_mean,species)) + \n  geom_point()\np2=ggplot(peng_summary,aes(bill_length_mean,fct_reorder(species,bill_length_mean))) + geom_point()\nplot_grid(p1,p2)"
  },
  {
    "objectID": "slides/lec-26.html#examples-for-fct_reorder2",
    "href": "slides/lec-26.html#examples-for-fct_reorder2",
    "title": "Data Wrangling",
    "section": "Examples for fct_reorder2",
    "text": "Examples for fct_reorder2\n\nReorder the factor by the y values associated with the largest x values\nEasier to read: colours line up with the legend\n\n\n\np1=ggplot(penguins,aes(bill_length_mm,flipper_length_mm,color=species)) + \n  geom_line()\np2=ggplot(penguins,aes(bill_length_mm,flipper_length_mm,color=fct_reorder2(species,bill_length_mm,flipper_length_mm))) + \n  geom_line() + guides(color=guide_legend(title=\"species\"))\nplot_grid(p1,p2)"
  },
  {
    "objectID": "slides/lec-26.html#fct_recode",
    "href": "slides/lec-26.html#fct_recode",
    "title": "Data Wrangling",
    "section": "fct_recode()",
    "text": "fct_recode()\n\npenguins %>%\n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\npenguins %>%\n  mutate(species = fct_recode(species,\n                              \"Adeliea\" = \"Adelie\",\n                               \"ChinChin\" = \"Chinstrap\",\n                               \"Gentooman\" = \"Gentoo\")) %>%\n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adeliea     152\n2 ChinChin     68\n3 Gentooman   124"
  },
  {
    "objectID": "slides/lec-26.html#fct_collapse",
    "href": "slides/lec-26.html#fct_collapse",
    "title": "Data Wrangling",
    "section": "fct_collapse",
    "text": "fct_collapse\nCan use to collapse lots of levels\n\npenguins %>% \n  mutate(species = fct_collapse(species,\n                                \"Adelie\"=\"Adelie\",\n                                \"Other\"=c(\"Chinstrap\",\"Gentoo\")))%>%\n  count(species)\n\n# A tibble: 2 × 2\n  species     n\n  <fct>   <int>\n1 Adelie    152\n2 Other     192"
  },
  {
    "objectID": "slides/lec-26.html#fct_lump",
    "href": "slides/lec-26.html#fct_lump",
    "title": "Data Wrangling",
    "section": "fct_lump",
    "text": "fct_lump\n\nvolcano <- read_csv(here::here(\"slides\", \"data/volcano.csv\"))\nvolcano %>% count(country) %>% nrow()\n\n[1] 89\n\nvolcano %>%\n  mutate(country=fct_lump(country,n=5))%>%\n  count(country)\n\n# A tibble: 6 × 2\n  country           n\n  <fct>         <int>\n1 Chile            43\n2 Indonesia        95\n3 Japan            92\n4 Russia           79\n5 United States    99\n6 Other           550"
  },
  {
    "objectID": "slides/lec-26.html#lubridate-for-dates-and-times-1",
    "href": "slides/lec-26.html#lubridate-for-dates-and-times-1",
    "title": "Data Wrangling",
    "section": "lubridate for dates and times",
    "text": "lubridate for dates and times\n\nlibrary(\"lubridate\")\nlibrary(nycflights13)"
  },
  {
    "objectID": "slides/lec-26.html#reference",
    "href": "slides/lec-26.html#reference",
    "title": "Data Wrangling",
    "section": "Reference",
    "text": "Reference\n\nAllison Horst’s Posts\nJulie Scholler’s Slides\nR for Data Science\nGina Reynolds’s Slides\nSharla Gelfand’s Slides\nDavid’s Blog\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-27.html#topics",
    "href": "slides/lec-27.html#topics",
    "title": "Exam 3 review",
    "section": "Topics",
    "text": "Topics\n\n\nExam 3 Review\nCourse Evaluation\nPeer Review"
  },
  {
    "objectID": "slides/lec-27.html#exam-instructions",
    "href": "slides/lec-27.html#exam-instructions",
    "title": "Exam 3 review",
    "section": "Exam instructions",
    "text": "Exam instructions\n\nThe exam is an individual assignment. Everything in your repository is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor. For example, you may not communicate with other students, the TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nIf you have questions, email me."
  },
  {
    "objectID": "slides/lec-27.html#exam-coverage-and-format",
    "href": "slides/lec-27.html#exam-coverage-and-format",
    "title": "Exam 3 review",
    "section": "Exam coverage and format",
    "text": "Exam coverage and format\n\nFocuses on content after exam 2, but can include material from previous weeks\nSimilar format as previous exams\n\nPart 1: Multiple choice/fill-in-the-blank questions on Sakai\nPart 2: Open-ended data analysis in GitHub and submitted on Gradescope"
  },
  {
    "objectID": "slides/lec-27.html#part-2-of-the-exam",
    "href": "slides/lec-27.html#part-2-of-the-exam",
    "title": "Exam 3 review",
    "section": "Part 2 of the exam",
    "text": "Part 2 of the exam\n\nGoal: Assess your understanding of the course material and how the methods you learned are applied to the analysis of real-world data.\nInclude all of your analysis steps in your exam write up, unless stated otherwise.\n\nFor example, if the exam says “assume conditions are met,” You can reference that information in your write up but don’t have to recheck the conditions."
  },
  {
    "objectID": "slides/lec-27.html#assessment-criteria",
    "href": "slides/lec-27.html#assessment-criteria",
    "title": "Exam 3 review",
    "section": "Assessment criteria",
    "text": "Assessment criteria\n\nYou can identify the correct approach, analysis method, and/or inferential results required to answer the question.\nYou understand the correct conditions and diagnostics needed to determine whether the conclusions drawn from the model will be reliable\nYou can write results and conclusions in a meaningful way that can be understood by a general audience (think a business or research partner)\nYou can produce a report that is suitable for a professional audience (e.g., narrative is written in complete sentences, all graphs have proper titles and axis labels, there is not extraneous output, all Latex is rendered)\nYou can conduct the analysis using a reproducible data analysis workflow that incorporates version control"
  },
  {
    "objectID": "slides/lec-27.html#review-on-logistic-regression-and-multinomial-logistic-regression",
    "href": "slides/lec-27.html#review-on-logistic-regression-and-multinomial-logistic-regression",
    "title": "Exam 3 review",
    "section": "Review on logistic regression and Multinomial Logistic Regression",
    "text": "Review on logistic regression and Multinomial Logistic Regression\nAnalogize to a linear regression\n\nMotivation\nModel\nEstimation\nInterpretation\nInference\nComparison\nPrediction\nCondition\nIssue\nExtend to Multinomial logistic regression"
  },
  {
    "objectID": "slides/lec-27.html#application-exercise",
    "href": "slides/lec-27.html#application-exercise",
    "title": "Exam 3 review",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 github.com/STA210-Summer22/ae-12-exam-3-review"
  },
  {
    "objectID": "slides/lec-27.html#course-evaluation",
    "href": "slides/lec-27.html#course-evaluation",
    "title": "Exam 3 review",
    "section": "Course Evaluation",
    "text": "Course Evaluation\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-1.html#meet-the-instructor",
    "href": "slides/lec-1.html#meet-the-instructor",
    "title": "Welcome to STA 210!",
    "section": "Meet the instructor",
    "text": "Meet the instructor\n\n\n\n\n\nYunran Chen (she/her)\n\n\n\n\nThird-year Ph.D. student, Department of Statistical Science\nFind out more at my personal website\nPecan, One-year-old male Bernese Mountain Dog"
  },
  {
    "objectID": "slides/lec-1.html#meet-the-ta",
    "href": "slides/lec-1.html#meet-the-ta",
    "title": "Welcome to STA 210!",
    "section": "Meet the TA",
    "text": "Meet the TA\n\nJoseph Ekpenyong\nHold office hour and grade labs + HWs"
  },
  {
    "objectID": "slides/lec-1.html#meet-each-other",
    "href": "slides/lec-1.html#meet-each-other",
    "title": "Welcome to STA 210!",
    "section": "Meet each other!",
    "text": "Meet each other!\n\nName, year, major, hometown\nAny pets or favorite movie star?\nWhat do you hope to get out of this course?\nAnything else you want to share/ask?\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-1.html#check-out-conversations",
    "href": "slides/lec-1.html#check-out-conversations",
    "title": "Welcome to STA 210!",
    "section": "Check out Conversations",
    "text": "Check out Conversations\n\nGo to Conversations💬\nAnswer the discussion question: What do you love most for summer (in one word) ?"
  },
  {
    "objectID": "slides/lec-1.html#what-is-regression-analysis",
    "href": "slides/lec-1.html#what-is-regression-analysis",
    "title": "Welcome to STA 210!",
    "section": "What is regression analysis",
    "text": "What is regression analysis\n\n\n“In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or predictors). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or ‘criterion variable’) changes when any one of the independent variables is varied, while the other independent variables are held fixed.”\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lec-1.html#course-faq",
    "href": "slides/lec-1.html#course-faq",
    "title": "Welcome to STA 210!",
    "section": "Course FAQ",
    "text": "Course FAQ\n\n\nWhat background is assumed for the course? Introductory statistics or probability course.\nWill we be doing computing? Yes. We will use R.\nWill we learn the mathematical theory of regression? Yes and No. The course is primarily focused on application; however, we will discuss some of the mathematics of simple linear regression. If you want to dive into more of the mathematics, I can introduce some mathematics during labs.\nWhat is expected course load? Super intense! 25 hr per week. Deadlines on Mon/Wed/Fri/Sun."
  },
  {
    "objectID": "slides/lec-1.html#course-learning-objectives",
    "href": "slides/lec-1.html#course-learning-objectives",
    "title": "Welcome to STA 210!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\n\nAnalyze real-world data to answer questions about multivariable relationships.\nFit and evaluate linear and logistic regression models.\nAssess whether a proposed model is appropriate and describe its limitations.\nUse Quarto to write reproducible reports and GitHub for version control and collaboration.\nCommunicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "slides/lec-1.html#examples-of-regression-in-practice",
    "href": "slides/lec-1.html#examples-of-regression-in-practice",
    "title": "Welcome to STA 210!",
    "section": "Examples of regression in practice",
    "text": "Examples of regression in practice\n\n\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\nHow FiveThirtyEight’s 2020 Presidential Forecast Works — And What’s Different Because Of COVID-19\nEffect of Forensic Evidence on Criminal Justice Case Processing\nWhy it’s so freaking hard to make a good COVID-19 model"
  },
  {
    "objectID": "slides/lec-1.html#homepage",
    "href": "slides/lec-1.html#homepage",
    "title": "Welcome to STA 210!",
    "section": "Homepage",
    "text": "Homepage\nyunranchen.github.io/STA210Summer/\n\nAll course materials\nLinks to Sakai, GitHub, RStudio containers, etc.\nLet’s take a tour!"
  },
  {
    "objectID": "slides/lec-1.html#course-toolkit",
    "href": "slides/lec-1.html#course-toolkit",
    "title": "Welcome to STA 210!",
    "section": "Course toolkit",
    "text": "Course toolkit\nAll linked from the course website:\n\nGitHub organization: github.com/STA210-Summer22\nRStudio containers: cmgr.oit.duke.edu/containers\nDiscussion forum: Conversations\nAssignment submission and feedback: Gradescope\n\n\n\n\n\n\n\nImportant\n\n\nReserve an RStudio Container (titled STA 210) before lab !"
  },
  {
    "objectID": "slides/lec-1.html#activities-prepare-participate-practice-perform",
    "href": "slides/lec-1.html#activities-prepare-participate-practice-perform",
    "title": "Welcome to STA 210!",
    "section": "Activities: Prepare, Participate, Practice, Perform",
    "text": "Activities: Prepare, Participate, Practice, Perform\n\nPrepare: Introduce new content and prepare for lectures by completing the readings (and sometimes watching the videos)\nParticipate: Attend and actively participate in lectures and labs, office hours, team meetings\nPractice: Practice applying statistical concepts and computing with application exercises during lecture, graded for completion\nPerform: Put together what you’ve learned to analyze real-world data\n\nLab assignments x 7 (team-based)\nHomework assignments x 5 (individual)\nThree take-home exams (individual)\nTerm project presented during the final exam period (team-based)"
  },
  {
    "objectID": "slides/lec-1.html#cadence",
    "href": "slides/lec-1.html#cadence",
    "title": "Welcome to STA 210!",
    "section": "Cadence",
    "text": "Cadence\n\n\nLabs, HWs, and AEs: Due on Mon/Wed/Fri/Sun 11:59pm.\nExams: Exam review Friday in class, exam posted Friday morning 9:00 am, due Monday 11:59pm.\nProject: Deadlines throughout the semester, with some lab and lecture time dedicated to working on them, and most work done in teams outside of class"
  },
  {
    "objectID": "slides/lec-1.html#teams",
    "href": "slides/lec-1.html#teams",
    "title": "Welcome to STA 210!",
    "section": "Teams",
    "text": "Teams\n\nTeam assignments\n\nAssigned by me (Weekly vs Whole semester ? )\nApplication exercises, labs, and project\nPeer evaluation during teamwork and after completion\n\nExpectations and roles\n\nEveryone is expected to contribute equal effort\nEveryone is expected to understand all code turned in\nIndividual contribution evaluated by peer evaluation, commits, etc."
  },
  {
    "objectID": "slides/lec-1.html#grading",
    "href": "slides/lec-1.html#grading",
    "title": "Welcome to STA 210!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nApplication exercises\n3%\n\n\nHomework\n35% (7% x 5)\n\n\nProject\n15%\n\n\nLab\n14% (2.5% x 6)\n\n\nExam 01\n10%\n\n\nExam 02\n10%\n\n\nExam 03\n10%\n\n\nTeamwork\n2%\n\n\n\nSee course syllabus for how the final letter grade will be determined."
  },
  {
    "objectID": "slides/lec-1.html#support",
    "href": "slides/lec-1.html#support",
    "title": "Welcome to STA 210!",
    "section": "Support",
    "text": "Support\n\nAttend office hours\nAsk and answer questions on the discussion forum\nReserve email for questions on personal matters and/or grades\nRead the course support page"
  },
  {
    "objectID": "slides/lec-1.html#announcements",
    "href": "slides/lec-1.html#announcements",
    "title": "Welcome to STA 210!",
    "section": "Announcements",
    "text": "Announcements\n\nPosted on Sakai (Announcements tool) and sent via email, be sure to check both regularly\nI’ll assume that you’ve read an announcement by the next “business” day\nGo to website to check what you need to do to prepare, practice, and perform"
  },
  {
    "objectID": "slides/lec-1.html#diversity-inclusion",
    "href": "slides/lec-1.html#diversity-inclusion",
    "title": "Welcome to STA 210!",
    "section": "Diversity + inclusion",
    "text": "Diversity + inclusion\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\n\nPlease let me know your preferred pronunciation of your name.\nPlease let me know your preferred pronouns.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me.\nI come from a different cultural background, and am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "slides/lec-1.html#accessibility",
    "href": "slides/lec-1.html#accessibility",
    "title": "Welcome to STA 210!",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments.\nI am committed to making all course materials accessible and I’m always learning how to do this better. If any course component is not accessible to you in any way, please don’t hesitate to let me know."
  },
  {
    "objectID": "slides/lec-1.html#covid-policies",
    "href": "slides/lec-1.html#covid-policies",
    "title": "Welcome to STA 210!",
    "section": "COVID policies",
    "text": "COVID policies\n\nWear a mask at all times!\nRead and follow university guidance"
  },
  {
    "objectID": "slides/lec-1.html#late-work-waivers-regrades-policy",
    "href": "slides/lec-1.html#late-work-waivers-regrades-policy",
    "title": "Welcome to STA 210!",
    "section": "Late work, waivers, regrades policy",
    "text": "Late work, waivers, regrades policy\n\nWe have policies!\nRead about them on the course syllabus and refer back to them when you need it"
  },
  {
    "objectID": "slides/lec-1.html#collaboration-policy",
    "href": "slides/lec-1.html#collaboration-policy",
    "title": "Welcome to STA 210!",
    "section": "Collaboration policy",
    "text": "Collaboration policy\n\nOnly work that is clearly assigned as team work should be completed collaboratively.\nHomeworks must be completed individually. You may not directly share answers / code with others, however you are welcome to discuss the problems in general and ask for advice.\nExams must be completed individually. You may not discuss any aspect of the exam with peers. If you have questions, post as private questions on the course forum, only the teaching team will see and answer."
  },
  {
    "objectID": "slides/lec-1.html#sharing-reusing-code-policy",
    "href": "slides/lec-1.html#sharing-reusing-code-policy",
    "title": "Welcome to STA 210!",
    "section": "Sharing / reusing code policy",
    "text": "Sharing / reusing code policy\n\nWe are aware that a huge volume of code is available on the web, and many tasks may have solutions posted\nUnless explicitly stated otherwise, this course’s policy is that you may make use of any online resources (e.g. RStudio Community, StackOverflow, etc.) but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solution(s).\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source"
  },
  {
    "objectID": "slides/lec-1.html#academic-integrity",
    "href": "slides/lec-1.html#academic-integrity",
    "title": "Welcome to STA 210!",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nTo uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "slides/lec-1.html#most-importantly",
    "href": "slides/lec-1.html#most-importantly",
    "title": "Welcome to STA 210!",
    "section": "Most importantly!",
    "text": "Most importantly!\nAsk if you’re not sure if something violates a policy!"
  },
  {
    "objectID": "slides/lec-1.html#five-tips-for-success",
    "href": "slides/lec-1.html#five-tips-for-success",
    "title": "Welcome to STA 210!",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work before class.\nAsk questions.\nDo the readings.\nDo the homework and lab.\nDon’t procrastinate and don’t let a week pass by with lingering questions."
  },
  {
    "objectID": "slides/lec-1.html#learning-during-a-pandemic",
    "href": "slides/lec-1.html#learning-during-a-pandemic",
    "title": "Welcome to STA 210!",
    "section": "Learning during a pandemic",
    "text": "Learning during a pandemic\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don’t hesitate to ask.\n\n\nYou never owe me personal information about your health (mental or physical) but you’re always welcome to talk to me. If I can’t help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this crisis."
  },
  {
    "objectID": "slides/lec-1.html#this-weeks-tasks",
    "href": "slides/lec-1.html#this-weeks-tasks",
    "title": "Welcome to STA 210!",
    "section": "This week’s tasks",
    "text": "This week’s tasks\n\nGet a GitHub account if you don’t have one (some advice for choosing a username here)\nComplete the Getting to know you survey if you haven’t yet done so!\nRead the syllabus\nWatch out for announcement email"
  },
  {
    "objectID": "slides/lec-3.html#announcements",
    "href": "slides/lec-3.html#announcements",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Announcements",
    "text": "Announcements\n\nIf you’re just joining the class, welcome! Go to the course website and review content you’ve missed, read the syllabus, and complete the Getting to know you survey.\nLab 1 is due Friday, at 11:59pm, on Gradescope."
  },
  {
    "objectID": "slides/lec-3.html#recap-of-last-lecture",
    "href": "slides/lec-3.html#recap-of-last-lecture",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Recap of last lecture",
    "text": "Recap of last lecture\n\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative outcome variable.\nUsed the least squares method to estimate the slope and intercept.\nWe interpreted the slope and intercept.\n\n\nSlope: For every one unit increase in \\(x\\), we expect y to be higher/lower by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units.\n\n\nPredicted the response given a value of the predictor variable.\nDefined extrapolation and why we should avoid it."
  },
  {
    "objectID": "slides/lec-3.html#interested-in-the-math-behind-it-all",
    "href": "slides/lec-3.html#interested-in-the-math-behind-it-all",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Interested in the math behind it all?",
    "text": "Interested in the math behind it all?\nSee the supplemental notes on Deriving the Least-Squares Estimates for Simple Linear Regression for more mathematical details on the derivations of the estimates of \\(\\beta_0\\) and \\(\\beta_1\\)."
  },
  {
    "objectID": "slides/lec-3.html#outline",
    "href": "slides/lec-3.html#outline",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Outline",
    "text": "Outline\n\nUse tidymodels to fit and summarize regression models in R\nComplete an application exercise on exploratory data analysis and modeling"
  },
  {
    "objectID": "slides/lec-3.html#computational-setup",
    "href": "slides/lec-3.html#computational-setup",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(tidymodels)      # for modeling\nlibrary(fivethirtyeight) # for the fandango dataset\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/lec-3.html#movie-ratings",
    "href": "slides/lec-3.html#movie-ratings",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Movie ratings",
    "text": "Movie ratings\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/lec-3.html#data-prep",
    "href": "slides/lec-3.html#data-prep",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores <- fandango %>%\n  rename(\n    critics = rottentomatoes, \n    audience = rottentomatoes_user\n  )"
  },
  {
    "objectID": "slides/lec-3.html#data-visualization",
    "href": "slides/lec-3.html#data-visualization",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Data visualization",
    "text": "Data visualization"
  },
  {
    "objectID": "slides/lec-3.html#step-1-specify-model",
    "href": "slides/lec-3.html#step-1-specify-model",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-3.html#step-2-set-model-fitting-engine",
    "href": "slides/lec-3.html#step-2-set-model-fitting-engine",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\n# #| code-line-numbers: \"|2\"\n\nlinear_reg() %>%\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-3.html#step-3-fit-model-estimate-parameters",
    "href": "slides/lec-3.html#step-3-fit-model-estimate-parameters",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Step 3: Fit model & estimate parameters",
    "text": "Step 3: Fit model & estimate parameters\nusing formula syntax\n\n# #| code-line-numbers: \"|3\"\n\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(audience ~ critics, data = movie_scores)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = audience ~ critics, data = data)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187"
  },
  {
    "objectID": "slides/lec-3.html#a-closer-look-at-model-output",
    "href": "slides/lec-3.html#a-closer-look-at-model-output",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "A closer look at model output",
    "text": "A closer look at model output\n\nmovie_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(audience ~ critics, data = movie_scores)\n\nmovie_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = audience ~ critics, data = data)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187  \n\n\n\\[\\widehat{\\text{audience}} = 32.3155 + 0.5187 \\times \\text{critics}\\]\n\nNote: The intercept is off by a tiny bit from the hand-calculated intercept, this is likely just rounding error in the hand calculation."
  },
  {
    "objectID": "slides/lec-3.html#the-regression-output",
    "href": "slides/lec-3.html#the-regression-output",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "The regression output",
    "text": "The regression output\nWe’ll focus on the first column for now…\n\n# #| code-line-numbers: \"|4\"\n\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(audience ~ critics, data = movie_scores) %>%\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/lec-3.html#prediction",
    "href": "slides/lec-3.html#prediction",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Prediction",
    "text": "Prediction\n\n# #| code-line-numbers: \"|2|5\"\n\n# create a data frame for a new movie\nnew_movie <- tibble(critics = 50)\n\n# predict the outcome for a new movie\npredict(movie_fit, new_movie)\n\n# A tibble: 1 × 1\n  .pred\n  <dbl>\n1  58.2"
  },
  {
    "objectID": "slides/lec-25.html#topics",
    "href": "slides/lec-25.html#topics",
    "title": "MultiLR: Predictive models",
    "section": "Topics",
    "text": "Topics\n\n\nBuilding predictive multinomial logistic regression models\nComparing models\nUnbalanced data\nChoosing the “final” model"
  },
  {
    "objectID": "slides/lec-25.html#computational-setup",
    "href": "slides/lec-25.html#computational-setup",
    "title": "MultiLR: Predictive models",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(colorblindr)\nlibrary(themis)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/lec-25.html#terminology",
    "href": "slides/lec-25.html#terminology",
    "title": "MultiLR: Predictive models",
    "section": "Terminology",
    "text": "Terminology\nWhat’s the difference between regression and classification?\n\nLogistic regression / binary classification\nMultinomial logistic regression / multinomial classification"
  },
  {
    "objectID": "slides/lec-25.html#volcanoes",
    "href": "slides/lec-25.html#volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Volcanoes",
    "text": "Volcanoes\nThe data come from The Smithsonian Institution, via TidyTuesday.\n\nvolcano <- read_csv(here::here(\"slides\", \"data/volcano.csv\"))\nnames(volcano)\n\n [1] \"volcano_number\"           \"volcano_name\"            \n [3] \"primary_volcano_type\"     \"last_eruption_year\"      \n [5] \"country\"                  \"region\"                  \n [7] \"subregion\"                \"latitude\"                \n [9] \"longitude\"                \"elevation\"               \n[11] \"tectonic_settings\"        \"evidence_category\"       \n[13] \"major_rock_1\"             \"major_rock_2\"            \n[15] \"major_rock_3\"             \"major_rock_4\"            \n[17] \"major_rock_5\"             \"minor_rock_1\"            \n[19] \"minor_rock_2\"             \"minor_rock_3\"            \n[21] \"minor_rock_4\"             \"minor_rock_5\"            \n[23] \"population_within_5_km\"   \"population_within_10_km\" \n[25] \"population_within_30_km\"  \"population_within_100_km\""
  },
  {
    "objectID": "slides/lec-25.html#volcanoes-1",
    "href": "slides/lec-25.html#volcanoes-1",
    "title": "MultiLR: Predictive models",
    "section": "Volcanoes",
    "text": "Volcanoes\n\nglimpse(volcano)\n\nRows: 958\nColumns: 26\n$ volcano_number           <dbl> 283001, 355096, 342080, 213004, 321040, 28317…\n$ volcano_name             <chr> \"Abu\", \"Acamarachi\", \"Acatenango\", \"Acigol-Ne…\n$ primary_volcano_type     <chr> \"Shield(s)\", \"Stratovolcano\", \"Stratovolcano(…\n$ last_eruption_year       <chr> \"-6850\", \"Unknown\", \"1972\", \"-2080\", \"950\", \"…\n$ country                  <chr> \"Japan\", \"Chile\", \"Guatemala\", \"Turkey\", \"Uni…\n$ region                   <chr> \"Japan, Taiwan, Marianas\", \"South America\", \"…\n$ subregion                <chr> \"Honshu\", \"Northern Chile, Bolivia and Argent…\n$ latitude                 <dbl> 34.500, -23.292, 14.501, 38.537, 46.206, 37.6…\n$ longitude                <dbl> 131.600, -67.618, -90.876, 34.621, -121.490, …\n$ elevation                <dbl> 641, 6023, 3976, 1683, 3742, 1728, 1733, 1250…\n$ tectonic_settings        <chr> \"Subduction zone / Continental crust (>25 km)…\n$ evidence_category        <chr> \"Eruption Dated\", \"Evidence Credible\", \"Erupt…\n$ major_rock_1             <chr> \"Andesite / Basaltic Andesite\", \"Dacite\", \"An…\n$ major_rock_2             <chr> \"Basalt / Picro-Basalt\", \"Andesite / Basaltic…\n$ major_rock_3             <chr> \"Dacite\", \" \", \" \", \"Basalt / Picro-Basalt\", …\n$ major_rock_4             <chr> \" \", \" \", \" \", \"Andesite / Basaltic Andesite\"…\n$ major_rock_5             <chr> \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", …\n$ minor_rock_1             <chr> \" \", \" \", \"Basalt / Picro-Basalt\", \" \", \"Daci…\n$ minor_rock_2             <chr> \" \", \" \", \" \", \" \", \" \", \"Basalt / Picro-Basa…\n$ minor_rock_3             <chr> \" \", \" \", \" \", \" \", \" \", \" \", \" \", \"Andesite …\n$ minor_rock_4             <chr> \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", …\n$ minor_rock_5             <chr> \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", …\n$ population_within_5_km   <dbl> 3597, 0, 4329, 127863, 0, 428, 101, 51, 0, 98…\n$ population_within_10_km  <dbl> 9594, 7, 60730, 127863, 70, 3936, 485, 6042, …\n$ population_within_30_km  <dbl> 117805, 294, 1042836, 218469, 4019, 717078, 1…\n$ population_within_100_km <dbl> 4071152, 9092, 7634778, 2253483, 393303, 5024…"
  },
  {
    "objectID": "slides/lec-25.html#types-of-volcanoes",
    "href": "slides/lec-25.html#types-of-volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Types of volcanoes",
    "text": "Types of volcanoes\nProbably too many types!\n\nvolcano %>%\n  count(primary_volcano_type, sort = TRUE) %>%\n  print(n = 26)\n\n# A tibble: 26 × 2\n   primary_volcano_type     n\n   <chr>                <int>\n 1 Stratovolcano          353\n 2 Stratovolcano(es)      107\n 3 Shield                  85\n 4 Volcanic field          71\n 5 Pyroclastic cone(s)     70\n 6 Caldera                 65\n 7 Complex                 46\n 8 Shield(s)               33\n 9 Submarine               27\n10 Lava dome(s)            26\n11 Fissure vent(s)         12\n12 Caldera(s)               9\n13 Compound                 9\n14 Maar(s)                  8\n15 Pyroclastic shield       7\n16 Tuff cone(s)             7\n17 Crater rows              5\n18 Subglacial               5\n19 Pyroclastic cone         4\n20 Lava dome                3\n21 Complex(es)              1\n22 Lava cone                1\n23 Lava cone(es)            1\n24 Lava cone(s)             1\n25 Stratovolcano?           1\n26 Tuff cone                1"
  },
  {
    "objectID": "slides/lec-25.html#relevel-volcanoes",
    "href": "slides/lec-25.html#relevel-volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Relevel volcanoes",
    "text": "Relevel volcanoes\n\nvolcano <- volcano %>%\n  mutate(\n    volcano_type = case_when(\n      str_detect(primary_volcano_type, \"Stratovolcano\") ~ \"Stratovolcano\",\n      str_detect(primary_volcano_type, \"Shield\") ~ \"Shield\",\n      TRUE ~ \"Other\"\n    ),\n    volcano_type = fct_relevel(volcano_type, \"Stratovolcano\", \"Shield\", \"Other\")\n  )\n\nvolcano %>%\n  count(volcano_type)\n\n# A tibble: 3 × 2\n  volcano_type      n\n  <fct>         <int>\n1 Stratovolcano   461\n2 Shield          118\n3 Other           379"
  },
  {
    "objectID": "slides/lec-25.html#data-prep",
    "href": "slides/lec-25.html#data-prep",
    "title": "MultiLR: Predictive models",
    "section": "Data prep",
    "text": "Data prep\n\nSelect a few variables as predictors for the model with\nConvert all character variables to factors\n\n\n\nvolcano <- volcano %>%\n  select(\n    volcano_type, latitude, longitude, \n    elevation, tectonic_settings, major_rock_1\n    ) %>%\n  mutate(across(where(is.character), as_factor))"
  },
  {
    "objectID": "slides/lec-25.html#mapping-the-volcanoes",
    "href": "slides/lec-25.html#mapping-the-volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Mapping the volcanoes",
    "text": "Mapping the volcanoes"
  },
  {
    "objectID": "slides/lec-25.html#world-map-data",
    "href": "slides/lec-25.html#world-map-data",
    "title": "MultiLR: Predictive models",
    "section": "World map data",
    "text": "World map data\n\nworld <- map_data(\"world\")\n\nworld %>% as_tibble()\n\n# A tibble: 99,338 × 6\n    long   lat group order region subregion\n   <dbl> <dbl> <dbl> <int> <chr>  <chr>    \n 1 -69.9  12.5     1     1 Aruba  <NA>     \n 2 -69.9  12.4     1     2 Aruba  <NA>     \n 3 -69.9  12.4     1     3 Aruba  <NA>     \n 4 -70.0  12.5     1     4 Aruba  <NA>     \n 5 -70.1  12.5     1     5 Aruba  <NA>     \n 6 -70.1  12.6     1     6 Aruba  <NA>     \n 7 -70.0  12.6     1     7 Aruba  <NA>     \n 8 -70.0  12.6     1     8 Aruba  <NA>     \n 9 -69.9  12.5     1     9 Aruba  <NA>     \n10 -69.9  12.5     1    10 Aruba  <NA>     \n# … with 99,328 more rows"
  },
  {
    "objectID": "slides/lec-25.html#draw-world-map",
    "href": "slides/lec-25.html#draw-world-map",
    "title": "MultiLR: Predictive models",
    "section": "Draw world map",
    "text": "Draw world map\n\n\nworld_map <- ggplot() +\n  geom_polygon(\n    data = world, \n    aes(\n      x = long, y = lat, group = group),\n      color = \"white\", fill = \"gray50\", \n      size = 0.05, alpha = 0.2\n    ) +\n  theme_minimal() +\n  coord_quickmap() +\n  labs(x = NULL, y = NULL)\n\nworld_map"
  },
  {
    "objectID": "slides/lec-25.html#add-volcanoes",
    "href": "slides/lec-25.html#add-volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Add volcanoes",
    "text": "Add volcanoes\n\n\nworld_map +\n  geom_point(\n    data = volcano,\n    aes(\n      x = longitude, y = latitude, \n      color = volcano_type, \n      shape = volcano_type),\n    alpha = 0.5\n  ) +\n  scale_color_OkabeIto() +\n  labs(color = NULL, shape = NULL)"
  },
  {
    "objectID": "slides/lec-25.html#split-into-testingtraining",
    "href": "slides/lec-25.html#split-into-testingtraining",
    "title": "MultiLR: Predictive models",
    "section": "Split into testing/training",
    "text": "Split into testing/training\n\nset.seed(1234)\n\nvolcano_split <- initial_split(volcano)\nvolcano_train <- training(volcano_split)\nvolcano_test  <- testing(volcano_split)"
  },
  {
    "objectID": "slides/lec-25.html#create-a-recipe",
    "href": "slides/lec-25.html#create-a-recipe",
    "title": "MultiLR: Predictive models",
    "section": "Create a recipe",
    "text": "Create a recipe\nStart with a model that doesn’t use geographic information:\nstep_other creates a specification of a recipe step that will potentially pool infrequently occurring values into an “other” category.\n\nvolcano_rec1 <- recipe(volcano_type ~ ., data = volcano_train) %>%\n  step_rm(latitude, longitude) %>%\n  step_other(tectonic_settings) %>%\n  step_other(major_rock_1) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors()) %>%\n  step_center(all_predictors())"
  },
  {
    "objectID": "slides/lec-25.html#specify-a-model",
    "href": "slides/lec-25.html#specify-a-model",
    "title": "MultiLR: Predictive models",
    "section": "Specify a model",
    "text": "Specify a model\n\nvolcano_spec <- multinom_reg() %>%\n  set_engine(\"nnet\")\n\nvolcano_spec\n\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet"
  },
  {
    "objectID": "slides/lec-25.html#create-a-workflow",
    "href": "slides/lec-25.html#create-a-workflow",
    "title": "MultiLR: Predictive models",
    "section": "Create a workflow",
    "text": "Create a workflow\n\nvolcano_wflow1 <- workflow() %>%\n  add_recipe(volcano_rec1) %>%\n  add_model(volcano_spec)\n\nvolcano_wflow1\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_rm()\n• step_other()\n• step_other()\n• step_dummy()\n• step_zv()\n• step_center()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet"
  },
  {
    "objectID": "slides/lec-25.html#create-cross-validation-folds",
    "href": "slides/lec-25.html#create-cross-validation-folds",
    "title": "MultiLR: Predictive models",
    "section": "Create cross validation folds",
    "text": "Create cross validation folds\n\nset.seed(9876)\n\nvolcano_folds <- vfold_cv(volcano_train, v = 5)\nvolcano_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [574/144]> Fold1\n2 <split [574/144]> Fold2\n3 <split [574/144]> Fold3\n4 <split [575/143]> Fold4\n5 <split [575/143]> Fold5"
  },
  {
    "objectID": "slides/lec-25.html#fit-resamples",
    "href": "slides/lec-25.html#fit-resamples",
    "title": "MultiLR: Predictive models",
    "section": "Fit resamples",
    "text": "Fit resamples\n\nvolcano_fit_rs1 <- volcano_wflow1 %>%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )\n\nvolcano_fit_rs1\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 5\n  splits            id    .metrics         .notes           .predictions      \n  <list>            <chr> <list>           <list>           <list>            \n1 <split [574/144]> Fold1 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [144 × 7]>\n2 <split [574/144]> Fold2 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [144 × 7]>\n3 <split [574/144]> Fold3 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [144 × 7]>\n4 <split [575/143]> Fold4 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [143 × 7]>\n5 <split [575/143]> Fold5 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [143 × 7]>"
  },
  {
    "objectID": "slides/lec-25.html#collect-metrics",
    "href": "slides/lec-25.html#collect-metrics",
    "title": "MultiLR: Predictive models",
    "section": "Collect metrics",
    "text": "Collect metrics\n\ncollect_metrics(volcano_fit_rs1)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy multiclass 0.596     5  0.0146 Preprocessor1_Model1\n2 roc_auc  hand_till  0.703     5  0.0244 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/lec-25.html#roc-curve",
    "href": "slides/lec-25.html#roc-curve",
    "title": "MultiLR: Predictive models",
    "section": "ROC curve",
    "text": "ROC curve\nROC curves for multiclass outcomes use a one-vs-all approach: calculate multiple curves, one per level vs. all other levels.\n\nvolcano_fit_rs1 %>%\n  collect_predictions() %>%\n  group_by(id) %>%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %>%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-25.html#roc-curve---under-the-hood",
    "href": "slides/lec-25.html#roc-curve---under-the-hood",
    "title": "MultiLR: Predictive models",
    "section": "ROC curve - under the hood",
    "text": "ROC curve - under the hood\nAn additional column, .level, identifies the “one” column in the one-vs-all calculation:\n\nvolcano_fit_rs1 %>%\n  collect_predictions() %>%\n  group_by(id) %>%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  )%>%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-25.html#unbalanced-data-1",
    "href": "slides/lec-25.html#unbalanced-data-1",
    "title": "MultiLR: Predictive models",
    "section": "Unbalanced data",
    "text": "Unbalanced data\nRemember that the observed volcano types are unbalanced:\n\nvolcano %>% \n  count(volcano_type)\n\n# A tibble: 3 × 2\n  volcano_type      n\n  <fct>         <int>\n1 Stratovolcano   461\n2 Shield          118\n3 Other           379\n\n\nFor educational purpose, we consider some statistical tools to address this issue."
  },
  {
    "objectID": "slides/lec-25.html#addressing-unbalance",
    "href": "slides/lec-25.html#addressing-unbalance",
    "title": "MultiLR: Predictive models",
    "section": "Addressing unbalance",
    "text": "Addressing unbalance\nTo address class unbalance, we generally use\n\noversampling data from levels that are less prevalent in the data\n\ne.g., step_smote(): Uses a technique called “Synthetic Minority Over-sampling Technique” to generate new examples of the minority class using nearest neighbors of these cases.\n\ndownsampling data from levels that are more prevalent in the data\n\ne.g., step_downsample(): Removes rows of a data set to make the occurrence of levels in a specific factor level equal."
  },
  {
    "objectID": "slides/lec-25.html#new-recipe---oversample",
    "href": "slides/lec-25.html#new-recipe---oversample",
    "title": "MultiLR: Predictive models",
    "section": "New recipe - oversample",
    "text": "New recipe - oversample\n\nvolcano_rec3 <- recipe(volcano_type ~ ., data = volcano_train) %>%\n  step_other(tectonic_settings) %>%\n  step_other(major_rock_1) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors()) %>%\n  step_center(all_predictors()) %>%\n  step_smote(volcano_type)"
  },
  {
    "objectID": "slides/lec-25.html#new-recipe---downsample",
    "href": "slides/lec-25.html#new-recipe---downsample",
    "title": "MultiLR: Predictive models",
    "section": "New recipe - downsample",
    "text": "New recipe - downsample\n\nvolcano_rec4 <- recipe(volcano_type ~ ., data = volcano_train) %>%\n  step_other(tectonic_settings) %>%\n  step_other(major_rock_1) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors()) %>%\n  step_center(all_predictors()) %>%\n  step_downsample(volcano_type)"
  },
  {
    "objectID": "slides/lec-25.html#new-workflows",
    "href": "slides/lec-25.html#new-workflows",
    "title": "MultiLR: Predictive models",
    "section": "New workflows",
    "text": "New workflows\n\nvolcano_wflow3 <- workflow() %>%\n  add_recipe(volcano_rec3) %>%\n  add_model(volcano_spec)\n\nvolcano_wflow4 <- workflow() %>%\n  add_recipe(volcano_rec4) %>%\n  add_model(volcano_spec)"
  },
  {
    "objectID": "slides/lec-25.html#fit-resamples-1",
    "href": "slides/lec-25.html#fit-resamples-1",
    "title": "MultiLR: Predictive models",
    "section": "Fit resamples",
    "text": "Fit resamples\n\nvolcano_fit_rs3 <- volcano_wflow3 %>%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )\n\nvolcano_fit_rs4 <- volcano_wflow4 %>%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )"
  },
  {
    "objectID": "slides/lec-25.html#collect-metrics-1",
    "href": "slides/lec-25.html#collect-metrics-1",
    "title": "MultiLR: Predictive models",
    "section": "Collect metrics",
    "text": "Collect metrics\n\ncollect_metrics(volcano_fit_rs3)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy multiclass 0.510     5  0.0169 Preprocessor1_Model1\n2 roc_auc  hand_till  0.693     5  0.0243 Preprocessor1_Model1\n\ncollect_metrics(volcano_fit_rs4)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy multiclass 0.504     5  0.0264 Preprocessor1_Model1\n2 roc_auc  hand_till  0.669     5  0.0147 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/lec-25.html#roc-curves---oversampling",
    "href": "slides/lec-25.html#roc-curves---oversampling",
    "title": "MultiLR: Predictive models",
    "section": "ROC curves - oversampling",
    "text": "ROC curves - oversampling\n\nvolcano_fit_rs3 %>%\n  collect_predictions() %>%\n  group_by(id) %>%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %>%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-25.html#roc-curves---downsampling",
    "href": "slides/lec-25.html#roc-curves---downsampling",
    "title": "MultiLR: Predictive models",
    "section": "ROC curves - downsampling",
    "text": "ROC curves - downsampling\n\nvolcano_fit_rs4 %>%\n  collect_predictions() %>%\n  group_by(id) %>%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %>%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-25.html#addressing-unbalance-1",
    "href": "slides/lec-25.html#addressing-unbalance-1",
    "title": "MultiLR: Predictive models",
    "section": "Addressing unbalance",
    "text": "Addressing unbalance\n\nCan you think of any issues resulting from over/down sampling?"
  },
  {
    "objectID": "slides/lec-25.html#the-chosen-model",
    "href": "slides/lec-25.html#the-chosen-model",
    "title": "MultiLR: Predictive models",
    "section": "The “chosen” model",
    "text": "The “chosen” model\nLet’s stick to the models without over/down sampling.\nFrom the application exercise:\n\nvolcano_rec2 <- recipe(volcano_type ~ ., data = volcano_train) %>%\n  step_other(tectonic_settings) %>%\n  step_other(major_rock_1) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors()) %>%\n  step_center(all_predictors())\n\nvolcano_wflow2 <- workflow() %>%\n  add_recipe(volcano_rec2) %>%\n  add_model(volcano_spec)"
  },
  {
    "objectID": "slides/lec-25.html#fitting-the-final-model",
    "href": "slides/lec-25.html#fitting-the-final-model",
    "title": "MultiLR: Predictive models",
    "section": "Fitting the final model",
    "text": "Fitting the final model\n\nfinal_fit <- last_fit(\n  volcano_wflow2, \n  split = volcano_split\n  )\n\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy multiclass     0.629 Preprocessor1_Model1\n2 roc_auc  hand_till      0.734 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/lec-25.html#confusion-matrix",
    "href": "slides/lec-25.html#confusion-matrix",
    "title": "MultiLR: Predictive models",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\ncollect_predictions(final_fit) %>%\n  conf_mat(volcano_type, .pred_class)\n\n               Truth\nPrediction      Stratovolcano Shield Other\n  Stratovolcano            96     13    38\n  Shield                    1      0     0\n  Other                    21     16    55"
  },
  {
    "objectID": "slides/lec-25.html#confusion-matrix---visualized",
    "href": "slides/lec-25.html#confusion-matrix---visualized",
    "title": "MultiLR: Predictive models",
    "section": "Confusion matrix - visualized",
    "text": "Confusion matrix - visualized\n\ncollect_predictions(final_fit) %>%\n  conf_mat(volcano_type, .pred_class) %>%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-25.html#roc-curve-1",
    "href": "slides/lec-25.html#roc-curve-1",
    "title": "MultiLR: Predictive models",
    "section": "ROC curve",
    "text": "ROC curve\n\ncollect_predictions(final_fit) %>%\n  roc_curve(truth = volcano_type, .pred_Stratovolcano:.pred_Other) %>%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-25.html#prediction",
    "href": "slides/lec-25.html#prediction",
    "title": "MultiLR: Predictive models",
    "section": "Prediction",
    "text": "Prediction\n\nfinal_fitted <- extract_workflow(final_fit)\n\nnew_volcano <- tibble(\n  latitude = 35.9940,\n  longitude = -78.8986,\n  elevation = 404,\n  tectonic_settings = \"Subduction zone / Continental crust (>25 km)\",\n  major_rock_1 = \"Andesite / Basaltic Andesite\"\n)\n\npredict(\n  final_fitted, \n  new_volcano, \n  type = \"prob\"\n  )\n\n# A tibble: 1 × 3\n  .pred_Stratovolcano .pred_Shield .pred_Other\n                <dbl>        <dbl>       <dbl>\n1               0.381       0.0379       0.581"
  },
  {
    "objectID": "slides/lec-25.html#acknowledgements",
    "href": "slides/lec-25.html#acknowledgements",
    "title": "MultiLR: Predictive models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nInspired by\n\n\nhttps://juliasilge.com/blog/multinomial-volcano-eruptions/\nhttps://juliasilge.com/blog/nber-papers/\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-19.html#announcements",
    "href": "slides/lec-19.html#announcements",
    "title": "Probabilities, odds, odds ratios",
    "section": "Announcements",
    "text": "Announcements\n\nExam 2 scores for part 1 are posted\nProject proposals + AE 9 due Wednesday, June 8, at 11:59pm"
  },
  {
    "objectID": "slides/lec-19.html#exam-2",
    "href": "slides/lec-19.html#exam-2",
    "title": "Probabilities, odds, odds ratios",
    "section": "Exam 2",
    "text": "Exam 2\n\nConceptual part\nApplied part"
  },
  {
    "objectID": "slides/lec-19.html#topics",
    "href": "slides/lec-19.html#topics",
    "title": "Probabilities, odds, odds ratios",
    "section": "Topics",
    "text": "Topics\n\n\nUse the odds ratio to compare the odds of two groups\nInterpret the coefficients of a logistic regression model with\n\na single categorical predictor\na single quantitative predictor\nmultiple predictors"
  },
  {
    "objectID": "slides/lec-19.html#computational-setup",
    "href": "slides/lec-19.html#computational-setup",
    "title": "Probabilities, odds, odds ratios",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-19.html#risk-of-coronary-heart-disease",
    "href": "slides/lec-19.html#risk-of-coronary-heart-disease",
    "title": "Probabilities, odds, odds ratios",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)\neducation: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College"
  },
  {
    "objectID": "slides/lec-19.html#high-risk-vs.-education",
    "href": "slides/lec-19.html#high-risk-vs.-education",
    "title": "Probabilities, odds, odds ratios",
    "section": "High risk vs. education",
    "text": "High risk vs. education\n\n\n\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/lec-19.html#compare-the-odds-for-two-groups",
    "href": "slides/lec-19.html#compare-the-odds-for-two-groups",
    "title": "Probabilities, odds, odds ratios",
    "section": "Compare the odds for two groups",
    "text": "Compare the odds for two groups\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\nWe want to compare the risk of heart disease for those with a High School diploma/GED and those with a college degree.\nWe’ll use the odds to compare the two groups\n\n\\[\n\\text{odds} = \\frac{P(\\text{success})}{P(\\text{failure})} = \\frac{\\text{# of successes}}{\\text{# of failures}}\n\\]"
  },
  {
    "objectID": "slides/lec-19.html#compare-the-odds-for-two-groups-1",
    "href": "slides/lec-19.html#compare-the-odds-for-two-groups-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Compare the odds for two groups",
    "text": "Compare the odds for two groups\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\nOdds of having high risk for the High school or GED group: \\(\\frac{147}{1106} = 0.133\\)\nOdds of having high risk for the College group: \\(\\frac{70}{403} = 0.174\\)\nBased on this, we see those with a college degree had higher odds of having high risk for heart disease than those with a high school diploma or GED."
  },
  {
    "objectID": "slides/lec-19.html#odds-ratio-or",
    "href": "slides/lec-19.html#odds-ratio-or",
    "title": "Probabilities, odds, odds ratios",
    "section": "Odds ratio (OR)",
    "text": "Odds ratio (OR)\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\nLet’s summarize the relationship between the two groups. To do so, we’ll use the odds ratio (OR).\n\\[\nOR = \\frac{\\text{odds}_1}{\\text{odds}_2} = \\frac{\\omega_1}{\\omega_2}\n\\]"
  },
  {
    "objectID": "slides/lec-19.html#or-college-vs.-high-school-or-ged",
    "href": "slides/lec-19.html#or-college-vs.-high-school-or-ged",
    "title": "Probabilities, odds, odds ratios",
    "section": "OR: College vs. High school or GED",
    "text": "OR: College vs. High school or GED\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\\[OR = \\frac{\\text{odds}_{College}}{\\text{odds}_{HS}} = \\frac{0.174}{0.133} = \\mathbf{1.308}\\]\n\nThe odds of having high risk for heart disease are 1.30 times higher for those with a college degree than those with a high school diploma or GED."
  },
  {
    "objectID": "slides/lec-19.html#or-college-vs.-some-high-school",
    "href": "slides/lec-19.html#or-college-vs.-some-high-school",
    "title": "Probabilities, odds, odds ratios",
    "section": "OR: College vs. Some high school",
    "text": "OR: College vs. Some high school\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\\[OR = \\frac{\\text{odds}_{College}}{\\text{odds}_{Some HS}} = \\frac{70/403}{323/1397} = 0.751\\]\n\nThe odds of having high risk for having heart disease for those with a college degree are 0.751 times the odds of having high risk for heart disease for those with some high school."
  },
  {
    "objectID": "slides/lec-19.html#more-natural-interpretation",
    "href": "slides/lec-19.html#more-natural-interpretation",
    "title": "Probabilities, odds, odds ratios",
    "section": "More natural interpretation",
    "text": "More natural interpretation\n\nIt’s more natural to interpret the odds ratio with a statement with the odds ratio greater than 1.\nThe odds of having high risk for heart disease are 1.33 times higher for those with some high school than those with a college degree."
  },
  {
    "objectID": "slides/lec-19.html#making-the-table-1",
    "href": "slides/lec-19.html#making-the-table-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Making the table 1",
    "text": "Making the table 1\nFirst, rename the levels of the categorical variables:\n\nheart_disease <- heart_disease %>%\n  mutate(\n    high_risk_names = if_else(high_risk == \"1\", \"High risk\", \"Not high risk\"),\n    education_names = case_when(\n      education == \"1\" ~ \"Some high school\",\n      education == \"2\" ~ \"High school or GED\",\n      education == \"3\" ~ \"Some college or vocational school\",\n      education == \"4\" ~ \"College\"\n    ),\n    education_names = fct_relevel(education_names, \"Some high school\", \"High school or GED\", \"Some college or vocational school\", \"College\")\n  )"
  },
  {
    "objectID": "slides/lec-19.html#making-the-table-2",
    "href": "slides/lec-19.html#making-the-table-2",
    "title": "Probabilities, odds, odds ratios",
    "section": "Making the table 2",
    "text": "Making the table 2\nThen, make the table:\n\nheart_disease %>%\n  count(education_names, high_risk_names) %>%\n  pivot_wider(names_from = high_risk_names, values_from = n) %>%\n  kable(col.names = c(\"Education\", \"High risk\", \"Not high risk\"))"
  },
  {
    "objectID": "slides/lec-19.html#deeper-look-into-the-code",
    "href": "slides/lec-19.html#deeper-look-into-the-code",
    "title": "Probabilities, odds, odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease %>%\n  count(education_names, high_risk_names)\n\n# A tibble: 8 × 3\n  education_names                   high_risk_names     n\n  <fct>                             <chr>           <int>\n1 Some high school                  High risk         323\n2 Some high school                  Not high risk    1397\n3 High school or GED                High risk         147\n4 High school or GED                Not high risk    1106\n5 Some college or vocational school High risk          88\n6 Some college or vocational school Not high risk     601\n7 College                           High risk          70\n8 College                           Not high risk     403"
  },
  {
    "objectID": "slides/lec-19.html#deeper-look-into-the-code-1",
    "href": "slides/lec-19.html#deeper-look-into-the-code-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease %>%\n  count(education_names, high_risk_names) %>%\n  pivot_wider(names_from = high_risk_names, values_from = n)\n\n# A tibble: 4 × 3\n  education_names                   `High risk` `Not high risk`\n  <fct>                                   <int>           <int>\n1 Some high school                          323            1397\n2 High school or GED                        147            1106\n3 Some college or vocational school          88             601\n4 College                                    70             403"
  },
  {
    "objectID": "slides/lec-19.html#deeper-look-into-the-code-2",
    "href": "slides/lec-19.html#deeper-look-into-the-code-2",
    "title": "Probabilities, odds, odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease %>%\n  count(education_names, high_risk_names) %>%\n  pivot_wider(names_from = high_risk_names, values_from = n) %>%\n  kable()\n\n\n\n\neducation_names\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/lec-19.html#deeper-look-into-the-code-3",
    "href": "slides/lec-19.html#deeper-look-into-the-code-3",
    "title": "Probabilities, odds, odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease %>%\n  count(education_names, high_risk_names) %>%\n  pivot_wider(names_from = high_risk_names, values_from = n) %>%\n  kable(col.names = c(\"Education\", \"High risk\", \"Not high risk\"))\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/lec-19.html#categorical-predictor",
    "href": "slides/lec-19.html#categorical-predictor",
    "title": "Probabilities, odds, odds ratios",
    "section": "Categorical predictor",
    "text": "Categorical predictor\nRecall: Education - 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\n\nheart_edu_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ education, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_edu_fit) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046"
  },
  {
    "objectID": "slides/lec-19.html#interpreting-education4---log-odds",
    "href": "slides/lec-19.html#interpreting-education4---log-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpreting education4 - log-odds",
    "text": "Interpreting education4 - log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046\n\n\n\n\n\n\nThe log-odds of having high risk for heart disease are expected to be 0.286 less for those with a college degree compared to those with some high school (the baseline group)."
  },
  {
    "objectID": "slides/lec-19.html#interpreting-education4---odds",
    "href": "slides/lec-19.html#interpreting-education4---odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpreting education4 - odds",
    "text": "Interpreting education4 - odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046\n\n\n\n\n\n\nThe odds of having high risk for heart disease for those with a college degree are expected to be 0.751 (exp(-0.286)) times the odds for those with some high school."
  },
  {
    "objectID": "slides/lec-19.html#coefficients-odds-ratios",
    "href": "slides/lec-19.html#coefficients-odds-ratios",
    "title": "Probabilities, odds, odds ratios",
    "section": "Coefficients + odds ratios",
    "text": "Coefficients + odds ratios\nThe model coefficient, -0.286, is the expected change in the log-odds when going from the Some high school group to the College group.\n\nTherefore, \\(e^{-0.286}\\) = 0.751 is the expected change in the odds when going from the Some high school group to the College group.\n\n\n\\[\nOR  = e^{\\hat{\\beta}_j} = \\exp\\{\\hat{\\beta}_j\\}\n\\]"
  },
  {
    "objectID": "slides/lec-19.html#quantitative-predictor",
    "href": "slides/lec-19.html#quantitative-predictor",
    "title": "Probabilities, odds, odds ratios",
    "section": "Quantitative predictor",
    "text": "Quantitative predictor\n\nheart_age_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_age_fit) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0"
  },
  {
    "objectID": "slides/lec-19.html#interpreting-age-log-odds",
    "href": "slides/lec-19.html#interpreting-age-log-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpreting age: log-odds",
    "text": "Interpreting age: log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0\n\n\n\n\n\n\nFor each additional year in age, the log-odds of having high risk for heart disease are expected to increase by 0.076."
  },
  {
    "objectID": "slides/lec-19.html#interpreting-age-odds",
    "href": "slides/lec-19.html#interpreting-age-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpreting age: odds",
    "text": "Interpreting age: odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0\n\n\n\n\n\n\n\nFor each additional year in age, the odds of having high risk for heart disease are expected to multiply by a factor of 1.08 (exp(0.076)).\nAlternate interpretation: For each additional year in age, the odds of having high risk for heart disease are expected to increase by 8%."
  },
  {
    "objectID": "slides/lec-19.html#multiple-predictors",
    "href": "slides/lec-19.html#multiple-predictors",
    "title": "Probabilities, odds, odds ratios",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\nheart_edu_age_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ education + age, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_edu_age_fit) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000"
  },
  {
    "objectID": "slides/lec-19.html#interpretation-in-terms-of-log-odds",
    "href": "slides/lec-19.html#interpretation-in-terms-of-log-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpretation in terms of log-odds",
    "text": "Interpretation in terms of log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\neducation4: The log-odds of having high risk for heart disease are expected to be 0.020 less for those with a college degree compared to those with some high school, holding age constant."
  },
  {
    "objectID": "slides/lec-19.html#interpretation-in-terms-of-log-odds-1",
    "href": "slides/lec-19.html#interpretation-in-terms-of-log-odds-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpretation in terms of log-odds",
    "text": "Interpretation in terms of log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\nage: For each additional year in age, the log-odds of having high risk for heart disease are expected to increase by 0.073, holding education level constant."
  },
  {
    "objectID": "slides/lec-19.html#interpretation-in-terms-of-odds",
    "href": "slides/lec-19.html#interpretation-in-terms-of-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpretation in terms of odds",
    "text": "Interpretation in terms of odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\neducation4: The odds of having high risk for heart disease for those with a college degree are expected to be 0.98 (exp(-0.020)) times the odds for those with some high school, holding age constant."
  },
  {
    "objectID": "slides/lec-19.html#interpretation-in-terms-of-odds-1",
    "href": "slides/lec-19.html#interpretation-in-terms-of-odds-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpretation in terms of odds",
    "text": "Interpretation in terms of odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\nage: For each additional year in age, the odds having high risk for heart disease are expected to multiply by a factor of 1.08 (exp(0.073)), holding education level constant."
  },
  {
    "objectID": "slides/lec-19.html#recap",
    "href": "slides/lec-19.html#recap",
    "title": "Probabilities, odds, odds ratios",
    "section": "Recap",
    "text": "Recap\n\nUse the odds ratio to compare the odds of two groups\nInterpret the coefficients of a logistic regression model with\n\na single categorical predictor\na single quantitative predictor\nmultiple predictors\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-18.html#announcements",
    "href": "slides/lec-18.html#announcements",
    "title": "Logistic regression",
    "section": "Announcements",
    "text": "Announcements\n\nAny questions on project proposals?\nExam 2 is due on 11:59 pm today."
  },
  {
    "objectID": "slides/lec-18.html#topics",
    "href": "slides/lec-18.html#topics",
    "title": "Logistic regression",
    "section": "Topics",
    "text": "Topics\n\nLogistic regression for binary response variable\nRelationship between odds and probabilities\nUse logistic regression model to calculate predicted odds and probabilities"
  },
  {
    "objectID": "slides/lec-18.html#computational-setup",
    "href": "slides/lec-18.html#computational-setup",
    "title": "Logistic regression",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(Stat2Data)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-18.html#types-of-outcome-variables",
    "href": "slides/lec-18.html#types-of-outcome-variables",
    "title": "Logistic regression",
    "section": "Types of outcome variables",
    "text": "Types of outcome variables\nQuantitative outcome variable:\n\nSales price of a house in Levittown, NY\nModel: Expected sales price given the number of bedrooms, lot size, etc.\n\n\nCategorical outcome variable:\n\nHigh risk of coronary heart disease\nModel: Probability an adult is high risk of heart disease given their age, total cholesterol, etc."
  },
  {
    "objectID": "slides/lec-18.html#models-for-categorical-outcomes",
    "href": "slides/lec-18.html#models-for-categorical-outcomes",
    "title": "Logistic regression",
    "section": "Models for categorical outcomes",
    "text": "Models for categorical outcomes\n\n\nLogistic regression\n2 Outcomes\n1: Yes, 0: No\n\nMultinomial logistic regression\n3+ Outcomes\n1: Democrat, 2: Republican, 3: Independent"
  },
  {
    "objectID": "slides/lec-18.html#election-forecasts",
    "href": "slides/lec-18.html#election-forecasts",
    "title": "Logistic regression",
    "section": "2020 election forecasts",
    "text": "2020 election forecasts\n\nSource: FiveThirtyEight Election Forcasts"
  },
  {
    "objectID": "slides/lec-18.html#nba-finals-predictions",
    "href": "slides/lec-18.html#nba-finals-predictions",
    "title": "Logistic regression",
    "section": "NBA finals predictions",
    "text": "NBA finals predictions\n\nSource: FiveThirtyEight 2021-22 NBA Predictions"
  },
  {
    "objectID": "slides/lec-18.html#do-teenagers-get-7-hours-of-sleep",
    "href": "slides/lec-18.html#do-teenagers-get-7-hours-of-sleep",
    "title": "Logistic regression",
    "section": "Do teenagers get 7+ hours of sleep?",
    "text": "Do teenagers get 7+ hours of sleep?\n\n\nStudents in grades 9 - 12 surveyed about health risk behaviors including whether they usually get 7 or more hours of sleep.\nSleep7\n1: yes\n0: no\n\n\ndata(YouthRisk2009)\nsleep <- YouthRisk2009 %>%\n  as_tibble() %>%\n  filter(!is.na(Age), !is.na(Sleep7))\nsleep %>%\n  relocate(Age, Sleep7)\n\n# A tibble: 446 × 6\n     Age Sleep7 Sleep           SmokeLife SmokeDaily MarijuaEver\n   <int>  <int> <fct>           <fct>     <fct>            <int>\n 1    16      1 8 hours         Yes       Yes                  1\n 2    17      0 5 hours         Yes       Yes                  1\n 3    18      0 5 hours         Yes       Yes                  1\n 4    17      1 7 hours         Yes       No                   1\n 5    15      0 4 or less hours No        No                   0\n 6    17      0 6 hours         No        No                   0\n 7    17      1 7 hours         No        No                   0\n 8    16      1 8 hours         Yes       No                   0\n 9    16      1 8 hours         No        No                   0\n10    18      0 4 or less hours Yes       Yes                  1\n# … with 436 more rows"
  },
  {
    "objectID": "slides/lec-18.html#plot-the-data",
    "href": "slides/lec-18.html#plot-the-data",
    "title": "Logistic regression",
    "section": "Plot the data",
    "text": "Plot the data\n\nggplot(sleep, aes(x = Age, y = Sleep7)) +\n  geom_point() + \n  labs(y = \"Getting 7+ hours of sleep\")"
  },
  {
    "objectID": "slides/lec-18.html#lets-fit-a-linear-regression-model",
    "href": "slides/lec-18.html#lets-fit-a-linear-regression-model",
    "title": "Logistic regression",
    "section": "Let’s fit a linear regression model",
    "text": "Let’s fit a linear regression model\nOutcome: \\(Y\\) = 1: yes, 0: no"
  },
  {
    "objectID": "slides/lec-18.html#lets-use-proportions",
    "href": "slides/lec-18.html#lets-use-proportions",
    "title": "Logistic regression",
    "section": "Let’s use proportions",
    "text": "Let’s use proportions\nOutcome: Probability of getting 7+ hours of sleep"
  },
  {
    "objectID": "slides/lec-18.html#what-happens-if-we-zoom-out",
    "href": "slides/lec-18.html#what-happens-if-we-zoom-out",
    "title": "Logistic regression",
    "section": "What happens if we zoom out?",
    "text": "What happens if we zoom out?\nOutcome: Probability of getting 7+ hours of sleep\n\n🛑 This model produces predictions outside of 0 and 1."
  },
  {
    "objectID": "slides/lec-18.html#lets-try-another-model",
    "href": "slides/lec-18.html#lets-try-another-model",
    "title": "Logistic regression",
    "section": "Let’s try another model",
    "text": "Let’s try another model\n\n✅ This model (called a logistic regression model) only produces predictions between 0 and 1."
  },
  {
    "objectID": "slides/lec-18.html#the-code",
    "href": "slides/lec-18.html#the-code",
    "title": "Logistic regression",
    "section": "The code",
    "text": "The code\n\nggplot(sleep_age, aes(x = Age, y = prop)) +\n  geom_point() + \n  geom_hline(yintercept = c(0,1), lty = 2) + \n  stat_smooth(method =\"glm\", method.args = list(family = \"binomial\"), \n              fullrange = TRUE, se = FALSE) +\n  labs(y = \"P(7+ hours of sleep)\") +\n  xlim(1, 40) +\n  ylim(-0.5, 1.5)"
  },
  {
    "objectID": "slides/lec-18.html#different-types-of-models",
    "href": "slides/lec-18.html#different-types-of-models",
    "title": "Logistic regression",
    "section": "Different types of models",
    "text": "Different types of models\n\n\n\n\n\n\n\n\nMethod\nOutcome\nModel\n\n\n\n\nLinear regression\nQuantitative\n\\(Y = \\beta_0 + \\beta_1~ X\\)\n\n\nLogistic regression\nBinary\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1 ~ X\\)"
  },
  {
    "objectID": "slides/lec-18.html#binary-response-variable",
    "href": "slides/lec-18.html#binary-response-variable",
    "title": "Logistic regression",
    "section": "Binary response variable",
    "text": "Binary response variable\n\n\\(Y = 1: \\text{ yes}, 0: \\text{ no}\\)\n\\(\\pi\\): probability that \\(Y=1\\), i.e., \\(P(Y = 1)\\)\n\\(\\frac{\\pi}{1-\\pi}\\): odds that \\(Y = 1\\)\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\): log odds\nGo from \\(\\pi\\) to \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\) using the logit transformation"
  },
  {
    "objectID": "slides/lec-18.html#odds",
    "href": "slides/lec-18.html#odds",
    "title": "Logistic regression",
    "section": "Odds",
    "text": "Odds\nSuppose there is a 70% chance it will rain tomorrow\n\nProbability it will rain is \\(\\mathbf{p = 0.7}\\)\nProbability it won’t rain is \\(\\mathbf{1 - p = 0.3}\\)\nOdds it will rain are 7 to 3, 7:3, \\(\\mathbf{\\frac{0.7}{0.3} \\approx 2.33}\\)"
  },
  {
    "objectID": "slides/lec-18.html#are-teenagers-getting-enough-sleep",
    "href": "slides/lec-18.html#are-teenagers-getting-enough-sleep",
    "title": "Logistic regression",
    "section": "Are teenagers getting enough sleep?",
    "text": "Are teenagers getting enough sleep?\n\nsleep %>%\n  count(Sleep7) %>%\n  mutate(p = round(n / sum(n), 3))\n\n# A tibble: 2 × 3\n  Sleep7     n     p\n   <int> <int> <dbl>\n1      0   150 0.336\n2      1   296 0.664\n\n\n\n\\(P(\\text{7+ hours of sleep}) = P(Y = 1) = p = 0.664\\)\n\n\n\\(P(\\text{< 7 hours of sleep}) = P(Y = 0) = 1 - p = 0.336\\)\n\n\n\\(P(\\text{odds of 7+ hours of sleep}) = \\frac{0.664}{0.336} = 1.976\\)"
  },
  {
    "objectID": "slides/lec-18.html#from-odds-to-probabilities",
    "href": "slides/lec-18.html#from-odds-to-probabilities",
    "title": "Logistic regression",
    "section": "From odds to probabilities",
    "text": "From odds to probabilities\n\n\nodds\n\\[\\omega = \\frac{\\pi}{1-\\pi}\\]\n\nprobability\n\\[\\pi = \\frac{\\omega}{1 + \\omega}\\]"
  },
  {
    "objectID": "slides/lec-18.html#logistic-regression",
    "href": "slides/lec-18.html#logistic-regression",
    "title": "Logistic regression",
    "section": "Logistic regression",
    "text": "Logistic regression"
  },
  {
    "objectID": "slides/lec-18.html#from-odds-to-probabilities-1",
    "href": "slides/lec-18.html#from-odds-to-probabilities-1",
    "title": "Logistic regression",
    "section": "From odds to probabilities",
    "text": "From odds to probabilities\n\nLogistic model: log odds = \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\)\nOdds = \\(\\exp\\big\\{\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\big\\} = \\frac{\\pi}{1-\\pi}\\)\nCombining (1) and (2) with what we saw earlier\n\n\\[\\text{probability} = \\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\\]"
  },
  {
    "objectID": "slides/lec-18.html#logistic-regression-model",
    "href": "slides/lec-18.html#logistic-regression-model",
    "title": "Logistic regression",
    "section": "Logistic regression model",
    "text": "Logistic regression model\nLogit form: \\[\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\]\n\nProbability form:\n\\[\\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\\]"
  },
  {
    "objectID": "slides/lec-18.html#risk-of-coronary-heart-disease",
    "href": "slides/lec-18.html#risk-of-coronary-heart-disease",
    "title": "Logistic regression",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use age to predict if a randomly selected adult is high risk of having coronary heart disease in the next 10 years.\nhigh_risk:\n\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\n\nage: Age at exam time (in years)"
  },
  {
    "objectID": "slides/lec-18.html#data-heart",
    "href": "slides/lec-18.html#data-heart",
    "title": "Logistic regression",
    "section": "Data: heart",
    "text": "Data: heart\n\nheart_disease <- read_csv(here::here(\"slides\", \"data/framingham.csv\")) %>%\n  select(age, TenYearCHD) %>%\n  drop_na() %>% # drop observations with NAs\n  mutate(high_risk = as.factor(TenYearCHD)) %>%\n  select(age, high_risk)\n\nheart_disease\n\n# A tibble: 4,240 × 2\n     age high_risk\n   <dbl> <fct>    \n 1    39 0        \n 2    46 0        \n 3    48 0        \n 4    61 1        \n 5    46 0        \n 6    43 0        \n 7    63 1        \n 8    45 0        \n 9    52 0        \n10    43 0        \n# … with 4,230 more rows"
  },
  {
    "objectID": "slides/lec-18.html#high-risk-vs.-age",
    "href": "slides/lec-18.html#high-risk-vs.-age",
    "title": "Logistic regression",
    "section": "High risk vs. age",
    "text": "High risk vs. age\n\nggplot(heart_disease, aes(x = high_risk, y = age)) +\n  geom_boxplot() +\n  labs(x = \"High risk - 1: yes, 0: no\",\n       y = \"Age\", \n       title = \"Age vs. High risk of heart disease\")"
  },
  {
    "objectID": "slides/lec-18.html#lets-fit-the-model",
    "href": "slides/lec-18.html#lets-fit-the-model",
    "title": "Logistic regression",
    "section": "Let’s fit the model",
    "text": "Let’s fit the model\n\nheart_disease_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) %>% kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n\n\nage\n0.075\n0.005\n14.178\n0"
  },
  {
    "objectID": "slides/lec-18.html#the-model",
    "href": "slides/lec-18.html#the-model",
    "title": "Logistic regression",
    "section": "The model",
    "text": "The model\n\ntidy(heart_disease_fit) %>% kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n\n\nage\n0.075\n0.005\n14.178\n0\n\n\n\n\n\n\\[\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.561 + 0.075 \\times \\text{age}\\] where \\(\\hat{\\pi}\\) is the predicted probability of being high risk"
  },
  {
    "objectID": "slides/lec-18.html#predicted-log-odds",
    "href": "slides/lec-18.html#predicted-log-odds",
    "title": "Logistic regression",
    "section": "Predicted log odds",
    "text": "Predicted log odds\n\naugment(heart_disease_fit$fit)\n\n# A tibble: 4,240 × 8\n   high_risk   age .fitted .resid .std.resid     .hat .sigma   .cooksd\n   <fct>     <dbl>   <dbl>  <dbl>      <dbl>    <dbl>  <dbl>     <dbl>\n 1 0            39  -2.65  -0.370     -0.370 0.000466  0.895 0.0000165\n 2 0            46  -2.13  -0.475     -0.475 0.000322  0.895 0.0000192\n 3 0            48  -1.98  -0.509     -0.509 0.000288  0.895 0.0000199\n 4 1            61  -1.01   1.62       1.62  0.000706  0.895 0.000968 \n 5 0            46  -2.13  -0.475     -0.475 0.000322  0.895 0.0000192\n 6 0            43  -2.35  -0.427     -0.427 0.000384  0.895 0.0000183\n 7 1            63  -0.858  1.56       1.56  0.000956  0.895 0.00113  \n 8 0            45  -2.20  -0.458     -0.458 0.000342  0.895 0.0000189\n 9 0            52  -1.68  -0.585     -0.585 0.000262  0.895 0.0000244\n10 0            43  -2.35  -0.427     -0.427 0.000384  0.895 0.0000183\n# … with 4,230 more rows\n\n\n\nFor observation 1\n\\[\\text{predicted odds} = \\hat{\\omega} = \\frac{\\hat{\\pi}}{1-\\hat{\\pi}} = \\exp\\{-2.650\\} = 0.071\\]"
  },
  {
    "objectID": "slides/lec-18.html#predicted-probabilities",
    "href": "slides/lec-18.html#predicted-probabilities",
    "title": "Logistic regression",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\npredict(heart_disease_fit, new_data = heart_disease, type = \"prob\")\n\n# A tibble: 4,240 × 2\n   .pred_0 .pred_1\n     <dbl>   <dbl>\n 1   0.934  0.0660\n 2   0.894  0.106 \n 3   0.878  0.122 \n 4   0.733  0.267 \n 5   0.894  0.106 \n 6   0.913  0.0870\n 7   0.702  0.298 \n 8   0.900  0.0996\n 9   0.843  0.157 \n10   0.913  0.0870\n# … with 4,230 more rows\n\n\n\n\\[\\text{predicted probabilities} = \\hat{\\pi} = \\frac{\\exp\\{-2.650\\}}{1 + \\exp\\{-2.650\\}} = 0.066\\]"
  },
  {
    "objectID": "slides/lec-18.html#predicted-classes",
    "href": "slides/lec-18.html#predicted-classes",
    "title": "Logistic regression",
    "section": "Predicted classes",
    "text": "Predicted classes\n\npredict(heart_disease_fit, new_data = heart_disease, type = \"class\")\n\n# A tibble: 4,240 × 1\n   .pred_class\n   <fct>      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# … with 4,230 more rows"
  },
  {
    "objectID": "slides/lec-18.html#default-prediction",
    "href": "slides/lec-18.html#default-prediction",
    "title": "Logistic regression",
    "section": "Default prediction",
    "text": "Default prediction\nFor a logistic regression, the default prediction is the class.\n\npredict(heart_disease_fit, new_data = heart_disease)\n\n# A tibble: 4,240 × 1\n   .pred_class\n   <fct>      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# … with 4,230 more rows"
  },
  {
    "objectID": "slides/lec-18.html#observed-vs.-predicted",
    "href": "slides/lec-18.html#observed-vs.-predicted",
    "title": "Logistic regression",
    "section": "Observed vs. predicted",
    "text": "Observed vs. predicted\n\nWhat does the following table show?\n\n\npredict(heart_disease_fit, new_data = heart_disease) %>%\n  bind_cols(heart_disease) %>%\n  count(high_risk, .pred_class)\n\n# A tibble: 2 × 3\n  high_risk .pred_class     n\n  <fct>     <fct>       <int>\n1 0         0            3596\n2 1         0             644"
  },
  {
    "objectID": "slides/lec-18.html#recap",
    "href": "slides/lec-18.html#recap",
    "title": "Logistic regression",
    "section": "Recap",
    "text": "Recap\n\nLogistic regression for binary response variable\nRelationship between odds and probabilities\nUsed logistic regression model to calculate predicted odds and probabilities"
  },
  {
    "objectID": "slides/lec-18.html#application-exercise",
    "href": "slides/lec-18.html#application-exercise",
    "title": "Logistic regression",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 ae-9-odds\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-24.html#topics",
    "href": "slides/lec-24.html#topics",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Topics",
    "text": "Topics\n\n\nPredictions\nModel selection\nChecking conditions"
  },
  {
    "objectID": "slides/lec-24.html#computational-setup",
    "href": "slides/lec-24.html#computational-setup",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(NHANES)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(colorblindr)\nlibrary(pROC)\nlibrary(Stat2Data)\nlibrary(nnet)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-24.html#nhanes-data",
    "href": "slides/lec-24.html#nhanes-data",
    "title": "MultiLR: Prediction + inferential models",
    "section": "NHANES Data",
    "text": "NHANES Data\n\n\nNational Health and Nutrition Examination Survey is conducted by the National Center for Health Statistics (NCHS).\nThe goal is to “assess the health and nutritional status of adults and children in the United States”.\nThis survey includes an interview and a physical examination."
  },
  {
    "objectID": "slides/lec-24.html#variables",
    "href": "slides/lec-24.html#variables",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Variables",
    "text": "Variables\nGoal: Use a person’s age and whether they do regular physical activity to predict their self-reported health rating.\n\nOutcome: HealthGen: Self-reported rating of participant’s health in general. Excellent, Vgood, Good, Fair, or Poor.\nPredictors:\n\nAge: Age at time of screening (in years). Participants 80 or older were recorded as 80.\nPhysActive: Participant does moderate to vigorous-intensity sports, fitness or recreational activities."
  },
  {
    "objectID": "slides/lec-24.html#the-data",
    "href": "slides/lec-24.html#the-data",
    "title": "MultiLR: Prediction + inferential models",
    "section": "The data",
    "text": "The data\n\nnhanes_adult <- NHANES %>%\n  filter(Age >= 18) %>%\n  select(HealthGen, Age, PhysActive, Education) %>%\n  drop_na() %>%\n  mutate(obs_num = 1:n())\n\n\nglimpse(nhanes_adult)\n\nRows: 6,465\nColumns: 5\n$ HealthGen  <fct> Good, Good, Good, Good, Vgood, Vgood, Vgood, Vgood, Vgood, …\n$ Age        <int> 34, 34, 34, 49, 45, 45, 45, 66, 58, 54, 50, 33, 60, 56, 56,…\n$ PhysActive <fct> No, No, No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, …\n$ Education  <fct> High School, High School, High School, Some College, Colleg…\n$ obs_num    <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …"
  },
  {
    "objectID": "slides/lec-24.html#model-in-r",
    "href": "slides/lec-24.html#model-in-r",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Model in R",
    "text": "Model in R\n\nhealth_fit <- multinom_reg() %>%\n  set_engine(\"nnet\") %>%\n  fit(HealthGen ~ Age + PhysActive, data = nhanes_adult)\n\nhealth_fit <- repair_call(health_fit, data = nhanes_adult)"
  },
  {
    "objectID": "slides/lec-24.html#model-summary",
    "href": "slides/lec-24.html#model-summary",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Model summary",
    "text": "Model summary\n\ntidy(health_fit) %>% print(n = 12)\n\n# A tibble: 12 × 6\n   y.level term            estimate std.error statistic  p.value\n   <chr>   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n 1 Vgood   (Intercept)    1.27        0.154      8.23   1.80e-16\n 2 Vgood   Age           -0.0000361   0.00259   -0.0139 9.89e- 1\n 3 Vgood   PhysActiveYes -0.332       0.0949    -3.50   4.72e- 4\n 4 Good    (Intercept)    1.99        0.150     13.3    2.81e-40\n 5 Good    Age           -0.00304     0.00256   -1.19   2.35e- 1\n 6 Good    PhysActiveYes -1.01        0.0921   -11.0    4.80e-28\n 7 Fair    (Intercept)    1.03        0.174      5.94   2.89e- 9\n 8 Fair    Age            0.00113     0.00302    0.373  7.09e- 1\n 9 Fair    PhysActiveYes -1.66        0.109    -15.2    4.14e-52\n10 Poor    (Intercept)   -1.34        0.299     -4.47   7.65e- 6\n11 Poor    Age            0.0193      0.00505    3.83   1.30e- 4\n12 Poor    PhysActiveYes -2.67        0.236    -11.3    1.20e-29"
  },
  {
    "objectID": "slides/lec-24.html#calculating-probabilities",
    "href": "slides/lec-24.html#calculating-probabilities",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Calculating probabilities",
    "text": "Calculating probabilities\n\nFor categories \\(2,\\ldots,K\\), the probability that the \\(i^{th}\\) observation is in the \\(j^{th}\\) category is\n\\[\n\\hat{\\pi}_{ij} = \\frac{e^{\\hat{\\beta}_{0j} + \\hat{\\beta}_{1j}x_{i1} + \\dots + \\hat{\\beta}_{pj}x_{ip}}}{1 + \\sum\\limits_{k=2}^K e^{\\hat{\\beta}_{0k} + \\hat{\\beta}_{1k}x_{i1} + \\dots \\hat{\\beta}_{pk}x_{ip}}}\n\\]\nFor the baseline category, \\(k=1\\), we calculate the probability \\(\\hat{\\pi}_{i1}\\) as\n\\[\n\\hat{\\pi}_{i1} = 1- \\sum\\limits_{k=2}^K \\hat{\\pi}_{ik}\n\\]"
  },
  {
    "objectID": "slides/lec-24.html#predicted-health-rating",
    "href": "slides/lec-24.html#predicted-health-rating",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Predicted health rating",
    "text": "Predicted health rating\nWe can use our model to predict a person’s perceived health rating given their age and whether they exercise.\n\nhealth_aug <- augment(health_fit, new_data = nhanes_adult)\nhealth_aug\n\n# A tibble: 6,465 × 11\n   HealthGen   Age PhysActive Education      obs_num .pred_class .pred_Excellent\n   <fct>     <int> <fct>      <fct>            <int> <fct>                 <dbl>\n 1 Good         34 No         High School          1 Good                 0.0687\n 2 Good         34 No         High School          2 Good                 0.0687\n 3 Good         34 No         High School          3 Good                 0.0687\n 4 Good         49 No         Some College         4 Good                 0.0691\n 5 Vgood        45 Yes        College Grad         5 Vgood                0.155 \n 6 Vgood        45 Yes        College Grad         6 Vgood                0.155 \n 7 Vgood        45 Yes        College Grad         7 Vgood                0.155 \n 8 Vgood        66 Yes        Some College         8 Vgood                0.157 \n 9 Vgood        58 Yes        College Grad         9 Vgood                0.156 \n10 Fair         54 Yes        9 - 11th Grade      10 Vgood                0.156 \n# … with 6,455 more rows, and 4 more variables: .pred_Vgood <dbl>,\n#   .pred_Good <dbl>, .pred_Fair <dbl>, .pred_Poor <dbl>"
  },
  {
    "objectID": "slides/lec-24.html#actual-vs.-predicted-health-rating",
    "href": "slides/lec-24.html#actual-vs.-predicted-health-rating",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Actual vs. predicted health rating",
    "text": "Actual vs. predicted health rating\nFor each observation, the predicted perceived health rating is the category with the highest predicted probability.\n\nhealth_aug %>% select(contains(\"pred\"))\n\n# A tibble: 6,465 × 6\n   .pred_class .pred_Excellent .pred_Vgood .pred_Good .pred_Fair .pred_Poor\n   <fct>                 <dbl>       <dbl>      <dbl>      <dbl>      <dbl>\n 1 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 2 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 3 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 4 Good                 0.0691       0.244      0.435     0.205     0.0467 \n 5 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 6 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 7 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 8 Vgood                0.157        0.400      0.342     0.0904    0.0102 \n 9 Vgood                0.156        0.397      0.349     0.0890    0.00872\n10 Vgood                0.156        0.396      0.352     0.0883    0.00804\n# … with 6,455 more rows"
  },
  {
    "objectID": "slides/lec-24.html#confusion-matrix",
    "href": "slides/lec-24.html#confusion-matrix",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nhealth_conf <- health_aug %>% \n  count(HealthGen, .pred_class, .drop = FALSE) %>%\n  pivot_wider(names_from = .pred_class, values_from = n)\nhealth_conf\n\n# A tibble: 5 × 6\n  HealthGen Excellent Vgood  Good  Fair  Poor\n  <fct>         <int> <int> <int> <int> <int>\n1 Excellent         0   528   210     0     0\n2 Vgood             0  1341   743     0     0\n3 Good              0  1226  1316     0     0\n4 Fair              0   296   625     0     0\n5 Poor              0    24   156     0     0"
  },
  {
    "objectID": "slides/lec-24.html#actual-vs.-predicted-health-rating-1",
    "href": "slides/lec-24.html#actual-vs.-predicted-health-rating-1",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Actual vs. predicted health rating",
    "text": "Actual vs. predicted health rating\n\nWhy do you think no observations were predicted to have a rating of “Excellent”, “Fair”, or “Poor”?\n\nWill discuss this later in the model diagnosis."
  },
  {
    "objectID": "slides/lec-24.html#comparing-nested-models",
    "href": "slides/lec-24.html#comparing-nested-models",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced model includes predictors \\(x_1, \\ldots, x_q\\)\nFull model includes predictors \\(x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p\\)\n\nWe want to test the following hypotheses:\n\n\\(H_0: \\beta_{q+1} = \\dots = \\beta_p = 0\\)\n\\(H_A: \\text{ at least 1 }\\beta_j \\text{ is not } 0\\)\n\nTo do so, we will use the drop-in-deviance test (very similar to logistic regression)"
  },
  {
    "objectID": "slides/lec-24.html#add-education-to-the-model",
    "href": "slides/lec-24.html#add-education-to-the-model",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Add Education to the model?",
    "text": "Add Education to the model?\n\nWe consider adding the participants’ Education level to the model.\n\nEducation takes values 8thGrade, 9-11thGrade, HighSchool, SomeCollege, and CollegeGrad\n\nModels we’re testing:\n\nReduced model: Age, PhysActive\nFull model: Age, PhysActive, Education\n\n\n\n\\[\n\\begin{align}\n&H_0: \\beta_{9-11thGrade} = \\beta_{HighSchool} = \\beta_{SomeCollege} = \\beta_{CollegeGrad} = 0\\\\\n&H_a: \\text{ at least one }\\beta_j \\text{ is not equal to }0\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lec-24.html#add-education-to-the-model-1",
    "href": "slides/lec-24.html#add-education-to-the-model-1",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Add Education to the model?",
    "text": "Add Education to the model?\n\nreduced_fit <- multinom_reg() %>%\n  set_engine(\"nnet\") %>%\n  fit(HealthGen ~ Age + PhysActive,\n  data = nhanes_adult)\n\nfull_fit <- multinom_reg() %>%\n  set_engine(\"nnet\") %>%\n  fit(HealthGen ~ Age + PhysActive + Education,\n  data = nhanes_adult)\n  \nreduced_fit <- repair_call(reduced_fit, data = nhanes_adult)\nfull_fit <- repair_call(full_fit, data = nhanes_adult)"
  },
  {
    "objectID": "slides/lec-24.html#add-education-to-the-model-2",
    "href": "slides/lec-24.html#add-education-to-the-model-2",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Add Education to the model?",
    "text": "Add Education to the model?\n\nanova(reduced_fit$fit, full_fit$fit, test = \"Chisq\") %>%\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nResid. df\nResid. Dev\nTest\nDf\nLR stat.\nPr(Chi)\n\n\n\n\nAge + PhysActive\n25848\n16994.23\n\nNA\nNA\nNA\n\n\nAge + PhysActive + Education\n25832\n16505.10\n1 vs 2\n16\n489.132\n0\n\n\n\n\n\n\nAt least one coefficient associated with Education is non-zero. Therefore, we will include Education in the model.\n\n\nIf you want to apply anova to the fitted model from workflow, you have to apply fitted_obj %>% extract_fit_engine() to obtain fitted object that can be put into anova()."
  },
  {
    "objectID": "slides/lec-24.html#model-with-education",
    "href": "slides/lec-24.html#model-with-education",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Model with Education",
    "text": "Model with Education\n\ntidy(full_fit, conf.int = T) %>% print(n = 28)\n\n# A tibble: 28 × 8\n   y.level term         estimate std.error statistic  p.value conf.low conf.high\n   <chr>   <chr>           <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n 1 Vgood   (Intercept)   5.82e-1   0.301      1.93   5.36e- 2 -0.00914   1.17   \n 2 Vgood   Age           1.12e-3   0.00266    0.419  6.75e- 1 -0.00411   0.00634\n 3 Vgood   PhysActiveY… -2.64e-1   0.0985    -2.68   7.33e- 3 -0.457    -0.0711 \n 4 Vgood   Education9 …  7.68e-1   0.308      2.49   1.27e- 2  0.164     1.37   \n 5 Vgood   EducationHi…  7.01e-1   0.280      2.51   1.21e- 2  0.153     1.25   \n 6 Vgood   EducationSo…  7.88e-1   0.271      2.90   3.71e- 3  0.256     1.32   \n 7 Vgood   EducationCo…  4.08e-1   0.268      1.52   1.28e- 1 -0.117     0.933  \n 8 Good    (Intercept)   2.04e+0   0.272      7.51   5.77e-14  1.51      2.57   \n 9 Good    Age          -1.72e-3   0.00263   -0.651  5.15e- 1 -0.00688   0.00345\n10 Good    PhysActiveY… -7.58e-1   0.0961    -7.88   3.16e-15 -0.946    -0.569  \n11 Good    Education9 …  3.60e-1   0.275      1.31   1.90e- 1 -0.179     0.899  \n12 Good    EducationHi…  8.52e-2   0.247      0.345  7.30e- 1 -0.399     0.569  \n13 Good    EducationSo… -1.13e-2   0.239     -0.0472 9.62e- 1 -0.480     0.457  \n14 Good    EducationCo… -8.91e-1   0.236     -3.77   1.65e- 4 -1.35     -0.427  \n15 Fair    (Intercept)   2.12e+0   0.288      7.35   1.91e-13  1.55      2.68   \n16 Fair    Age           3.35e-4   0.00312    0.107  9.14e- 1 -0.00578   0.00645\n17 Fair    PhysActiveY… -1.19e+0   0.115    -10.4    3.50e-25 -1.42     -0.966  \n18 Fair    Education9 … -2.24e-1   0.279     -0.802  4.22e- 1 -0.771     0.323  \n19 Fair    EducationHi… -8.32e-1   0.252     -3.31   9.44e- 4 -1.33     -0.339  \n20 Fair    EducationSo… -1.34e+0   0.246     -5.46   4.71e- 8 -1.82     -0.861  \n21 Fair    EducationCo… -2.51e+0   0.253     -9.91   3.67e-23 -3.00     -2.01   \n22 Poor    (Intercept)  -2.00e-1   0.411     -0.488  6.26e- 1 -1.01      0.605  \n23 Poor    Age           1.79e-2   0.00509    3.53   4.21e- 4  0.00797   0.0279 \n24 Poor    PhysActiveY… -2.27e+0   0.242     -9.38   6.81e-21 -2.74     -1.79   \n25 Poor    Education9 … -3.60e-1   0.353     -1.02   3.08e- 1 -1.05      0.332  \n26 Poor    EducationHi… -1.15e+0   0.334     -3.44   5.86e- 4 -1.81     -0.494  \n27 Poor    EducationSo… -1.07e+0   0.316     -3.40   6.77e- 4 -1.69     -0.454  \n28 Poor    EducationCo… -2.32e+0   0.366     -6.34   2.27e-10 -3.04     -1.60"
  },
  {
    "objectID": "slides/lec-24.html#compare-nhanes-models-using-aic",
    "href": "slides/lec-24.html#compare-nhanes-models-using-aic",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Compare NHANES models using AIC",
    "text": "Compare NHANES models using AIC\nReduced model:\n\nglance(reduced_fit)$AIC\n\n[1] 17018.23\n\n\n\nFull model:\n\nglance(full_fit)$AIC\n\n[1] 16561.1\n\n\n\n\nWe can also use BIC to compare model. Since the glance() does not provide us the BIC. For this class, we just use AIC."
  },
  {
    "objectID": "slides/lec-24.html#conditions-for-inference",
    "href": "slides/lec-24.html#conditions-for-inference",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Conditions for inference",
    "text": "Conditions for inference\nWe want to check the following conditions for inference for the multinomial logistic regression model:\n\nLinearity: Is there a linear relationship between the log-odds and the predictor variables?\nRandomness: Was the sample randomly selected? Or can we reasonably treat it as random?\nIndependence: Are the observations independent?"
  },
  {
    "objectID": "slides/lec-24.html#checking-linearity",
    "href": "slides/lec-24.html#checking-linearity",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking linearity",
    "text": "Checking linearity\nSimilar to logistic regression, we will check linearity by examining empirical logit plots between each level of the response and the quantitative predictor variables.\n\nnhanes_adult <- nhanes_adult %>%\n  mutate(\n    Excellent = factor(if_else(HealthGen == \"Excellent\", \"1\", \"0\")),\n    Vgood = factor(if_else(HealthGen == \"Vgood\", \"1\", \"0\")),\n    Good = factor(if_else(HealthGen == \"Good\", \"1\", \"0\")),\n    Fair = factor(if_else(HealthGen == \"Fair\", \"1\", \"0\")),\n    Poor = factor(if_else(HealthGen == \"Poor\", \"1\", \"0\"))\n  )"
  },
  {
    "objectID": "slides/lec-24.html#checking-linearity-1",
    "href": "slides/lec-24.html#checking-linearity-1",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking linearity",
    "text": "Checking linearity\nemplogitplot1: numerical predictor\nemplogitplot2: numerical + categorical\n\nemplogitplot1(Excellent ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Excellent vs. Age\")\n\nemplogitplot1(Vgood ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Vgood vs. Age\")"
  },
  {
    "objectID": "slides/lec-24.html#checking-linearity-2",
    "href": "slides/lec-24.html#checking-linearity-2",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nemplogitplot1(Good ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Good vs. Age\")\n\nemplogitplot1(Fair ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Fair vs. Age\")"
  },
  {
    "objectID": "slides/lec-24.html#checking-linearity-3",
    "href": "slides/lec-24.html#checking-linearity-3",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nemplogitplot1(Poor ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Poor vs. Age\")\n\n\n\nThe linearity condition is not satisfied. If we recall the previous confusion matrix, the fitted model only provide two fitted class: very good and good, which may due to the unreasonable model assumption."
  },
  {
    "objectID": "slides/lec-24.html#checking-randomness",
    "href": "slides/lec-24.html#checking-randomness",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking randomness",
    "text": "Checking randomness\nWe can check the randomness condition based on the context of the data and how the observations were collected.\n\nWas the sample randomly selected?\nIf the sample was not randomly selected, ask whether there is reason to believe the observations in the sample differ systematically from the population of interest.\n\n\n✅ The randomness condition is satisfied. We do not have reason to believe that the participants in this study differ systematically from adults in the U.S.."
  },
  {
    "objectID": "slides/lec-24.html#checking-independence",
    "href": "slides/lec-24.html#checking-independence",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking independence",
    "text": "Checking independence\nWe can check the independence condition based on the context of the data and how the observations were collected.\nIndependence is most often violated if the data were collected over time or there is a strong spatial relationship between the observations.\n\n✅ The independence condition is satisfied. It is reasonable to conclude that the participants’ health and behavior characteristics are independent of one another."
  },
  {
    "objectID": "slides/lec-24.html#recap",
    "href": "slides/lec-24.html#recap",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Recap",
    "text": "Recap\n\nPredictions\nModel selection for inference\nChecking conditions for inference\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-2.html#announcements",
    "href": "slides/lec-2.html#announcements",
    "title": "Simple Linear Regression",
    "section": "Announcements",
    "text": "Announcements\n\nIf you’re just joining the class, welcome! Go to the course website and review content you’ve missed, read the syllabus, and complete the Getting to know you survey.\nLab 1 is due Friday, at 11:59pm, on Gradescope."
  },
  {
    "objectID": "slides/lec-2.html#review-on-lab-1",
    "href": "slides/lec-2.html#review-on-lab-1",
    "title": "Simple Linear Regression",
    "section": "Review on Lab 1",
    "text": "Review on Lab 1\n\nGet used to R and R studio\nGet used to use R with Git : clone, commit, pull, push\nLearn the components of drawing a plot\n\ndata\nmapping (x,y,color,alpha)\ngeometry (histogram, point, density)\nlab (label for x,y-axises, title, etc.)\nCheck cheatsheet"
  },
  {
    "objectID": "slides/lec-2.html#review-on-lab-1-1",
    "href": "slides/lec-2.html#review-on-lab-1-1",
    "title": "Simple Linear Regression",
    "section": "Review on Lab 1",
    "text": "Review on Lab 1\n\nQuestions:\n\nChoice of binwidth for histogram: Pick the one and reveal the main trend. Too small: too noisy; Too large: ignore trend\nDensity Plot (Smooth version of histogram. eg. Normal Distribution)"
  },
  {
    "objectID": "slides/lec-2.html#outline",
    "href": "slides/lec-2.html#outline",
    "title": "Simple Linear Regression",
    "section": "Outline",
    "text": "Outline\n\nUse simple linear regression to describe the relationship between a quantitative predictor and quantitative outcome variable\nEstimate the slope and intercept of the regression line using the least squares method\nInterpret the slope and intercept of the regression line"
  },
  {
    "objectID": "slides/lec-2.html#computational-setup",
    "href": "slides/lec-2.html#computational-setup",
    "title": "Simple Linear Regression",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(tidymodels)      # for modeling\nlibrary(fivethirtyeight) # for the fandango dataset\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/lec-2.html#movie-ratings",
    "href": "slides/lec-2.html#movie-ratings",
    "title": "Simple Linear Regression",
    "section": "Movie ratings",
    "text": "Movie ratings\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/lec-2.html#data-prep",
    "href": "slides/lec-2.html#data-prep",
    "title": "Simple Linear Regression",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores <- fandango %>%\n  rename(\n    critics = rottentomatoes, \n    audience = rottentomatoes_user\n  )"
  },
  {
    "objectID": "slides/lec-2.html#data-overview",
    "href": "slides/lec-2.html#data-overview",
    "title": "Simple Linear Regression",
    "section": "Data overview",
    "text": "Data overview\n\nglimpse(movie_scores)\n\nRows: 146\nColumns: 23\n$ film                       <chr> \"Avengers: Age of Ultron\", \"Cinderella\", \"A…\n$ year                       <dbl> 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2…\n$ critics                    <int> 74, 85, 80, 18, 14, 63, 42, 86, 99, 89, 84,…\n$ audience                   <int> 86, 80, 90, 84, 28, 62, 53, 64, 82, 87, 77,…\n$ metacritic                 <int> 66, 67, 64, 22, 29, 50, 53, 81, 81, 80, 71,…\n$ metacritic_user            <dbl> 7.1, 7.5, 8.1, 4.7, 3.4, 6.8, 7.6, 6.8, 8.8…\n$ imdb                       <dbl> 7.8, 7.1, 7.8, 5.4, 5.1, 7.2, 6.9, 6.5, 7.4…\n$ fandango_stars             <dbl> 5.0, 5.0, 5.0, 5.0, 3.5, 4.5, 4.0, 4.0, 4.5…\n$ fandango_ratingvalue       <dbl> 4.5, 4.5, 4.5, 4.5, 3.0, 4.0, 3.5, 3.5, 4.0…\n$ rt_norm                    <dbl> 3.70, 4.25, 4.00, 0.90, 0.70, 3.15, 2.10, 4…\n$ rt_user_norm               <dbl> 4.30, 4.00, 4.50, 4.20, 1.40, 3.10, 2.65, 3…\n$ metacritic_norm            <dbl> 3.30, 3.35, 3.20, 1.10, 1.45, 2.50, 2.65, 4…\n$ metacritic_user_nom        <dbl> 3.55, 3.75, 4.05, 2.35, 1.70, 3.40, 3.80, 3…\n$ imdb_norm                  <dbl> 3.90, 3.55, 3.90, 2.70, 2.55, 3.60, 3.45, 3…\n$ rt_norm_round              <dbl> 3.5, 4.5, 4.0, 1.0, 0.5, 3.0, 2.0, 4.5, 5.0…\n$ rt_user_norm_round         <dbl> 4.5, 4.0, 4.5, 4.0, 1.5, 3.0, 2.5, 3.0, 4.0…\n$ metacritic_norm_round      <dbl> 3.5, 3.5, 3.0, 1.0, 1.5, 2.5, 2.5, 4.0, 4.0…\n$ metacritic_user_norm_round <dbl> 3.5, 4.0, 4.0, 2.5, 1.5, 3.5, 4.0, 3.5, 4.5…\n$ imdb_norm_round            <dbl> 4.0, 3.5, 4.0, 2.5, 2.5, 3.5, 3.5, 3.5, 3.5…\n$ metacritic_user_vote_count <int> 1330, 249, 627, 31, 88, 34, 17, 124, 62, 54…\n$ imdb_user_vote_count       <int> 271107, 65709, 103660, 3136, 19560, 39373, …\n$ fandango_votes             <int> 14846, 12640, 12055, 1793, 1021, 397, 252, …\n$ fandango_difference        <dbl> 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5…"
  },
  {
    "objectID": "slides/lec-2.html#data-visualization",
    "href": "slides/lec-2.html#data-visualization",
    "title": "Simple Linear Regression",
    "section": "Data visualization",
    "text": "Data visualization"
  },
  {
    "objectID": "slides/lec-2.html#fit-a-line",
    "href": "slides/lec-2.html#fit-a-line",
    "title": "Simple Linear Regression",
    "section": "Fit a line",
    "text": "Fit a line\n… to describe the relationship between the critics and audience score"
  },
  {
    "objectID": "slides/lec-2.html#terminology",
    "href": "slides/lec-2.html#terminology",
    "title": "Simple Linear Regression",
    "section": "Terminology",
    "text": "Terminology\n\n\n\nOutcome, Y: variable describing the outcome of interest\nPredictor, X: variable used to help understand the variability in the outcome"
  },
  {
    "objectID": "slides/lec-2.html#regression-model-1",
    "href": "slides/lec-2.html#regression-model-1",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\nA regression model is a function that describes the relationship between the outcome, \\(Y\\), and the predictor, \\(X\\).\n\\[\\begin{aligned} Y &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/lec-2.html#regression-model-2",
    "href": "slides/lec-2.html#regression-model-2",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n$$\n\\[\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n\n\n&= \\color{purple}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon\n\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "slides/lec-2.html#regression-model-residuals",
    "href": "slides/lec-2.html#regression-model-residuals",
    "title": "Simple Linear Regression",
    "section": "Regression model + residuals",
    "text": "Regression model + residuals\n\n\n\\[\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\color{blue}{\\textbf{Error}} \\\\[8pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[8pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[8pt]\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lec-2.html#simple-linear-regression-1",
    "href": "slides/lec-2.html#simple-linear-regression-1",
    "title": "Simple Linear Regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nUse simple linear regression to model the relationthip between a quantitative outcome (\\(Y\\)) and a single quantitative predictor (\\(X\\)): \\[\\Large{Y = \\beta_0 + \\beta_1 X + \\epsilon}\\]\n\n\n\\(\\beta_1\\): True slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\beta_0\\): True intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\epsilon\\): Error (residual)"
  },
  {
    "objectID": "slides/lec-2.html#simple-linear-regression-2",
    "href": "slides/lec-2.html#simple-linear-regression-2",
    "title": "Simple Linear Regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\\[\\Large{\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X}\\]\n\n\\(\\hat{\\beta}_1\\): Estimated slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\hat{\\beta}_0\\): Estimated intercept of the relationship between \\(X\\) and \\(Y\\)\nNo error term!"
  },
  {
    "objectID": "slides/lec-2.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "href": "slides/lec-2.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "title": "Simple Linear Regression",
    "section": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)",
    "text": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)"
  },
  {
    "objectID": "slides/lec-2.html#residuals",
    "href": "slides/lec-2.html#residuals",
    "title": "Simple Linear Regression",
    "section": "Residuals",
    "text": "Residuals\n\n\\[\\text{residual} = \\text{observed} - \\text{predicted} = y - \\hat{y}\\]"
  },
  {
    "objectID": "slides/lec-2.html#least-squares-line",
    "href": "slides/lec-2.html#least-squares-line",
    "title": "Simple Linear Regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nThe residual for the \\(i^{th}\\) observation is\n\n\\[e_i = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i\\]\n\nThe sum of squared residuals is\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nThe least squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/lec-2.html#properties-of-least-squares-regression",
    "href": "slides/lec-2.html#properties-of-least-squares-regression",
    "title": "Simple Linear Regression",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(X\\) and average \\(Y\\): \\(\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\\)\nThe slope has the same sign as the correlation coefficient: \\(\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}\\)\nThe sum of the residuals is zero: \\(\\sum_{i = 1}^n \\epsilon_i = 0\\)\nThe residuals and \\(X\\) values are uncorrelated"
  },
  {
    "objectID": "slides/lec-2.html#estimating-the-slope",
    "href": "slides/lec-2.html#estimating-the-slope",
    "title": "Simple Linear Regression",
    "section": "Estimating the slope",
    "text": "Estimating the slope\n\\[\\large{\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}}\\]\n\n\n\\[\\begin{aligned}\ns_X &= 30.1688 \\\\\ns_Y &=  20.0244 \\\\\nr &= 0.7814\n\\end{aligned}\\]\n\n$$\n\\[\\begin{aligned}\n\n\n\\hat{\\beta}_1 &= 0.7814 \\times \\frac{20.0244}{30.1688} \\\\\n&= 0.5187\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "slides/lec-2.html#estimating-the-intercept",
    "href": "slides/lec-2.html#estimating-the-intercept",
    "title": "Simple Linear Regression",
    "section": "Estimating the intercept",
    "text": "Estimating the intercept\n\\[\\large{\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}}\\]\n\n\n$$\n\\[\\begin{aligned}\n\n\n&\\bar{x} = 60.8493 \\\\\n&\\bar{y} = 63.8767 \\\\\n&\\hat{\\beta}_1 = 0.5187\n\\end{aligned}\\]\n$$\n\n\\[\\begin{aligned}\\hat{\\beta}_0 &= 63.8767 - 0.5187 \\times 60.8493 \\\\\n&= 32.3142\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lec-2.html#interpreting-the-slope",
    "href": "slides/lec-2.html#interpreting-the-slope",
    "title": "Simple Linear Regression",
    "section": "Interpreting the slope",
    "text": "Interpreting the slope\nPoll: The slope of the model for predicting audience score from critics score is 32.3142. Which of the following is the best interpretation of this value?\n\nFor every one point increase in the critics score, the audience score goes up by 0.5187 points, on average.\nFor every one point increase in the critics score, we expect the audience score to be higher by 0.5187 points, on average.\nFor every one point increase in the critics score, the audience score goes up by 0.5187 points.\nFor every one point increase in the audience score, the critics score goes up by 0.5187 points, on average."
  },
  {
    "objectID": "slides/lec-2.html#interpreting-slope-intercept",
    "href": "slides/lec-2.html#interpreting-slope-intercept",
    "title": "Simple Linear Regression",
    "section": "Interpreting slope & intercept",
    "text": "Interpreting slope & intercept\n\\[\\widehat{\\text{audience}} = 32.3142 + 0.5187 \\times \\text{critics}\\]\n\n\nSlope: For every one point increase in the critics score, we expect the audience score to be higher by 0.5187 points, on average.\nIntercept: If the critics score is 0 points, we expect the audience score to be 32.3142 points, on average."
  },
  {
    "objectID": "slides/lec-2.html#is-the-intercept-meaningful",
    "href": "slides/lec-2.html#is-the-intercept-meaningful",
    "title": "Simple Linear Regression",
    "section": "Is the intercept meaningful?",
    "text": "Is the intercept meaningful?\n✅ The intercept is meaningful in context of the data if\n\nthe predictor can feasibly take values equal to or near zero or\nthe predictor has values near zero in the observed data\n\n\n🛑 Otherwise, it might not be meaningful!"
  },
  {
    "objectID": "slides/lec-2.html#making-a-prediction",
    "href": "slides/lec-2.html#making-a-prediction",
    "title": "Simple Linear Regression",
    "section": "Making a prediction",
    "text": "Making a prediction\nSuppose that a movie has a critics score of 50. According to this model, what is the movie’s predicted audience score?\n\\[\n\\begin{aligned}\n\\widehat{\\text{audience}} &= 32.3142 + 0.5187 \\times \\text{critics} \\\\\n&= 32.3142 + 0.5187 \\times 50 \\\\\n&= 58.2492\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-2.html#extrapolation",
    "href": "slides/lec-2.html#extrapolation",
    "title": "Simple Linear Regression",
    "section": "Extrapolation",
    "text": "Extrapolation\nExtrapolation is prediction outside of the ranged covered by data.\nSuppose that a movie has a critics score of 0. According to this model, what is the movie’s predicted audience score?"
  },
  {
    "objectID": "slides/lec-2.html#recap-1",
    "href": "slides/lec-2.html#recap-1",
    "title": "Simple Linear Regression",
    "section": "Recap",
    "text": "Recap\n\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative outcome variable.\nUsed the least squares method to estimate the slope and intercept.\nWe interpreted the slope and intercept.\n\n\nSlope: For every one unit increase in \\(x\\), we expect y to be higher/lower by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units.\n\n\nPredicted the response given a value of the predictor variable.\nDefined extrapolation and why we should avoid it.\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\qquad(1)\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\qquad(2)\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\qquad(3)\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n$$\n = = - \n$$ {#eq-matrix_mean}"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n$$\n{i=1}^{n} e{i}^2 = ^T = ( - )^T( - )\n$$ {#eq-sum_sq_resid}\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n$$\n( - )^T( - ) = (^T - ^T - ({T}T + {T}T )\n$$ {#eq-model_equation}\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, ?@eq-model_equation becomes\n$$\n^T - 2 T{T} + {T}T \n$$ {#eq-model_equation}\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes ?@eq-sum_sq_resid, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n$$\n\\[\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\\]\n$$ {#eq-ete}\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n$$\n\\[\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\\]\n$$ {#eq-expected_error}\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write ?@eq-expected_error as\n$$\nE[^T] =\n\\[\\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix}\\]\n= ^2 \n$$ {#eq-expected_error2}\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n$$\n\\[\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\\]\n$$ {#eq-est-beta}\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n$$\n\\[\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\\]\n$$ {#eq-var-cov}"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\qquad(1)\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\qquad(2)\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\qquad(3)\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n$$\n = = - \n$$ {#eq-matrix_mean}"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n$$\n = (T){-1}^T\n$$ {#eq-beta-hat}\nCombining ?@eq-matrix_mean and ?@eq-beta-hat, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n$$\n\\[\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\\]\n$$ {#eq-y-hat}\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus ?@eq-y-hat becomes\n$$\n = \n$$ {#eq-y-hat-matrix}\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n$$\nh_{ii} = + \n$$\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from ?@eq-matrix_mean using ?@eq-y-hat-matrix.\n$$\n\\[\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\\]\n$$ {#eq-resid-hat}\nNote that the identity matrix and hat matrix are idempotent, i.e. \\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and ?@eq-resid-hat, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n$$\n\\[\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\\]\n$$ {#eq-resid-var}\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n$$\nstd.res_i = \n$$\nThe expected value of the residuals is 0, i.e. \\(E(e_i) = 0\\). From ?@eq-resid-var), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n$$\nstd.res_i = \n$$ {#eq-std-resid}"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\nCook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the \\(i^{th}\\) observation can be written as\n$$\nD_i = \n$$ {#eq-cooksd}\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since ?@eq-cooksd-v2 below is mathematically equivalent to ?@eq-cooksd.\n$$\nD_i = std.res_i^2= \n$$ {#eq-cooksd-v2}"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "This document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation 1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\qquad(1)\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation 1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation 2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\qquad(2)\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation 3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\qquad(3)\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation 4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\qquad(4)\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that’s more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\qquad(5)\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\qquad(6)\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation 5 and Equation 6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\qquad(7)\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation 7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\qquad(8)\\]"
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Equation 1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\qquad(1)\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in Equation 2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\qquad(2)\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in Equation 2 for interpretations and predictions, we will use Equation 3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\qquad(3)\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Equation 2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\qquad(4)\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in Equation 5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\qquad(5)\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\qquad(1)\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation 1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\qquad(2)\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation 2, i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\qquad(3)\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\qquad(4)\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation 4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike’s Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\qquad(5)\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\qquad(6)\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ tidyr   1.1.4     ✓ stringr 1.4.0\n✓ readr   2.1.1     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(tidymodels)\n\nRegistered S3 method overwritten by 'tune':\n  method                   from   \n  required_pkgs.model_spec parsnip\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 0.1.4 ──\n\n\n✓ broom        0.7.10         ✓ rsample      0.1.1     \n✓ dials        0.0.10         ✓ tune         0.1.6     \n✓ infer        1.0.1.9000     ✓ workflows    0.2.4     \n✓ modeldata    0.1.1          ✓ workflowsets 0.1.0     \n✓ parsnip      0.1.7          ✓ yardstick    0.0.9     \n✓ recipes      0.2.0          \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(knitr)"
  },
  {
    "objectID": "computing-pipelines.html#simple-linear-regression",
    "href": "computing-pipelines.html#simple-linear-regression",
    "title": "Pipelines",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nModel fitting\nFit model:\n\npenguins_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(body_mass_g ~ flipper_length_mm, data = penguins)\n\nTidy model output:\n\ntidy(penguins_fit)\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic   p.value\n  <chr>                <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\nFormat model output as table:\n\ntidy(penguins_fit) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5780.831\n305.815\n-18.903\n0\n\n\nflipper_length_mm\n49.686\n1.518\n32.722\n0\n\n\n\n\n\nAugment data with model:\n\naugment(penguins_fit$fit)\n\n# A tibble: 342 × 9\n   .rownames body_mass_g flipper_length_… .fitted  .resid    .hat .sigma .cooksd\n   <chr>           <int>            <int>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>\n 1 1                3750              181   3212.  538.   0.00881   394. 8.34e-3\n 2 2                3800              186   3461.  339.   0.00622   394. 2.33e-3\n 3 3                3250              195   3908. -658.   0.00344   393. 4.83e-3\n 4 5                3450              193   3808. -358.   0.00385   394. 1.60e-3\n 5 6                3650              190   3659.   -9.43 0.00469   395. 1.35e-6\n 6 7                3625              181   3212.  413.   0.00881   394. 4.91e-3\n 7 8                4675              195   3908.  767.   0.00344   393. 6.56e-3\n 8 9                3475              193   3808. -333.   0.00385   394. 1.39e-3\n 9 10               4250              190   3659.  591.   0.00469   394. 5.31e-3\n10 11               3300              186   3461. -161.   0.00622   395. 5.23e-4\n# … with 332 more rows, and 1 more variable: .std.resid <dbl>\n\n\n\n\nStatistical inference"
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, you’ll upload your PDF and them mark the page(s) where each question can be found. It’s OK if a question spans multiple pages, just mark them all. It’s also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, I’d rather you didn’t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you’re working in the containers we have provided for you. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.1.2: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone’s office hours here."
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Conversations as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Conversations), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email me at yunran.chen@duke.edu.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “STA 210” in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may not be replied for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. ARC services are available free to any Duke undergraduate student, in any year, studying in any discipline. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact theARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu,\nDuWell: (919) 681-8421, duwell@studentaffairs.duke.edu, or studentaffairs.duke.edu/duwell\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach. Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student’s behavior or health visit the website for resources and assistance. studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS). CAPS helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000. studentaffairs.duke.edu/caps\nBlue Devils Care. A convenient and cost-effective way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu\nTwo-Click Support. Duke Student Government and DukeReach partnership that connects students to help in just two clicks. bit.ly/TwoClickSupport\nWellTrack. Sign up for WellTrack at app.welltrack.com."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that we will be using Duke’s computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-sakai",
    "href": "course-support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission and discussion will take place on GitHub instead.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "exams/exam-1.html",
    "href": "exams/exam-1.html",
    "title": "Exam 1",
    "section": "",
    "text": "Grading (50 pts)\n\n\n\nPart\nPoints\n\n\n\n\nPart 1 - Conceptual (on Sakai)\n10\n\n\nPart 2 - Applied (on Gradescope)\n40\n\n\nTotal\n50"
  },
  {
    "objectID": "exams/exam-3.html",
    "href": "exams/exam-3.html",
    "title": "Exam 3",
    "section": "",
    "text": "Grading (50 pts)\n\n\n\nPart\nPoints\n\n\n\n\nPart 1 - Conceptual (on Sakai)\n10\n\n\nPart 2 - Applied (on Gradescope)\n40\n\n\nTotal\n50"
  },
  {
    "objectID": "exams/exam-2.html",
    "href": "exams/exam-2.html",
    "title": "Exam 2",
    "section": "",
    "text": "Grading (50 pts)\n\n\n\nPart\nPoints\n\n\n\n\nPart 1 - Conceptual (on Sakai)\n10\n\n\nPart 2 - Applied (on Gradescope)\n40\n\n\nTotal\n50"
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Project description",
    "section": "",
    "text": "Topic ideas due Sun, May 29\nProposal due Wed, June 8\nDraft report due Wed, June 15\nProject peer review of drafts due Thur, June 16\nFinal report due Wed, June 22\nVideo presentation + slides and final GitHub repo due Wed, June 22\nPresentation comments due Wed, June 22"
  },
  {
    "objectID": "project-description.html#introduction",
    "href": "project-description.html#introduction",
    "title": "Project description",
    "section": "Introduction",
    "text": "Introduction\nTL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group’s interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio, and all components of the project must be reproducible (with the exception of the presentation).\n\nLogistics\nYou will work on the project with your lab groups.\nThe four primary deliverables for the final project are\n\nA written, reproducible report detailing your analysis\nA GitHub repository corresponding to your report\nSlides + a video presentation\nFormal peer review on another team’s project"
  },
  {
    "objectID": "project-description.html#topic-ideas",
    "href": "project-description.html#topic-ideas",
    "title": "Project description",
    "section": "Topic ideas",
    "text": "Topic ideas\nIdentify 2-3 data sets you’re interested in potentially using for the final project. If you’re unsure where to find data, you can use the list of potential data sources in the Tips + Resources section as a starting point. It may also help to think of topics you’re interested in investigating and find data sets on those topics.\nThe purpose of submitting project ideas is to give you time to find data for the project and to make sure you have a data set that can help you be successful in the project. Therefore, you must use one of the data sets submitted as a topic idea, unless otherwise notified by the teaching team.\nThe data sets should meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns\nAt least 6 of the columns must be useful and unique predictor variables.\n\nIdentifier variables such as “name”, “social security number”, etc. are not useful predictor variables.\nIf you have multiple columns with the same information (e.g. “state abbreviation” and “state name”), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nObservations should reasonably meet the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nYou may not use data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\nPlease ask TA or me if you’re unsure whether your data set meets the criteria.\nFor each data set, include the following:\n\nIntroduction and data\n\nState the source of the data set.\nDescribe when and how it was originally collected (by the original data curator, not necessarily how you found the data)\nDescribe the observations and the general characteristics being measured in the data\n\n\n\nResearch question\n\nDescribe a research question you’re interested in answering using this data.\n\n\n\nGlimpse of data\n\nUse the glimpse function to provide an overview of each data set\n\nSubmit the PDF of the topic ideas to Gradescope. Mark all pages associated with each data set."
  },
  {
    "objectID": "project-description.html#project-proposal",
    "href": "project-description.html#project-proposal",
    "title": "Project description",
    "section": "Project proposal",
    "text": "Project proposal\nThe purpose of the project proposal is to help you think about your analysis strategy early.\nInclude the following in the proposal:\n\nSection 1 - Introduction\nThe introduction section includes\n\nan introduction to the subject matter you’re investigating\nthe motivation for your research question (citing any relevant literature)\nthe general research question you wish to explore\nyour hypotheses regarding the research question of interest.\n\n\n\nSection 2 - Data description\nIn this section, you will describe the data set you wish to explore. This includes\n\ndescription of the observations in the data set,\ndescription of how the data was originally collected (not how you found the data but how the original curator of the data collected it).\n\n\n\nSection 3 - Analysis approach\nIn this section, you will provide a brief overview of your analysis approach. This includes:\n\nDescription of the response variable.\nVisualization and summary statistics for the response variable.\nList of variables that will be considered as predictors\nRegression model technique (multiple linear regression and logistic regression)\n\n\n\nData dictionary (aka code book)\nSubmit a data dictionary for all the variables in your data set in the README of your project repo, in the data folder. Link to this file from your proposal writeup.\n\n\nSubmission\nPush all of your final changes to the GitHub repo, and submit the PDF of your proposal to Gradescope.\n\n\nProposal grading\n\n\n\nTotal\n10 pts\n\n\n\n\nIntroduction\n3 pts\n\n\nData description\n2 pts\n\n\nAnalysis plan\n4 pts\n\n\nData dictionary\n1 pts\n\n\n\nEach component will be graded as follows:\n\nMeets expectations (full credit): All required elements are completed and are accurate. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nClose to expectations (half credit): There are some elements missing and/or inaccurate. There are some issues with formatting.\nDoes not meet expectations (no credit): Major elements missing. Work is not neatly formatted and would not be presentable in a professional setting."
  },
  {
    "objectID": "project-description.html#draft-report",
    "href": "project-description.html#draft-report",
    "title": "Project description",
    "section": "Draft report",
    "text": "Draft report\nThe purpose of the draft and peer review is to give you an opportunity to get early feedback on your analysis. Therefore, the draft and peer review will focus primarily on the exploratory data analysis, modeling, and initial interpretations.\nWrite the draft in the written-report.qmd file in your project repo. You do not need to submit the draft on Gradescope.\nBelow is a brief description of the sections to focus on in the draft:\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.\n\n\nResults\nIn this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics.\nThis section also includes initial interpretations and conclusions drawn from the model."
  },
  {
    "objectID": "project-description.html#peer-review",
    "href": "project-description.html#peer-review",
    "title": "Project description",
    "section": "Peer review",
    "text": "Peer review\nCritically reviewing others’ work is a crucial part of the scientific process, and STA 210 is no exception. Each lab team will be assigned two other teams’s projects to review. Each team should push their draft to their GitHub repo by the due date. One lab in the following week will be dedicated to the peer review, and all reviews will be due by the end of that lab session.\nDuring the peer review process, you will be provided read-only access to your partner teams’ GitHub repos. Provide your review in the form of GitHub issues to your partner team’s GitHub repo using the issue template provided. The peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the partner team’s report: the research context and motivation, exploratory data analysis, modeling, interpretations, and conclusions.\n\nProcess and questions\nSpend ~30 mins to review the other team’s project.\n\nOpen the repo of the team you’re reviewing, read their project draft, and browser around the rest of their repo.\nThen, go to the Issues tab in that repo, click on New issue, and click on Get started for the Peer review issue. Fill out this issue, answering the following questions:\n\nPeer review by: [NAME OF TEAM DOING THE REVIEW]\nNames of team members that participated in this review: [FULL NAMES OF TEAM MEMBERS DOING THE REVIEW]\nDescribe the goal of the project.\nDescribe the data used or collected, if any. If the proposal does not include the use of a specific dataset, comment on whether the project would be strengthened by the inclusion of a dataset.\nDescribe the approaches, tools, and methods that will be used.\nIs there anything that is unclear from the proposal?\nProvide constructive feedback on how the team might be able to improve their project. Make sure your feedback includes at least one comment on the statistical modeling aspect of the project, but do feel free to comment on aspects beyond the modeling.\nWhat aspect of this project are you most interested in and would like to see highlighted in the presentation.\nProvide constructive feedback on any issues with file and/or code organization.\n(Optional) Any further comments or feedback?"
  },
  {
    "objectID": "project-description.html#written-report",
    "href": "project-description.html#written-report",
    "title": "Project description",
    "section": "Written report",
    "text": "Written report\nYour written report must be completed in the written-report.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\nBefore you finalize your write up, make sure the printing of code chunks is off with the option echo = FALSE.\nYou will submit the PDF of your final report on Gradescope.\nThe PDF you submit must match the files in your GitHub repository exactly. The mandatory components of the report are below. You are free to add additional sections as necessary. The report, including visualizations, should be no more than 10 pages long. is no minimum page requirement; however, you should comprehensively address all of the analysis and report.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\nThe written report is worth 40 points, broken down as follows\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction/data\n6 pts\n\n\nMethodology\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion + conclusion\n6 pts\n\n\nOrganization + formatting\n4 pts\n\n\n\nClick here for a PDF of the written report rubric.\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\nGrading criteria\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The explanatory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\n\nGrading criteria\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to select the final model; the approach is clearly described in the report. The model selection process took into account potential interaction effects and addressed any violations in model conditions. The model conditions and diagnostics are thoroughly and accurately assessed for their model. If violations of model conditions are still present, there was a reasonable attempt to address the violations based on the course content.\n\n\n\nResults\nThis is where you will output the final model with any relevant model fit statistics.\nDescribe the key results from the model. The goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\n\nGrading criteria\nThe model fit is clearly assessed, and interesting findings from the model are clearly described. Interpretations of model coefficients are used to support the key findings and conclusions, rather than merely listing the interpretation of every model coefficient. If the primary modeling objective is prediction, the model’s predictive power is thoroughly assessed.\n\n\n\nDiscussion + Conclusion\nIn this section you’ll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\n\nGrading criteria\nOverall conclusions from analysis are clearly described, and the model results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\nOrganization + formatting\nThis is an assessment of the overall presentation and formatting of the written report.\n\nGrading criteria\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages."
  },
  {
    "objectID": "project-description.html#video-presentation-slides",
    "href": "project-description.html#video-presentation-slides",
    "title": "Project description",
    "section": "Video presentation + slides",
    "text": "Video presentation + slides\n\nSlides\nIn addition to the written report, your team will also create presentation slides and record a video presentation that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and discuss the primary conclusions. These slides should serve as a brief visual addition to your written report and will be graded for content and quality.\nFor submission, convert these slides to a .pdf document, and submit the PDF of the slides on Gradescope.\nThe slide deck should have no more than 6 content slides + 1 title slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 6 slides.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3: Highlights from EDA\nSlide 4: Final model\nSlide 5: Interesting findings from the model\nSlide 6: Conclusions + future work\n\n\n\nVideo presentation\nFor the video presentation, you can speak over your slide deck, similar to the lecture content videos. The video presentation must be no longer than 8 minutes. It is fine if the video is shorter than 8 minutes, but it cannot exceed 8 minutes. You may use can use any platform that works best for your group to record your presentation. Below are a few resources on recording videos:\n\nRecording presentations in Zoom\nApple Quicktime for screen recording\nWindows 10 built-in screen recording functionality\nKap for screen recording\n\nOnce your video is ready, upload the video to Warpwire, then embed the video in an new discussion post on Conversations.\n\nTo upload your video to Warpwire:\n\nClick the Warpwire tab in the course Sakai site.\nClick the “+” and select “Upload files”.\nLocate the video on your computer and click to upload.\nOnce you’ve uploaded the video to Warpwire, click to share the video and copy the video’s URL. You will need this when you post the video in the discussion forum.\n\n\n\nTo post the video to the discussion forum\n\nClick the Presentations tab in the course Sakai site.\nClick the Presentations topic.\nClick “Start a new conversation”.\nMake the title “Your Team Name: Project Title”. For example, “Teaching Team: Our Awesome Presentation”.\nClick the Warpwire icon (between the table and shopping cart icons).\nSelect your video, then click “Insert 1 item.” This will embed your video in the conversation.\nUnder the video, paste the URL to your video.\nYou’re done!"
  },
  {
    "objectID": "project-description.html#presentation-comments",
    "href": "project-description.html#presentation-comments",
    "title": "Project description",
    "section": "Presentation comments",
    "text": "Presentation comments\nWatch the other group’s video, then click “Reply” to post a question for the group. You may not post a question that’s already been asked on the discussion thread. Additionally, the question should be (i) substantive (i.e. it shouldn’t be “Why did you use a bar plot instead of a pie chart”?), (ii) demonstrate your understanding of the content from the course, and (iii) relevant to that group’s specific presentation, i.e demonstrating that you’ve watched the presentation.\nThis portion of the project will be assessed individually."
  },
  {
    "objectID": "project-description.html#reproducibility-organization",
    "href": "project-description.html#reproducibility-organization",
    "title": "Project description",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization\nAll written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nThe GitHub repo should have the following structure:\n\nREADME: Short project description and data dictionary\nwritten-report.qmd & written-report.pdf: Final written report\n/data: Folder that contains the data set for the final project.\n/previous-work: Folder that contains the topic-ideas and project-proposal files.\n/presentation: Folder with the presentation slides.\n\nIf your presentation slides are online, you can put a link to the slides in a README.md file in the presentation folder.\n\n\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo. The repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable."
  },
  {
    "objectID": "project-description.html#peer-teamwork-evaluation",
    "href": "project-description.html#peer-teamwork-evaluation",
    "title": "Project description",
    "section": "Peer teamwork evaluation",
    "text": "Peer teamwork evaluation\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member by assigning a contribution percentage for each team member. Filling out the survey is a prerequisite for getting credit on the team member evaluation. If you are suggesting that an individual did less than half the expected contribution given your team size (e.g., for a team of four students, if a student contributed less than 12.5% of the total effort), please provide some explanation. If any individual gets an average peer score indicating that this was the case, their grade will be assessed accordingly.\nIf you have concerns with the teamwork and/or contribution from any team members, please email me by the project video deadline. You only need to email me if you have concerns. Otherwise, I will assume everyone on the team equally contributed and will receive full credit for the teamwork portion of the grade."
  },
  {
    "objectID": "project-description.html#overall-grading",
    "href": "project-description.html#overall-grading",
    "title": "Project description",
    "section": "Overall grading",
    "text": "Overall grading\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nTopic ideas\n5 pts\n\n\nProject proposal\n10 pts\n\n\nPeer review\n10 pts\n\n\nWritten report\n40 pts\n\n\nSlides + video presentation\n20 pts\n\n\nReproducibility + organization\n5 pts\n\n\nVideo comments\n5 pts\n\n\nPeer teamwork evaluation\n5 pts\n\n\n\nNote: No late project reports or videos are accepted.\n\nGrading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.\n\n\n\nLate work policy\nThere is no late work accepted on this project. Be sure to turn in your work early to avoid any technological mishaps."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus."
  },
  {
    "objectID": "course-syllabus.html#course-info",
    "href": "course-syllabus.html#course-info",
    "title": "Syllabus",
    "section": "Course info",
    "text": "Course info\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nLectures\nWeekdays\n11:00 am - 12:15 pm\nOld Chemistry 101\n\n\nLabs\nMon and Wed\n2:00 pm - 3:15 pm\nOld Chemistry 101\n\n\nYunran’s OH\nThursdays\n2:00 pm - 3:00 pm\nOld Chemistry 025\n\n\n\nMondays\n7:00 pm - 8:00 pm\nZoom\n\n\nJoseph’s OH\nTue and Thur\n8:00 pm - 9:00 pm\nZoom"
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the semester, you will be able to…\n\nanalyze real-world data to answer questions about multivariable relationships.\nfit and evaluate linear and logistic regression models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\ncommunicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "course-syllabus.html#community",
    "href": "course-syllabus.html#community",
    "title": "Syllabus",
    "section": "Community",
    "text": "Community\n\nDuke Community Standard\nAs a student in this course, you have agreed to uphold the Duke Community Standard as well as the practices specific to this course.\n\n\n\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Duke’s Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean is an excellent resource.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nWhere to get help\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. Please use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the course forum Conversations. There is a chance another student has already asked a similar question, so please check the other posts in Conversations before adding a new question. If you know the answer to a question posted in the discussion forum, I encourage you to respond!\nEmails should be reserved for questions not appropriate for the public forum. If you email me, please include “STA 210” in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Emails sent Friday evening - Sunday may be answered on Monday.\n\nCheck out the Support page for more resources."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nWhile there is no official textbook for the course, we will be assigning readings from the following textbooks.\n\nR for Data Science by Garret Grolemund and Hadley Wickham\nIntroduction to Modern Statistics by Mine Çetinkaya-Rundel and Johanna Hardin\nTidy modeling with R by Max Kuhn and Julia Silge\nBeyond Multiple Linear Regression by Paul Roback and Julie Legler"
  },
  {
    "objectID": "course-syllabus.html#lectures-and-labs",
    "href": "course-syllabus.html#lectures-and-labs",
    "title": "Syllabus",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nThe goal of both the lectures and the labs is for them to be as interactive as possible. My role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. You are expected to attend all lecture and lab sessions and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded. In addition to application exercises will be periodic activities help build a learning community. These will be short, fun activities that will help everyone in the class connect throughout the semester.\nYou are expected to bring a laptop to each class so that you can take part in the in-class exercises. Please make sure your laptop is fully charged before you come to class as the number of outlets in the classroom will not be sufficient to accommodate everyone. More information on loaner laptops can be found here."
  },
  {
    "objectID": "course-syllabus.html#teams",
    "href": "course-syllabus.html#teams",
    "title": "Syllabus",
    "section": "Teams",
    "text": "Teams\nYou will be assigned to a team at the beginning of each week. You are encouraged to sit with your teammates in lecture and you will also work with them in the lab sessions. All team members are expected to contribute equally to the completion of the labs and project and you will be asked to evaluate your team members throughout the semester. Failure to adequately contribute to an assignment will result in a penalty to your mark relative to the team’s overall mark.\nYou are expected to make use of the provided GitHub repository as their central collaborative platform. Commits to this repository will be used as a metric (one of several) of each team member’s relative contribution for each project."
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nAssessment for the course is comprised of six components: application exercises, homework assignments, labs, exams, projects, and teamwork.\n\nApplication exercises\nParts of some lectures will be dedicated to working on Application Exercises (AEs). These exercises which give you an opportunity to practice apply the statistical concepts and code introduced in the readings and lectures. These AEs are released on Monday, Wednesday and Friday. They are due within two days of the corresponding lecture period. Specifically, AEs from Monday and Wednesday lectures are due Wednesday and Friday by 11:59 pm ET respectively. AEs from Friday lectures are due Sunday by 11:59 pm ET.\nBecause these AEs are for practice, they will be graded based on completion, i.e., a good-faith effort has been made in attempting all parts. Successful on-time completion of at least 80% of AEs will result in full credit for AEs in the final course grade.\n\n\nLabs\nIn labs, you will apply the concepts discussed in lecture to various data analysis scenarios, with a focus on the computation. Most lab assignments will be completed in teams, and all team members are expected to contribute equally to the completion of each assignment. You are expected to use the team’s GitHub repository on the course’s GitHub organization as the central platform for collaboration. Commits to this repository will be used as a metric of each team member’s relative contribution for each lab, and there will be periodic peer evaluation on the team collaboration. Lab assignments will be completed using Quarto, correspond to an appropriate GitHub repository, and submitted for grading in Gradescope.\n\n\nHomework\nIn homework, you will apply what you’ve learned during lecture and lab to complete data analysis tasks. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. Similar to lab assignments, homework must be typed up using Quarto and GitHub and submitted as a PDF in Gradescope.\nOne homework assignment will be dedicated to a statistics experience. The statistics experience is an opportunity to engage with statistics and data science outside of the classroom through podcasts, books, seminars, data analysis competitions, and other activities. As you complete these experiences, the goal is to consider how the material you’re learning in the course connects with society more broadly.\n\n\nExams\nThere will be three, take-home, open-note exams. Through these exams you have the opportunity to demonstrate what you’ve learned in the course thus far. The exams will focus on the conceptual understanding of the content, and they may also include small analysis and computational tasks. The content of the exam will be related to the content in the prepare, practice, and perform assignments. More details about the exams will be given during the semester.\n\n\nProject\nThe purpose of the project is to apply what you’ve learned throughout the semester to analyze an interesting, data-driven research question. The project will be completed with your teams, and each team will present their work in class and in writing during the final exam period. More information about the project will be provided during the semester."
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nApplication exercises\n3%\n\n\nHomework\n35% (7% x 5)\n\n\nProject\n15%\n\n\nLab\n14% (2.5% x 6)\n\n\nExam 01\n10%\n\n\nExam 02\n10%\n\n\nExam 03\n10%\n\n\nTeamwork\n2%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n>= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n< 60"
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic integrity\nTL;DR: Don’t cheat!\nAll students must adhere to the Duke Community Standard (DCS): Duke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nTo uphold the Duke Community Standard:\nStudents affirm their commitment to uphold the values of the Duke University community by signing a pledge that states:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors;\nI will act if the Standard is compromised\n\nRegardless of course delivery format, it is your responsibility to understand and follow Duke policies regarding academic integrity, including doing one’s own work, following proper citation of sources, and adhering to guidance around group work projects. Ignoring these requirements is a violation of the Duke Community Standard. If you have any questions about how to follow these requirements, please contact Jeanna McCullers (jeanna.mccullers@duke.edu), Director of the Office of Student Conduct.\n\n\nCollaboration policy\nOnly work that is clearly assigned as team work should be completed collaboratively.\n\nThe homework assignments must be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what’s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the teaching assistants.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\n\n\n\nPolicy on sharing and reusing code\nI am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course’s policy is that you may make use of any online resources (e.g.RStudio Community, StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. On individual assignments you may not directly share code with another student in this class, and on team assignments you may not directly share code with another team in this class.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework and lab assignment will be dropped to accommodate such circumstances.\n\nHomework and labs may be submitted up to 1 day late. There will be a 5% deduction for each 6-hour period the assignment is late.\nThere is no late work accepted for application exercises, since these are designed to help you prepare for labs and homework.\nThe late work policy for exams will be provided with the exam instructions.\nThe late work policy for the project will be provided with the project instructions.\n\n\n\nRegrade request policy\nRegrade requests must be submitted on Gradescope within 24 hours of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the final project presentations.\n\n\nAttendance policy\nResponsibility for class attendance rests with individual students. Since regular and punctual class attendance is expected, students must accept the consequences of failure to attend. More details on Trinity attendance policies are available here.\nHowever, there may be many reasons why you cannot be in class on a given day, particularly with possible extra personal and academic stress and health concerns. If you miss a lecture, make sure to review the material before the next class session. Lab time is dedicated to working on your lab assignments and collaborating with your teammates on your project. If you miss a lab session, make sure to communicate with your team about how you can make up your contribution. Given the technologies we use in the course, this is straightforward to do asynchronously. If you know you’re going to miss a lab session and you’re feeling well enough to do so, notify your teammates ahead of time. Overall these policies are put in place to ensure communication between team members, respect for each others’ time, and also to give you a safety net in the case of illness or other reasons that keep you away from attending class.\n\n\nAttendance policy related to COVID symptoms, exposure, or infection\nStudent health, safety, and well-being are the university’s top priorities. Please wear a mask when attending the class. To help ensure your well-being and the well-being of those around you, please do not come to class if you have symptoms related to COVID-19, have had a known exposure to COVID-19, or have tested positive for COVID-19. If any of these situations apply to you, you must follow university guidance related to the ongoing COVID-19 pandemic and current health and safety protocols. If you are experiencing any COVID-19 symptoms, contact student health at 919-681-9355. To keep the university community as safe and healthy as possible, you will be expected to follow these guidelines. Please reach out to me and your academic dean as soon as possible if you need to quarantine or isolate so that we can discuss arrangements for your continued participation in class.\n\n\nInclement weather policy\nIn the event of inclement weather or other connectivity-related events that prohibit class attendance, I will notify you how we will make up missed course content and work. This might entail holding the class on Zoom synchronously or watching a recording of the class."
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nMay 11: Classes begin (Monday meeting schedule).\nMay 12: Regular class meeting schedule begins.\nMay 13: Drop/add for term 1 ends.\nMay 30: Memorial Day holiday. No classes are held.\nJune 8: Last day to withdraw with W.\nJune 15: Last lab.\nJune 17: Classes end.\nJune 20: Juneteenth holiday. No classes/office hour are held.\nJune 21: Reading period.\nJune 22: Final project presentation. Due date for final project report.\n\nClick here for the full Duke academic calendar."
  },
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it’s been resolved. If there’s a deadline coming up soon, post on the course forum to let us know that there’s an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don’t anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you’ve tried and the errors you see (including verbatim errors and/or screenshots).\nEither way you can also fill out the form here, which will notify our the R TA for the department as well as our undergraduate coordinator. They’ll be able to help diagnose the issue."
  },
  {
    "objectID": "slides/lec-26.html#pivot_wider-function-in-tidyr-pkg",
    "href": "slides/lec-26.html#pivot_wider-function-in-tidyr-pkg",
    "title": "Data Wrangling",
    "section": "pivot_wider function in tidyr pkg",
    "text": "pivot_wider function in tidyr pkg\n\npivot_wider: from long to wide\n\n\n\ntable2 %>%\n    pivot_wider(names_from = key, values_from = value)"
  },
  {
    "objectID": "slides/lec-26.html#separate-function-in-tidyr-pkg",
    "href": "slides/lec-26.html#separate-function-in-tidyr-pkg",
    "title": "Data Wrangling",
    "section": "separate function in tidyr pkg",
    "text": "separate function in tidyr pkg\n\nseparate: from 1 column to 2+ column\ncan sep based on digits or characters.\n\n\n\ntable3 %>% \n  separate(rate, into = c(\"cases\", \"population\"), sep = \"/\")"
  },
  {
    "objectID": "slides/lec-26.html#packages-in-tidyverse-1",
    "href": "slides/lec-26.html#packages-in-tidyverse-1",
    "title": "Data Wrangling",
    "section": "Packages in Tidyverse",
    "text": "Packages in Tidyverse"
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important\n\n\n\n\nDue dates:\n\nHW 5: Monday, June 13, 11:59pm\nHW 4 + Project drafts: Wednesday, June 15, 11:59pm\nLab 6 + AE 11: Friday, June 17, 11:59pm\nAE 12: Sunday, June 19, 11:59pm\nExam 3: Monday, June 20, 11:59pm\n\nReleased:\n\nHW 5\nHW 4\nLab 6\nAE 11\nExam 3\nAE 12"
  },
  {
    "objectID": "weeks/week-6.html#participate",
    "href": "weeks/week-6.html#participate",
    "title": "Week 6",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 24 - MultiLR: Prediction + inferential models\n🖥️ Lecture 25 - MultiLR: Predictive models\n🖥️ Lecture 26 - Data Wrangling\n🖥️ Lecture 27 - Exam 3 Review\n🖥️ Lecture 28 - Wrap up"
  },
  {
    "objectID": "weeks/week-6.html#practice",
    "href": "weeks/week-6.html#practice",
    "title": "Week 6",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 11 - Volcanoes\n📋 Application Exercise 12 - Exam 3 Review"
  },
  {
    "objectID": "weeks/week-6.html#perform",
    "href": "weeks/week-6.html#perform",
    "title": "Week 6",
    "section": "Perform",
    "text": "Perform\n✍️ HW 4 - Multinomial logistic regression\n💻 Lab 6 - Why Many Americans Don’t Vote\n✍️ Project - Peer review of drafts\n✍️ HW 5 - Statistics experience\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important\n\n\n\n\nDue dates:\n\nExam 3: Monday, June 20, 11:59pm\nProject presentation: Wednesday, June 22, 11:59pm\nProject presentation comments: Wednesday, June 22, 11:59pm\nProject write-up: Wednesday, June 22, 11:59pm"
  },
  {
    "objectID": "weeks/week-7.html#participate",
    "href": "weeks/week-7.html#participate",
    "title": "Week 7",
    "section": "Participate",
    "text": "Participate\nNone"
  },
  {
    "objectID": "weeks/week-7.html#practice",
    "href": "weeks/week-7.html#practice",
    "title": "Week 7",
    "section": "Practice",
    "text": "Practice\nNone"
  },
  {
    "objectID": "weeks/week-7.html#perform",
    "href": "weeks/week-7.html#perform",
    "title": "Week 7",
    "section": "Perform",
    "text": "Perform\n✍️ Project\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "slides/lec-28.html#remaining-deadlines-for-project",
    "href": "slides/lec-28.html#remaining-deadlines-for-project",
    "title": "Wrap up",
    "section": "Remaining deadlines for project",
    "text": "Remaining deadlines for project\n\nFinal report due Tue, June 22\nVideo presentation + slides and final GitHub repo due Tue, June 22\nPresentation comments due Tue, June 22\n\n\n\nAny questions related to projects?"
  },
  {
    "objectID": "slides/lec-28.html#grading",
    "href": "slides/lec-28.html#grading",
    "title": "Wrap up",
    "section": "Grading",
    "text": "Grading\n\nCheck your grades on Sakai, make sure they match Gradescope, email me before June 22\nWatch for new feedback released on Gradescope"
  },
  {
    "objectID": "slides/lec-28.html#evaluations",
    "href": "slides/lec-28.html#evaluations",
    "title": "Wrap up",
    "section": "Evaluations",
    "text": "Evaluations\n\nCourse evaluation\nTA evaluation"
  },
  {
    "objectID": "slides/lec-28.html#review",
    "href": "slides/lec-28.html#review",
    "title": "Wrap up",
    "section": "Review",
    "text": "Review\n\nCourse Review\nPrevious Expectation and Syllabus\nAbout Statistics\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  }
]