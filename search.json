[
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone‚Äôs office hours here."
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that can‚Äôt wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Conversations as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Conversations), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email me at yunran.chen@duke.edu.\nIf there is a question that‚Äôs not appropriate for the public forum, you are welcome to email me directly. If you email me, please include ‚ÄúSTA 210‚Äù in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may not be replied for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. ARC services are available free to any Duke undergraduate student, in any year, studying in any discipline. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact theARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu,\nDuWell: (919) 681-8421, duwell@studentaffairs.duke.edu, or studentaffairs.duke.edu/duwell\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach. Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student‚Äôs behavior or health visit the website for resources and assistance. studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS). CAPS helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000. studentaffairs.duke.edu/caps\nBlue Devils Care. A convenient and cost-effective way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu\nTwo-Click Support. Duke Student Government and DukeReach partnership that connects students to help in just two clicks. bit.ly/TwoClickSupport\nWellTrack. Sign up for WellTrack at app.welltrack.com."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that we will be using Duke‚Äôs computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-sakai",
    "href": "course-support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission and discussion will take place on GitHub instead.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\n\nDue dates:\n\nExam 1: Mon, May 23, 11:59pm ET\nLab 3 + AE 5: Fri, May 27, 11:59pm ET\nHW 2 + Project topic ideas: Sun, May 29, 11:59pm ET\n\nReleased:\n\nHW 2\nLab 3\nAE 5\nProject topic ideas"
  },
  {
    "objectID": "weeks/week-3.html#prepare",
    "href": "weeks/week-3.html#prepare",
    "title": "Week 3",
    "section": "Prepare",
    "text": "Prepare\nüìñ Read Tidy Modeling in R Chp 8: Feature engineering with recipes"
  },
  {
    "objectID": "weeks/week-3.html#participate",
    "href": "weeks/week-3.html#participate",
    "title": "Week 3",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Lecture 8 - Multiple linear regression (MLR)\nüñ•Ô∏è Lecture 10 - MLR: Types of predictors\nüñ•Ô∏è Lecture 11 - MLR: Model comparison\nüñ•Ô∏è Lecture 12 - MLR: Feature engineering\nüñ•Ô∏è Lecture 13 - MLR: Feature engineering (cont.)"
  },
  {
    "objectID": "weeks/week-3.html#practice",
    "href": "weeks/week-3.html#practice",
    "title": "Week 3",
    "section": "Practice",
    "text": "Practice\nüìã Application Exercise 5 - The Office"
  },
  {
    "objectID": "weeks/week-3.html#perform",
    "href": "weeks/week-3.html#perform",
    "title": "Week 3",
    "section": "Perform",
    "text": "Perform\nüñ•Ô∏è Lab 3 - Coffee ratings\n‚úÖ Exam 1\n‚úçÔ∏è HW 2 - Multiple linear regression\nüìÇ Project - Topic ideas\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\n\nDue dates:\n\nHW 1 + AE 2: Wed, May 18, 11:59pm ET\nLab 2 + AE 3: Fri, May 20, 11:59pm ET\nAE 4: Sun, May 22, 11:59pm ET\nExam 1: Mon, May 23, 11:59pm ET\n\nReleased:\n\nHW 1\nLab 2\nAE 2 + AE 3 + AE 4\nExam 1"
  },
  {
    "objectID": "weeks/week-2.html#prepare",
    "href": "weeks/week-2.html#prepare",
    "title": "Week 2",
    "section": "Prepare",
    "text": "Prepare\nüìñ Read Introduction to Modern Statistics, Sec 24.1: Case study: Sandwich store\nüìñ Read Introduction to Modern Statistics, Sec 24.2: Randomization test for the slope\nüìñ Read Introduction to Modern Statistics, Sec 24.3: Bootstrap confidence interval for the slope\nüìñ Read Introduction to Modern Statistics, Sec 24.4: Mathematical model for testing the slope\nüìñ Read Introduction to Modern Statistics, Sec 24.5: Mathematical model, interval for the slope\nüìñ Read Introduction to Modern Statistics, Sec 24.6: Checking model conditions\nüìñ Read Introduction to Modern Statistics, Sec 24.7: Chapter review\nüìñ Read Introduction to Modern Statistics, Sec 8.1: Indicator and categorical predictors\nüìñ Read Introduction to Modern Statistics, Sec 8.2: Many predictors in a model\nüìñ Read Introduction to Modern Statistics, Sec 8.3: Adjusted R-squared"
  },
  {
    "objectID": "weeks/week-2.html#participate",
    "href": "weeks/week-2.html#participate",
    "title": "Week 2",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Lecture 4 - SLR: Prediction + model evaluation\nüñ•Ô∏è Lecture 5 - SLR: Simulation-based inference\n\nüñ•Ô∏è Lab 2 - College scorecard\nüñ•Ô∏è Lecture 6 - SLR: Mathematical models for inference\nüñ•Ô∏è Lecture 7 - SLR: Model diagnostics\n\nüñ•Ô∏è Lecture 9 - Exam 1 review"
  },
  {
    "objectID": "weeks/week-2.html#practice",
    "href": "weeks/week-2.html#practice",
    "title": "Week 2",
    "section": "Practice",
    "text": "Practice\nüìã Application Exercise 2 - Bike rentals in DC\nüìã Application Exercise 3 - Checking model conditions\nüìã Application Exercise 4 - Exam 1 Review"
  },
  {
    "objectID": "weeks/week-2.html#perform",
    "href": "weeks/week-2.html#perform",
    "title": "Week 2",
    "section": "Perform",
    "text": "Perform\n‚å®Ô∏è Lab 2 - College scorecard\n‚úçÔ∏è HW 1 - In-person voting trends\n‚úÖ Exam 1\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "slides/lec-4.html#questions-from-last-week",
    "href": "slides/lec-4.html#questions-from-last-week",
    "title": "SLR: Prediction + model evaluation",
    "section": "Questions from last week",
    "text": "Questions from last week\n\nIn YAML, set format: pdf instead of pdf_format\nset_engine: what alternatives? can use show_engines(\"linear_reg\")\nIn R code chunk, can set #| message: false and #| warning: false\nTwo teams, check github!\nGood news: set up Github Classroom! You can find your own repo and directly clone and edit! No need to fork anymore."
  },
  {
    "objectID": "slides/lec-4.html#computational-setup",
    "href": "slides/lec-4.html#computational-setup",
    "title": "SLR: Prediction + model evaluation",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/lec-4.html#data-source",
    "href": "slides/lec-4.html#data-source",
    "title": "SLR: Prediction + model evaluation",
    "section": "Data source",
    "text": "Data source\n\nThe data come from usdata::county_2019\nThese data have been compiled from the 2019 American Community Survey"
  },
  {
    "objectID": "slides/lec-4.html#uninsurance-rate",
    "href": "slides/lec-4.html#uninsurance-rate",
    "title": "SLR: Prediction + model evaluation",
    "section": "Uninsurance rate",
    "text": "Uninsurance rate"
  },
  {
    "objectID": "slides/lec-4.html#high-school-graduation-rate",
    "href": "slides/lec-4.html#high-school-graduation-rate",
    "title": "SLR: Prediction + model evaluation",
    "section": "High school graduation rate",
    "text": "High school graduation rate"
  },
  {
    "objectID": "slides/lec-4.html#examining-the-relationship",
    "href": "slides/lec-4.html#examining-the-relationship",
    "title": "SLR: Prediction + model evaluation",
    "section": "Examining the relationship",
    "text": "Examining the relationship\n\nThe NC Labor and Economic Analysis Division (LEAD), which ‚Äúadministers and collects data, conducts research, and publishes information on the state‚Äôs economy, labor force, educational, and workforce-related issues‚Äù.\nSuppose that an analyst working for LEAD is interested in the relationship between uninsurance and high school graduation rates in NC counties.\n\n\n\nWhat type of visualization should the analyst make to examine the relationship between these two variables?"
  },
  {
    "objectID": "slides/lec-4.html#data-prep",
    "href": "slides/lec-4.html#data-prep",
    "title": "SLR: Prediction + model evaluation",
    "section": "Data prep",
    "text": "Data prep\n\ncounty_2019_nc <- county_2019 %>%\n  as_tibble() %>%\n  filter(state == \"North Carolina\") %>%\n  select(name, hs_grad, uninsured)\n\ncounty_2019_nc\n\n# A tibble: 100 √ó 3\n   name             hs_grad uninsured\n   <chr>              <dbl>     <dbl>\n 1 Alamance County     86.3      11.2\n 2 Alexander County    82.4       8.9\n 3 Alleghany County    77.5      11.3\n 4 Anson County        80.7      11.1\n 5 Ashe County         85.1      12.6\n 6 Avery County        83.6      15.9\n 7 Beaufort County     87.7      12  \n 8 Bertie County       78.4      11.9\n 9 Bladen County       81.3      12.9\n10 Brunswick County    91.3       9.8\n# ‚Ä¶ with 90 more rows"
  },
  {
    "objectID": "slides/lec-4.html#uninsurance-vs.-hs-graduation-rates",
    "href": "slides/lec-4.html#uninsurance-vs.-hs-graduation-rates",
    "title": "SLR: Prediction + model evaluation",
    "section": "Uninsurance vs.¬†HS graduation rates",
    "text": "Uninsurance vs.¬†HS graduation rates\n\n\nCode\nggplot(county_2019_nc,\n       aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  ) +\n  geom_point(data = county_2019_nc %>% filter(name == \"Durham County\"), aes(x = hs_grad, y = uninsured), shape = \"circle open\", color = \"#8F2D56\", size = 4, stroke = 2) +\n  geom_text(data = county_2019_nc %>% filter(name == \"Durham County\"), aes(x = hs_grad, y = uninsured, label = name), color = \"#8F2D56\", fontface = \"bold\", nudge_y = 3, nudge_x = 2)"
  },
  {
    "objectID": "slides/lec-4.html#modeling-the-relationship",
    "href": "slides/lec-4.html#modeling-the-relationship",
    "title": "SLR: Prediction + model evaluation",
    "section": "Modeling the relationship",
    "text": "Modeling the relationship\n\n\nCode\nggplot(county_2019_nc, aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#8F2D56\") +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  )"
  },
  {
    "objectID": "slides/lec-4.html#fitting-the-model",
    "href": "slides/lec-4.html#fitting-the-model",
    "title": "SLR: Prediction + model evaluation",
    "section": "Fitting the model",
    "text": "Fitting the model\nWith fit():\n\nnc_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(uninsured ~ hs_grad, data = county_2019_nc)\n\ntidy(nc_fit)\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   33.9      3.99        8.50 2.12e-13\n2 hs_grad       -0.262    0.0468     -5.61 1.88e- 7"
  },
  {
    "objectID": "slides/lec-4.html#augmenting-the-data",
    "href": "slides/lec-4.html#augmenting-the-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Augmenting the data",
    "text": "Augmenting the data\nWith augment() to add columns for predicted values (.fitted), residuals (.resid), etc.:\n\nnc_aug <- augment(nc_fit$fit)\nnc_aug\n\n# A tibble: 100 √ó 8\n   uninsured hs_grad .fitted  .resid   .hat .sigma    .cooksd .std.resid\n       <dbl>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>      <dbl>      <dbl>\n 1      11.2    86.3   11.3  -0.0633 0.0107   2.10 0.00000501    -0.0305\n 2       8.9    82.4   12.3  -3.39   0.0138   2.07 0.0186        -1.63  \n 3      11.3    77.5   13.6  -2.27   0.0393   2.09 0.0252        -1.11  \n 4      11.1    80.7   12.7  -1.63   0.0199   2.09 0.00633       -0.790 \n 5      12.6    85.1   11.6   1.02   0.0100   2.10 0.00122        0.492 \n 6      15.9    83.6   12.0   3.93   0.0112   2.06 0.0203         1.89  \n 7      12      87.7   10.9   1.10   0.0133   2.10 0.00191        0.532 \n 8      11.9    78.4   13.3  -1.44   0.0328   2.09 0.00830       -0.700 \n 9      12.9    81.3   12.6   0.324  0.0174   2.10 0.000218       0.157 \n10       9.8    91.3    9.95 -0.151  0.0291   2.10 0.0000806     -0.0734\n# ‚Ä¶ with 90 more rows"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-the-model-i",
    "href": "slides/lec-4.html#visualizing-the-model-i",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model I",
    "text": "Visualizing the model I\n\n\n\n\nBlack circles: Observed values (y = uninsured)"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-the-model-ii",
    "href": "slides/lec-4.html#visualizing-the-model-ii",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model II",
    "text": "Visualizing the model II\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-the-model-iii",
    "href": "slides/lec-4.html#visualizing-the-model-iii",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model III",
    "text": "Visualizing the model III\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\nMaroon triangles: Predicted values (y = .fitted)"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-the-model-iv",
    "href": "slides/lec-4.html#visualizing-the-model-iv",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model IV",
    "text": "Visualizing the model IV\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\nMaroon triangles: Predicted values (y = .fitted)\nGray dashed lines: Residuals"
  },
  {
    "objectID": "slides/lec-4.html#evaluating-the-model-fit",
    "href": "slides/lec-4.html#evaluating-the-model-fit",
    "title": "SLR: Prediction + model evaluation",
    "section": "Evaluating the model fit",
    "text": "Evaluating the model fit\n\nHow can we evaluate whether the model for predicting uninsurance rate from high school graduation rate for NC counties is a good fit?"
  },
  {
    "objectID": "slides/lec-4.html#two-statistics",
    "href": "slides/lec-4.html#two-statistics",
    "title": "SLR: Prediction + model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/lec-4.html#r-squared",
    "href": "slides/lec-4.html#r-squared",
    "title": "SLR: Prediction + model evaluation",
    "section": "R-squared",
    "text": "R-squared\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nUnitless (Having no units of measurement; such as a ratio or percentage of two numbers which have the same units.)\nCalculate with rsq():\n\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.243"
  },
  {
    "objectID": "slides/lec-4.html#interpreting-r-squared",
    "href": "slides/lec-4.html#interpreting-r-squared",
    "title": "SLR: Prediction + model evaluation",
    "section": "Interpreting R-squared",
    "text": "Interpreting R-squared\n\n\n\n\nüó≥Ô∏è Vote\nThe \\(R^2\\) of the model for predicting uninsurance rate from high school graduation rate for NC counties is 24.3%. Which of the following is the correct interpretation of this value?\n\n\nHigh school graduation rates correctly predict 24.3% of uninsurance rates in NC counties.\n24.3% of the variability in uninsurance rates in NC counties can be explained by high school graduation rates.\n24.3% of the variability in high school graduation rates in NC counties can be explained by uninsurance rates.\n24.3% of the time uninsurance rates in NC counties can be predicted by high school graduation rates."
  },
  {
    "objectID": "slides/lec-4.html#alternative-approach-for-r-squared",
    "href": "slides/lec-4.html#alternative-approach-for-r-squared",
    "title": "SLR: Prediction + model evaluation",
    "section": "Alternative approach for R-squared",
    "text": "Alternative approach for R-squared\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(nc_fit)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.243         0.235  2.09      31.5 0.000000188     1  -214.  435.  443.\n# ‚Ä¶ with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\nglance(nc_fit)$r.squared\n\n[1] 0.2430694"
  },
  {
    "objectID": "slides/lec-4.html#rmse",
    "href": "slides/lec-4.html#rmse",
    "title": "SLR: Prediction + model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the outcome variable\nCalculate with rmse():\n\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        2.07\n\n\nThe value of RMSE is not very meaningful on its own, but it‚Äôs useful for comparing across models (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/lec-4.html#obtaining-r-squared-and-rmse",
    "href": "slides/lec-4.html#obtaining-r-squared-and-rmse",
    "title": "SLR: Prediction + model evaluation",
    "section": "Obtaining R-squared and RMSE",
    "text": "Obtaining R-squared and RMSE\n\nUse rsq() and rmse(), respectively\n\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n\nFirst argument: data frame containing truth and estimate columns\nSecond argument: name of the column containing truth (observed outcome)\nThird argument: name of the column containing estimate (predicted outcome)"
  },
  {
    "objectID": "slides/lec-4.html#purpose-of-model-evaluation",
    "href": "slides/lec-4.html#purpose-of-model-evaluation",
    "title": "SLR: Prediction + model evaluation",
    "section": "Purpose of model evaluation",
    "text": "Purpose of model evaluation\n\n\\(R^2\\) tells us how our model is doing to predict the data we already have\nBut generally we are interested in prediction for a new observation, not for one that is already in our sample, i.e.¬†out-of-sample prediction\nWe have a couple ways of simulating out-of-sample prediction before actually getting new data to evaluate the performance of our models"
  },
  {
    "objectID": "slides/lec-4.html#spending-our-data",
    "href": "slides/lec-4.html#spending-our-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Spending our data",
    "text": "Spending our data\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available leaves us with no other data to assess our choices\nWe can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we‚Äôve done so far)"
  },
  {
    "objectID": "slides/lec-4.html#simulation-data-splitting",
    "href": "slides/lec-4.html#simulation-data-splitting",
    "title": "SLR: Prediction + model evaluation",
    "section": "Simulation: data splitting",
    "text": "Simulation: data splitting\n\n\n\n\nTake a random sample of 10% of the data and set aside (testing data)\nFit a model on the remaining 90% of the data (training data)\nUse the coefficients from this model to make predictions for the testing data\nRepeat 10 times"
  },
  {
    "objectID": "slides/lec-4.html#predictive-performance",
    "href": "slides/lec-4.html#predictive-performance",
    "title": "SLR: Prediction + model evaluation",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\n\nHow consistent are the predictions for different testing datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs.¬†in the edges?"
  },
  {
    "objectID": "slides/lec-4.html#bootstrapping-our-data",
    "href": "slides/lec-4.html#bootstrapping-our-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Bootstrapping our data",
    "text": "Bootstrapping our data\n\nThe idea behind bootstrapping is that if a given observation exists in a sample, there may be more like it in the population\nWith bootstrapping, we simulate resampling from the population by resampling from the sample we observed\nBootstrap samples are the sampled with replacement from the original sample and same size as the original sample\n\nFor example, if our sample consists of the observations {A, B, C}, bootstrap samples could be {A, A, B}, {A, C, A}, {B, C, C}, {A, B, C}, etc."
  },
  {
    "objectID": "slides/lec-4.html#simulation-bootstrapping",
    "href": "slides/lec-4.html#simulation-bootstrapping",
    "title": "SLR: Prediction + model evaluation",
    "section": "Simulation: bootstrapping",
    "text": "Simulation: bootstrapping\n\n\n\n\nTake a bootstrap sample ‚Äì sample with replacement from the original data, same size as the original data\nFit model to the sample and make predictions for that sample\nRepeat many times"
  },
  {
    "objectID": "slides/lec-4.html#predictive-performance-1",
    "href": "slides/lec-4.html#predictive-performance-1",
    "title": "SLR: Prediction + model evaluation",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\n\nHow consistent are the predictions for different bootstrap datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs.¬†in the edges?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-26.html#topics",
    "href": "slides/lec-26.html#topics",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Topics",
    "text": "Topics\n\n\nUnbalanced data\nChoosing the ‚Äúfinal‚Äù model"
  },
  {
    "objectID": "slides/lec-26.html#computational-setup",
    "href": "slides/lec-26.html#computational-setup",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(colorblindr)\nlibrary(themis)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/lec-26.html#volcanoes",
    "href": "slides/lec-26.html#volcanoes",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Volcanoes",
    "text": "Volcanoes\nThe data come from The Smithsonian Institution, via TidyTuesday.\n\nvolcano <- read_csv(here::here(\"slides\", \"data/volcano.csv\"))\nnames(volcano)\n\n [1] \"volcano_number\"           \"volcano_name\"            \n [3] \"primary_volcano_type\"     \"last_eruption_year\"      \n [5] \"country\"                  \"region\"                  \n [7] \"subregion\"                \"latitude\"                \n [9] \"longitude\"                \"elevation\"               \n[11] \"tectonic_settings\"        \"evidence_category\"       \n[13] \"major_rock_1\"             \"major_rock_2\"            \n[15] \"major_rock_3\"             \"major_rock_4\"            \n[17] \"major_rock_5\"             \"minor_rock_1\"            \n[19] \"minor_rock_2\"             \"minor_rock_3\"            \n[21] \"minor_rock_4\"             \"minor_rock_5\"            \n[23] \"population_within_5_km\"   \"population_within_10_km\" \n[25] \"population_within_30_km\"  \"population_within_100_km\""
  },
  {
    "objectID": "slides/lec-26.html#data-prep",
    "href": "slides/lec-26.html#data-prep",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Data prep",
    "text": "Data prep\n\nvolcano <- volcano %>%\n  mutate(\n    volcano_type = case_when(\n      str_detect(primary_volcano_type, \"Stratovolcano\") ~ \"Stratovolcano\",\n      str_detect(primary_volcano_type, \"Shield\") ~ \"Shield\",\n      TRUE ~ \"Other\"\n    ),\n    volcano_type = fct_relevel(volcano_type, \"Stratovolcano\", \"Shield\", \"Other\")\n  ) %>%\n  select(\n    volcano_type, latitude, longitude, \n    elevation, tectonic_settings, major_rock_1\n    ) %>%\n  mutate(across(where(is.character), as_factor))"
  },
  {
    "objectID": "slides/lec-26.html#split-into-testingtraining",
    "href": "slides/lec-26.html#split-into-testingtraining",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Split into testing/training",
    "text": "Split into testing/training\n\nset.seed(1234)\n\nvolcano_split <- initial_split(volcano)\nvolcano_train <- training(volcano_split)\nvolcano_test  <- testing(volcano_split)"
  },
  {
    "objectID": "slides/lec-26.html#specify-a-model",
    "href": "slides/lec-26.html#specify-a-model",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Specify a model",
    "text": "Specify a model\n\nvolcano_spec <- multinom_reg() %>%\n  set_engine(\"nnet\")\n\nvolcano_spec\n\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet"
  },
  {
    "objectID": "slides/lec-26.html#create-cross-validation-folds",
    "href": "slides/lec-26.html#create-cross-validation-folds",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Create cross validation folds",
    "text": "Create cross validation folds\n\nset.seed(9876)\n\nvolcano_folds <- vfold_cv(volcano_train, v = 5)\nvolcano_folds\n\n#  5-fold cross-validation \n# A tibble: 5 √ó 2\n  splits            id   \n  <list>            <chr>\n1 <split [574/144]> Fold1\n2 <split [574/144]> Fold2\n3 <split [574/144]> Fold3\n4 <split [575/143]> Fold4\n5 <split [575/143]> Fold5"
  },
  {
    "objectID": "slides/lec-26.html#unbalanced-data-1",
    "href": "slides/lec-26.html#unbalanced-data-1",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Unbalanced data",
    "text": "Unbalanced data\nRemember that the observed volcano types are unbalanced:\n\nvolcano %>% \n  count(volcano_type)\n\n# A tibble: 3 √ó 2\n  volcano_type      n\n  <fct>         <int>\n1 Stratovolcano   461\n2 Shield          118\n3 Other           379"
  },
  {
    "objectID": "slides/lec-26.html#addressing-unbalance",
    "href": "slides/lec-26.html#addressing-unbalance",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Addressing unbalance",
    "text": "Addressing unbalance\nTo address class unbalance, we generally use\n\noversampling data from levels that are less prevalent in the data\n\ne.g., step_smote(): Uses a technique called ‚ÄúSynthetic Minority Over-sampling Technique‚Äù to generate new examples of the minority class using nearest neighbors of these cases.\n\ndownsampling data from levels that are more prevalent in the data\n\ne.g., step_downsample(): Removes rows of a data set to make the occurrence of levels in a specific factor level equal."
  },
  {
    "objectID": "slides/lec-26.html#new-recipe---oversample",
    "href": "slides/lec-26.html#new-recipe---oversample",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "New recipe - oversample",
    "text": "New recipe - oversample\n\nvolcano_rec3 <- recipe(volcano_type ~ ., data = volcano_train) %>%\n  step_other(tectonic_settings) %>%\n  step_other(major_rock_1) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors()) %>%\n  step_center(all_predictors()) %>%\n  step_smote(volcano_type)"
  },
  {
    "objectID": "slides/lec-26.html#new-recipe---downsample",
    "href": "slides/lec-26.html#new-recipe---downsample",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "New recipe - downsample",
    "text": "New recipe - downsample\n\nvolcano_rec4 <- recipe(volcano_type ~ ., data = volcano_train) %>%\n  step_other(tectonic_settings) %>%\n  step_other(major_rock_1) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors()) %>%\n  step_center(all_predictors()) %>%\n  step_downsample(volcano_type)"
  },
  {
    "objectID": "slides/lec-26.html#new-workflows",
    "href": "slides/lec-26.html#new-workflows",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "New workflows",
    "text": "New workflows\n\nvolcano_wflow3 <- workflow() %>%\n  add_recipe(volcano_rec3) %>%\n  add_model(volcano_spec)\n\nvolcano_wflow4 <- workflow() %>%\n  add_recipe(volcano_rec4) %>%\n  add_model(volcano_spec)"
  },
  {
    "objectID": "slides/lec-26.html#fit-resamples",
    "href": "slides/lec-26.html#fit-resamples",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Fit resamples",
    "text": "Fit resamples\n\nvolcano_fit_rs3 <- volcano_wflow3 %>%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )\n\nvolcano_fit_rs4 <- volcano_wflow4 %>%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )"
  },
  {
    "objectID": "slides/lec-26.html#collect-metrics",
    "href": "slides/lec-26.html#collect-metrics",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Collect metrics",
    "text": "Collect metrics\n\ncollect_metrics(volcano_fit_rs3)\n\n# A tibble: 2 √ó 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy multiclass 0.510     5  0.0173 Preprocessor1_Model1\n2 roc_auc  hand_till  0.694     5  0.0212 Preprocessor1_Model1\n\ncollect_metrics(volcano_fit_rs4)\n\n# A tibble: 2 √ó 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy multiclass 0.504     5  0.0148 Preprocessor1_Model1\n2 roc_auc  hand_till  0.687     5  0.0168 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/lec-26.html#roc-curves---oversampling",
    "href": "slides/lec-26.html#roc-curves---oversampling",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "ROC curves - oversampling",
    "text": "ROC curves - oversampling\n\nvolcano_fit_rs3 %>%\n  collect_predictions() %>%\n  group_by(id) %>%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %>%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-26.html#roc-curves---downsampling",
    "href": "slides/lec-26.html#roc-curves---downsampling",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "ROC curves - downsampling",
    "text": "ROC curves - downsampling\n\nvolcano_fit_rs4 %>%\n  collect_predictions() %>%\n  group_by(id) %>%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %>%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-26.html#addressing-unbalance-1",
    "href": "slides/lec-26.html#addressing-unbalance-1",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Addressing unbalance",
    "text": "Addressing unbalance\n\nCan you think of any issues resulting from over/down sampling?"
  },
  {
    "objectID": "slides/lec-26.html#the-chosen-model",
    "href": "slides/lec-26.html#the-chosen-model",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "The ‚Äúchosen‚Äù model",
    "text": "The ‚Äúchosen‚Äù model\nLet‚Äôs stick to the models without over/down sampling.\nFrom the application exercise:\n\nvolcano_rec2 <- recipe(volcano_type ~ ., data = volcano_train) %>%\n  step_other(tectonic_settings) %>%\n  step_other(major_rock_1) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors()) %>%\n  step_center(all_predictors())\n\nvolcano_wflow2 <- workflow() %>%\n  add_recipe(volcano_rec2) %>%\n  add_model(volcano_spec)"
  },
  {
    "objectID": "slides/lec-26.html#fitting-the-final-model",
    "href": "slides/lec-26.html#fitting-the-final-model",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Fitting the final model",
    "text": "Fitting the final model\n\nfinal_fit <- last_fit(\n  volcano_wflow2, \n  split = volcano_split\n  )\n\ncollect_metrics(final_fit)\n\n# A tibble: 2 √ó 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy multiclass     0.629 Preprocessor1_Model1\n2 roc_auc  hand_till      0.734 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/lec-26.html#confusion-matrix",
    "href": "slides/lec-26.html#confusion-matrix",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\ncollect_predictions(final_fit) %>%\n  conf_mat(volcano_type, .pred_class)\n\n               Truth\nPrediction      Stratovolcano Shield Other\n  Stratovolcano            96     13    38\n  Shield                    1      0     0\n  Other                    21     16    55"
  },
  {
    "objectID": "slides/lec-26.html#confusion-matrix---visualized",
    "href": "slides/lec-26.html#confusion-matrix---visualized",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Confusion matrix - visualized",
    "text": "Confusion matrix - visualized\n\ncollect_predictions(final_fit) %>%\n  conf_mat(volcano_type, .pred_class) %>%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-26.html#roc-curve",
    "href": "slides/lec-26.html#roc-curve",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "ROC curve",
    "text": "ROC curve\n\ncollect_predictions(final_fit) %>%\n  roc_curve(truth = volcano_type, .pred_Stratovolcano:.pred_Other) %>%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-26.html#roc-curve---altogether",
    "href": "slides/lec-26.html#roc-curve---altogether",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "ROC curve - altogether",
    "text": "ROC curve - altogether\n\nüìã github.com/STA210-Summer22/ae-11-volcanoes - Exercise 3"
  },
  {
    "objectID": "slides/lec-26.html#prediction",
    "href": "slides/lec-26.html#prediction",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Prediction",
    "text": "Prediction\n\nfinal_fitted <- extract_workflow(final_fit)\n\nnew_volcano <- tibble(\n  latitude = 35.9940,\n  longitude = -78.8986,\n  elevation = 404,\n  tectonic_settings = \"Subduction zone / Continental crust (>25 km)\",\n  major_rock_1 = \"Andesite / Basaltic Andesite\"\n)\n\npredict(\n  final_fitted, \n  new_volcano, \n  type = \"prob\"\n  )\n\n# A tibble: 1 √ó 3\n  .pred_Stratovolcano .pred_Shield .pred_Other\n                <dbl>        <dbl>       <dbl>\n1               0.381       0.0379       0.581"
  },
  {
    "objectID": "slides/lec-26.html#acknowledgements",
    "href": "slides/lec-26.html#acknowledgements",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nInspired by\n\n\nhttps://juliasilge.com/blog/multinomial-volcano-eruptions/\nhttps://juliasilge.com/blog/nber-papers/\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-27.html#exam-instructions",
    "href": "slides/lec-27.html#exam-instructions",
    "title": "Exam 3 review",
    "section": "Exam instructions",
    "text": "Exam instructions\n\nThe exam is an individual assignment. Everything in your repository is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor. For example, you may not communicate with other students, the TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nNo TA office hours will be held during the exam. You may not email the TAs questions about the exam.\nIf you have questions, email me."
  },
  {
    "objectID": "slides/lec-27.html#exam-coverage-and-format",
    "href": "slides/lec-27.html#exam-coverage-and-format",
    "title": "Exam 3 review",
    "section": "Exam coverage and format",
    "text": "Exam coverage and format\n\nFocuses on content after exam 2, but can include material from previous weeks\nSimilar format as previous exams\n\nPart 1: Multiple choice/fill-in-the-blank questions on Sakai\nPart 2: Open-ended data analysis in GitHub and submitted on Gradescope"
  },
  {
    "objectID": "slides/lec-27.html#part-2-of-the-exam",
    "href": "slides/lec-27.html#part-2-of-the-exam",
    "title": "Exam 3 review",
    "section": "Part 2 of the exam",
    "text": "Part 2 of the exam\n\nGoal: Assess your understanding of the course material and how the methods you learned are applied to the analysis of real-world data.\nInclude all of your analysis steps in your exam write up, unless stated otherwise.\n\nFor example, if the exam says ‚Äúassume conditions are met,‚Äù You can reference that information in your write up but don‚Äôt have to recheck the conditions."
  },
  {
    "objectID": "slides/lec-27.html#assessment-criteria",
    "href": "slides/lec-27.html#assessment-criteria",
    "title": "Exam 3 review",
    "section": "Assessment criteria",
    "text": "Assessment criteria\n\nYou can identify the correct approach, analysis method, and/or inferential results required to answer the question.\nYou understand the correct conditions and diagnostics needed to determine whether the conclusions drawn from the model will be reliable\nYou can write results and conclusions in a meaningful way that can be understood by a general audience (think a business or research partner)\nYou can produce a report that is suitable for a professional audience (e.g., narrative is written in complete sentences, all graphs have proper titles and axis labels, there is not extraneous output, all Latex is rendered)\nYou can conduct the analysis using a reproducible data analysis workflow that incorporates version control"
  },
  {
    "objectID": "slides/lec-27.html#application-exercise",
    "href": "slides/lec-27.html#application-exercise",
    "title": "Exam 3 review",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã github.com/STA210-Summer22/ae-12-exam-3-review\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-1.html#meet-the-instructor",
    "href": "slides/lec-1.html#meet-the-instructor",
    "title": "Welcome to STA 210!",
    "section": "Meet the instructor",
    "text": "Meet the instructor\n\n\n\n\n\nYunran Chen (she/her)\n\n\n\n\nThird-year Ph.D.¬†student, Department of Statistical Science\nFind out more at my personal website\nPecan, One-year-old male Bernese Mountain Dog"
  },
  {
    "objectID": "slides/lec-1.html#meet-the-ta",
    "href": "slides/lec-1.html#meet-the-ta",
    "title": "Welcome to STA 210!",
    "section": "Meet the TA",
    "text": "Meet the TA\n\nJoseph Ekpenyong\nHold office hour and grade labs + HWs"
  },
  {
    "objectID": "slides/lec-1.html#meet-each-other",
    "href": "slides/lec-1.html#meet-each-other",
    "title": "Welcome to STA 210!",
    "section": "Meet each other!",
    "text": "Meet each other!\n\nName, year, major, hometown\nAny pets or favorite movie star?\nWhat do you hope to get out of this course?\nAnything else you want to share/ask?\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-1.html#check-out-conversations",
    "href": "slides/lec-1.html#check-out-conversations",
    "title": "Welcome to STA 210!",
    "section": "Check out Conversations",
    "text": "Check out Conversations\n\nGo to Conversationsüí¨\nAnswer the discussion question: What do you love most for summer (in one word) ?"
  },
  {
    "objectID": "slides/lec-1.html#what-is-regression-analysis",
    "href": "slides/lec-1.html#what-is-regression-analysis",
    "title": "Welcome to STA 210!",
    "section": "What is regression analysis",
    "text": "What is regression analysis\n\n\n‚ÄúIn statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or predictors). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or ‚Äòcriterion variable‚Äô) changes when any one of the independent variables is varied, while the other independent variables are held fixed.‚Äù\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lec-1.html#course-faq",
    "href": "slides/lec-1.html#course-faq",
    "title": "Welcome to STA 210!",
    "section": "Course FAQ",
    "text": "Course FAQ\n\n\nWhat background is assumed for the course? Introductory statistics or probability course.\nWill we be doing computing? Yes. We will use R.\nWill we learn the mathematical theory of regression? Yes and No.¬†The course is primarily focused on application; however, we will discuss some of the mathematics of simple linear regression. If you want to dive into more of the mathematics, I can introduce some mathematics during labs.\nWhat is expected course load? Super intense! 25 hr per week. Deadlines on Mon/Wed/Fri/Sun."
  },
  {
    "objectID": "slides/lec-1.html#course-learning-objectives",
    "href": "slides/lec-1.html#course-learning-objectives",
    "title": "Welcome to STA 210!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\n\nAnalyze real-world data to answer questions about multivariable relationships.\nFit and evaluate linear and logistic regression models.\nAssess whether a proposed model is appropriate and describe its limitations.\nUse Quarto to write reproducible reports and GitHub for version control and collaboration.\nCommunicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "slides/lec-1.html#examples-of-regression-in-practice",
    "href": "slides/lec-1.html#examples-of-regression-in-practice",
    "title": "Welcome to STA 210!",
    "section": "Examples of regression in practice",
    "text": "Examples of regression in practice\n\n\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\nHow FiveThirtyEight‚Äôs 2020 Presidential Forecast Works ‚Äî And What‚Äôs Different Because Of COVID-19\nEffect of Forensic Evidence on Criminal Justice Case Processing\nWhy it‚Äôs so freaking hard to make a good COVID-19 model"
  },
  {
    "objectID": "slides/lec-1.html#homepage",
    "href": "slides/lec-1.html#homepage",
    "title": "Welcome to STA 210!",
    "section": "Homepage",
    "text": "Homepage\nyunranchen.github.io/STA210Summer/\n\nAll course materials\nLinks to Sakai, GitHub, RStudio containers, etc.\nLet‚Äôs take a tour!"
  },
  {
    "objectID": "slides/lec-1.html#course-toolkit",
    "href": "slides/lec-1.html#course-toolkit",
    "title": "Welcome to STA 210!",
    "section": "Course toolkit",
    "text": "Course toolkit\nAll linked from the course website:\n\nGitHub organization: github.com/STA210-Summer22\nRStudio containers: cmgr.oit.duke.edu/containers\nDiscussion forum: Conversations\nAssignment submission and feedback: Gradescope\n\n\n\n\n\n\n\nImportant\n\n\nReserve an RStudio Container (titled STA 210) before lab !"
  },
  {
    "objectID": "slides/lec-1.html#activities-prepare-participate-practice-perform",
    "href": "slides/lec-1.html#activities-prepare-participate-practice-perform",
    "title": "Welcome to STA 210!",
    "section": "Activities: Prepare, Participate, Practice, Perform",
    "text": "Activities: Prepare, Participate, Practice, Perform\n\nPrepare: Introduce new content and prepare for lectures by completing the readings (and sometimes watching the videos)\nParticipate: Attend and actively participate in lectures and labs, office hours, team meetings\nPractice: Practice applying statistical concepts and computing with application exercises during lecture, graded for completion\nPerform: Put together what you‚Äôve learned to analyze real-world data\n\nLab assignments x 7 (team-based)\nHomework assignments x 5 (individual)\nThree take-home exams (individual)\nTerm project presented during the final exam period (team-based)"
  },
  {
    "objectID": "slides/lec-1.html#cadence",
    "href": "slides/lec-1.html#cadence",
    "title": "Welcome to STA 210!",
    "section": "Cadence",
    "text": "Cadence\n\n\nLabs, HWs, and AEs: Due on Mon/Wed/Fri/Sun 11:59pm.\nExams: Exam review Friday in class, exam posted Friday morning 9:00 am, due Monday 11:59pm.\nProject: Deadlines throughout the semester, with some lab and lecture time dedicated to working on them, and most work done in teams outside of class"
  },
  {
    "objectID": "slides/lec-1.html#teams",
    "href": "slides/lec-1.html#teams",
    "title": "Welcome to STA 210!",
    "section": "Teams",
    "text": "Teams\n\nTeam assignments\n\nAssigned by me (Weekly vs Whole semester ? )\nApplication exercises, labs, and project\nPeer evaluation during teamwork and after completion\n\nExpectations and roles\n\nEveryone is expected to contribute equal effort\nEveryone is expected to understand all code turned in\nIndividual contribution evaluated by peer evaluation, commits, etc."
  },
  {
    "objectID": "slides/lec-1.html#grading",
    "href": "slides/lec-1.html#grading",
    "title": "Welcome to STA 210!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nApplication exercises\n3%\n\n\nHomework\n35% (7% x 5)\n\n\nProject\n15%\n\n\nLab\n14% (2.5% x 6)\n\n\nExam 01\n10%\n\n\nExam 02\n10%\n\n\nExam 03\n10%\n\n\nTeamwork\n2%\n\n\n\nSee course syllabus for how the final letter grade will be determined."
  },
  {
    "objectID": "slides/lec-1.html#support",
    "href": "slides/lec-1.html#support",
    "title": "Welcome to STA 210!",
    "section": "Support",
    "text": "Support\n\nAttend office hours\nAsk and answer questions on the discussion forum\nReserve email for questions on personal matters and/or grades\nRead the course support page"
  },
  {
    "objectID": "slides/lec-1.html#announcements",
    "href": "slides/lec-1.html#announcements",
    "title": "Welcome to STA 210!",
    "section": "Announcements",
    "text": "Announcements\n\nPosted on Sakai (Announcements tool) and sent via email, be sure to check both regularly\nI‚Äôll assume that you‚Äôve read an announcement by the next ‚Äúbusiness‚Äù day\nGo to website to check what you need to do to prepare, practice, and perform"
  },
  {
    "objectID": "slides/lec-1.html#diversity-inclusion",
    "href": "slides/lec-1.html#diversity-inclusion",
    "title": "Welcome to STA 210!",
    "section": "Diversity + inclusion",
    "text": "Diversity + inclusion\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students‚Äô learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\n\nPlease let me know your preferred pronunciation of your name.\nPlease let me know your preferred pronouns.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don‚Äôt hesitate to come and talk with me.\nI come from a different cultural background, and am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "slides/lec-1.html#accessibility",
    "href": "slides/lec-1.html#accessibility",
    "title": "Welcome to STA 210!",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments.\nI am committed to making all course materials accessible and I‚Äôm always learning how to do this better. If any course component is not accessible to you in any way, please don‚Äôt hesitate to let me know."
  },
  {
    "objectID": "slides/lec-1.html#covid-policies",
    "href": "slides/lec-1.html#covid-policies",
    "title": "Welcome to STA 210!",
    "section": "COVID policies",
    "text": "COVID policies\n\nWear a mask at all times!\nRead and follow university guidance"
  },
  {
    "objectID": "slides/lec-1.html#late-work-waivers-regrades-policy",
    "href": "slides/lec-1.html#late-work-waivers-regrades-policy",
    "title": "Welcome to STA 210!",
    "section": "Late work, waivers, regrades policy",
    "text": "Late work, waivers, regrades policy\n\nWe have policies!\nRead about them on the course syllabus and refer back to them when you need it"
  },
  {
    "objectID": "slides/lec-1.html#collaboration-policy",
    "href": "slides/lec-1.html#collaboration-policy",
    "title": "Welcome to STA 210!",
    "section": "Collaboration policy",
    "text": "Collaboration policy\n\nOnly work that is clearly assigned as team work should be completed collaboratively.\nHomeworks must be completed individually. You may not directly share answers / code with others, however you are welcome to discuss the problems in general and ask for advice.\nExams must be completed individually. You may not discuss any aspect of the exam with peers. If you have questions, post as private questions on the course forum, only the teaching team will see and answer."
  },
  {
    "objectID": "slides/lec-1.html#sharing-reusing-code-policy",
    "href": "slides/lec-1.html#sharing-reusing-code-policy",
    "title": "Welcome to STA 210!",
    "section": "Sharing / reusing code policy",
    "text": "Sharing / reusing code policy\n\nWe are aware that a huge volume of code is available on the web, and many tasks may have solutions posted\nUnless explicitly stated otherwise, this course‚Äôs policy is that you may make use of any online resources (e.g.¬†RStudio Community, StackOverflow, etc.) but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solution(s).\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source"
  },
  {
    "objectID": "slides/lec-1.html#academic-integrity",
    "href": "slides/lec-1.html#academic-integrity",
    "title": "Welcome to STA 210!",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nTo uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "slides/lec-1.html#most-importantly",
    "href": "slides/lec-1.html#most-importantly",
    "title": "Welcome to STA 210!",
    "section": "Most importantly!",
    "text": "Most importantly!\nAsk if you‚Äôre not sure if something violates a policy!"
  },
  {
    "objectID": "slides/lec-1.html#five-tips-for-success",
    "href": "slides/lec-1.html#five-tips-for-success",
    "title": "Welcome to STA 210!",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work before class.\nAsk questions.\nDo the readings.\nDo the homework and lab.\nDon‚Äôt procrastinate and don‚Äôt let a week pass by with lingering questions."
  },
  {
    "objectID": "slides/lec-1.html#learning-during-a-pandemic",
    "href": "slides/lec-1.html#learning-during-a-pandemic",
    "title": "Welcome to STA 210!",
    "section": "Learning during a pandemic",
    "text": "Learning during a pandemic\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don‚Äôt hesitate to ask.\n\n\nYou never owe me personal information about your health (mental or physical) but you‚Äôre always welcome to talk to me. If I can‚Äôt help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this crisis."
  },
  {
    "objectID": "slides/lec-1.html#this-weeks-tasks",
    "href": "slides/lec-1.html#this-weeks-tasks",
    "title": "Welcome to STA 210!",
    "section": "This week‚Äôs tasks",
    "text": "This week‚Äôs tasks\n\nGet a GitHub account if you don‚Äôt have one (some advice for choosing a username here)\nComplete the Getting to know you survey if you haven‚Äôt yet done so!\nRead the syllabus\nWatch out for announcement email"
  },
  {
    "objectID": "slides/lec-3.html#announcements",
    "href": "slides/lec-3.html#announcements",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Announcements",
    "text": "Announcements\n\nIf you‚Äôre just joining the class, welcome! Go to the course website and review content you‚Äôve missed, read the syllabus, and complete the Getting to know you survey.\nLab 1 is due Friday, at 11:59pm, on Gradescope."
  },
  {
    "objectID": "slides/lec-3.html#recap-of-last-lecture",
    "href": "slides/lec-3.html#recap-of-last-lecture",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Recap of last lecture",
    "text": "Recap of last lecture\n\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative outcome variable.\nUsed the least squares method to estimate the slope and intercept.\nWe interpreted the slope and intercept.\n\n\nSlope: For every one unit increase in \\(x\\), we expect y to be higher/lower by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units.\n\n\nPredicted the response given a value of the predictor variable.\nDefined extrapolation and why we should avoid it."
  },
  {
    "objectID": "slides/lec-3.html#interested-in-the-math-behind-it-all",
    "href": "slides/lec-3.html#interested-in-the-math-behind-it-all",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Interested in the math behind it all?",
    "text": "Interested in the math behind it all?\nSee the supplemental notes on Deriving the Least-Squares Estimates for Simple Linear Regression for more mathematical details on the derivations of the estimates of \\(\\beta_0\\) and \\(\\beta_1\\)."
  },
  {
    "objectID": "slides/lec-3.html#outline",
    "href": "slides/lec-3.html#outline",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Outline",
    "text": "Outline\n\nUse tidymodels to fit and summarize regression models in R\nComplete an application exercise on exploratory data analysis and modeling"
  },
  {
    "objectID": "slides/lec-3.html#computational-setup",
    "href": "slides/lec-3.html#computational-setup",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(tidymodels)      # for modeling\nlibrary(fivethirtyeight) # for the fandango dataset\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/lec-3.html#movie-ratings",
    "href": "slides/lec-3.html#movie-ratings",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Movie ratings",
    "text": "Movie ratings\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango‚Äôs\nIn the fivethirtyeight package: fandango\nContains every film that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/lec-3.html#data-prep",
    "href": "slides/lec-3.html#data-prep",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores <- fandango %>%\n  rename(\n    critics = rottentomatoes, \n    audience = rottentomatoes_user\n  )"
  },
  {
    "objectID": "slides/lec-3.html#data-visualization",
    "href": "slides/lec-3.html#data-visualization",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Data visualization",
    "text": "Data visualization"
  },
  {
    "objectID": "slides/lec-3.html#step-1-specify-model",
    "href": "slides/lec-3.html#step-1-specify-model",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-3.html#step-2-set-model-fitting-engine",
    "href": "slides/lec-3.html#step-2-set-model-fitting-engine",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\n# #| code-line-numbers: \"|2\"\n\nlinear_reg() %>%\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-3.html#step-3-fit-model-estimate-parameters",
    "href": "slides/lec-3.html#step-3-fit-model-estimate-parameters",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Step 3: Fit model & estimate parameters",
    "text": "Step 3: Fit model & estimate parameters\nusing formula syntax\n\n# #| code-line-numbers: \"|3\"\n\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(audience ~ critics, data = movie_scores)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = audience ~ critics, data = data)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187"
  },
  {
    "objectID": "slides/lec-3.html#a-closer-look-at-model-output",
    "href": "slides/lec-3.html#a-closer-look-at-model-output",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "A closer look at model output",
    "text": "A closer look at model output\n\nmovie_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(audience ~ critics, data = movie_scores)\n\nmovie_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = audience ~ critics, data = data)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187  \n\n\n\\[\\widehat{\\text{audience}} = 32.3155 + 0.5187 \\times \\text{critics}\\]\n\nNote: The intercept is off by a tiny bit from the hand-calculated intercept, this is likely just rounding error in the hand calculation."
  },
  {
    "objectID": "slides/lec-3.html#the-regression-output",
    "href": "slides/lec-3.html#the-regression-output",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "The regression output",
    "text": "The regression output\nWe‚Äôll focus on the first column for now‚Ä¶\n\n# #| code-line-numbers: \"|4\"\n\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(audience ~ critics, data = movie_scores) %>%\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/lec-3.html#prediction",
    "href": "slides/lec-3.html#prediction",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Prediction",
    "text": "Prediction\n\n# #| code-line-numbers: \"|2|5\"\n\n# create a data frame for a new movie\nnew_movie <- tibble(critics = 50)\n\n# predict the outcome for a new movie\npredict(movie_fit, new_movie)\n\n# A tibble: 1 √ó 1\n  .pred\n  <dbl>\n1  58.2"
  },
  {
    "objectID": "slides/lec-25.html#topics",
    "href": "slides/lec-25.html#topics",
    "title": "MultiLR: Predictive models",
    "section": "Topics",
    "text": "Topics\n\n\nBuilding predictive multinomial logistic regression models\nComparing models"
  },
  {
    "objectID": "slides/lec-25.html#computational-setup",
    "href": "slides/lec-25.html#computational-setup",
    "title": "MultiLR: Predictive models",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(colorblindr)\nlibrary(themis)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/lec-25.html#terminology",
    "href": "slides/lec-25.html#terminology",
    "title": "MultiLR: Predictive models",
    "section": "Terminology",
    "text": "Terminology\n\nWhat‚Äôs the difference between regression and classification?\n\n\nLogistic regression / binary classification\nMultinomial logistic regression / multinomial classification"
  },
  {
    "objectID": "slides/lec-25.html#volcanoes",
    "href": "slides/lec-25.html#volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Volcanoes",
    "text": "Volcanoes\nThe data come from The Smithsonian Institution, via TidyTuesday.\n\nvolcano <- read_csv(here::here(\"slides\", \"data/volcano.csv\"))\nnames(volcano)\n\n [1] \"volcano_number\"           \"volcano_name\"            \n [3] \"primary_volcano_type\"     \"last_eruption_year\"      \n [5] \"country\"                  \"region\"                  \n [7] \"subregion\"                \"latitude\"                \n [9] \"longitude\"                \"elevation\"               \n[11] \"tectonic_settings\"        \"evidence_category\"       \n[13] \"major_rock_1\"             \"major_rock_2\"            \n[15] \"major_rock_3\"             \"major_rock_4\"            \n[17] \"major_rock_5\"             \"minor_rock_1\"            \n[19] \"minor_rock_2\"             \"minor_rock_3\"            \n[21] \"minor_rock_4\"             \"minor_rock_5\"            \n[23] \"population_within_5_km\"   \"population_within_10_km\" \n[25] \"population_within_30_km\"  \"population_within_100_km\""
  },
  {
    "objectID": "slides/lec-25.html#volcanoes-1",
    "href": "slides/lec-25.html#volcanoes-1",
    "title": "MultiLR: Predictive models",
    "section": "Volcanoes",
    "text": "Volcanoes\n\nglimpse(volcano)\n\nRows: 958\nColumns: 26\n$ volcano_number           <dbl> 283001, 355096, 342080, 213004, 321040, 28317‚Ä¶\n$ volcano_name             <chr> \"Abu\", \"Acamarachi\", \"Acatenango\", \"Acigol-Ne‚Ä¶\n$ primary_volcano_type     <chr> \"Shield(s)\", \"Stratovolcano\", \"Stratovolcano(‚Ä¶\n$ last_eruption_year       <chr> \"-6850\", \"Unknown\", \"1972\", \"-2080\", \"950\", \"‚Ä¶\n$ country                  <chr> \"Japan\", \"Chile\", \"Guatemala\", \"Turkey\", \"Uni‚Ä¶\n$ region                   <chr> \"Japan, Taiwan, Marianas\", \"South America\", \"‚Ä¶\n$ subregion                <chr> \"Honshu\", \"Northern Chile, Bolivia and Argent‚Ä¶\n$ latitude                 <dbl> 34.500, -23.292, 14.501, 38.537, 46.206, 37.6‚Ä¶\n$ longitude                <dbl> 131.600, -67.618, -90.876, 34.621, -121.490, ‚Ä¶\n$ elevation                <dbl> 641, 6023, 3976, 1683, 3742, 1728, 1733, 1250‚Ä¶\n$ tectonic_settings        <chr> \"Subduction zone / Continental crust (>25 km)‚Ä¶\n$ evidence_category        <chr> \"Eruption Dated\", \"Evidence Credible\", \"Erupt‚Ä¶\n$ major_rock_1             <chr> \"Andesite / Basaltic Andesite\", \"Dacite\", \"An‚Ä¶\n$ major_rock_2             <chr> \"Basalt / Picro-Basalt\", \"Andesite / Basaltic‚Ä¶\n$ major_rock_3             <chr> \"Dacite\", \"¬†\", \"¬†\", \"Basalt / Picro-Basalt\", ‚Ä¶\n$ major_rock_4             <chr> \"¬†\", \"¬†\", \"¬†\", \"Andesite / Basaltic Andesite\"‚Ä¶\n$ major_rock_5             <chr> \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", ‚Ä¶\n$ minor_rock_1             <chr> \"¬†\", \"¬†\", \"Basalt / Picro-Basalt\", \"¬†\", \"Daci‚Ä¶\n$ minor_rock_2             <chr> \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"Basalt / Picro-Basa‚Ä¶\n$ minor_rock_3             <chr> \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"Andesite ‚Ä¶\n$ minor_rock_4             <chr> \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", ‚Ä¶\n$ minor_rock_5             <chr> \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", \"¬†\", ‚Ä¶\n$ population_within_5_km   <dbl> 3597, 0, 4329, 127863, 0, 428, 101, 51, 0, 98‚Ä¶\n$ population_within_10_km  <dbl> 9594, 7, 60730, 127863, 70, 3936, 485, 6042, ‚Ä¶\n$ population_within_30_km  <dbl> 117805, 294, 1042836, 218469, 4019, 717078, 1‚Ä¶\n$ population_within_100_km <dbl> 4071152, 9092, 7634778, 2253483, 393303, 5024‚Ä¶"
  },
  {
    "objectID": "slides/lec-25.html#types-of-volcanoes",
    "href": "slides/lec-25.html#types-of-volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Types of volcanoes",
    "text": "Types of volcanoes\nProbably too many types!\n\nvolcano %>%\n  count(primary_volcano_type, sort = TRUE) %>%\n  print(n = 26)\n\n# A tibble: 26 √ó 2\n   primary_volcano_type     n\n   <chr>                <int>\n 1 Stratovolcano          353\n 2 Stratovolcano(es)      107\n 3 Shield                  85\n 4 Volcanic field          71\n 5 Pyroclastic cone(s)     70\n 6 Caldera                 65\n 7 Complex                 46\n 8 Shield(s)               33\n 9 Submarine               27\n10 Lava dome(s)            26\n11 Fissure vent(s)         12\n12 Caldera(s)               9\n13 Compound                 9\n14 Maar(s)                  8\n15 Pyroclastic shield       7\n16 Tuff cone(s)             7\n17 Crater rows              5\n18 Subglacial               5\n19 Pyroclastic cone         4\n20 Lava dome                3\n21 Complex(es)              1\n22 Lava cone                1\n23 Lava cone(es)            1\n24 Lava cone(s)             1\n25 Stratovolcano?           1\n26 Tuff cone                1"
  },
  {
    "objectID": "slides/lec-25.html#relevel-volcanoes",
    "href": "slides/lec-25.html#relevel-volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Relevel volcanoes",
    "text": "Relevel volcanoes\n\nvolcano <- volcano %>%\n  mutate(\n    volcano_type = case_when(\n      str_detect(primary_volcano_type, \"Stratovolcano\") ~ \"Stratovolcano\",\n      str_detect(primary_volcano_type, \"Shield\") ~ \"Shield\",\n      TRUE ~ \"Other\"\n    ),\n    volcano_type = fct_relevel(volcano_type, \"Stratovolcano\", \"Shield\", \"Other\")\n  )\n\nvolcano %>%\n  count(volcano_type)\n\n# A tibble: 3 √ó 2\n  volcano_type      n\n  <fct>         <int>\n1 Stratovolcano   461\n2 Shield          118\n3 Other           379"
  },
  {
    "objectID": "slides/lec-25.html#data-prep",
    "href": "slides/lec-25.html#data-prep",
    "title": "MultiLR: Predictive models",
    "section": "Data prep",
    "text": "Data prep\n\nSelect a few variables as predictors for the model with\nConvert all character variables to factors\n\n\n\nvolcano <- volcano %>%\n  select(\n    volcano_type, latitude, longitude, \n    elevation, tectonic_settings, major_rock_1\n    ) %>%\n  mutate(across(where(is.character), as_factor))"
  },
  {
    "objectID": "slides/lec-25.html#mapping-the-volcanoes",
    "href": "slides/lec-25.html#mapping-the-volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Mapping the volcanoes",
    "text": "Mapping the volcanoes"
  },
  {
    "objectID": "slides/lec-25.html#world-map-data",
    "href": "slides/lec-25.html#world-map-data",
    "title": "MultiLR: Predictive models",
    "section": "World map data",
    "text": "World map data\n\nworld <- map_data(\"world\")\n\nworld %>% as_tibble()\n\n# A tibble: 99,338 √ó 6\n    long   lat group order region subregion\n   <dbl> <dbl> <dbl> <int> <chr>  <chr>    \n 1 -69.9  12.5     1     1 Aruba  <NA>     \n 2 -69.9  12.4     1     2 Aruba  <NA>     \n 3 -69.9  12.4     1     3 Aruba  <NA>     \n 4 -70.0  12.5     1     4 Aruba  <NA>     \n 5 -70.1  12.5     1     5 Aruba  <NA>     \n 6 -70.1  12.6     1     6 Aruba  <NA>     \n 7 -70.0  12.6     1     7 Aruba  <NA>     \n 8 -70.0  12.6     1     8 Aruba  <NA>     \n 9 -69.9  12.5     1     9 Aruba  <NA>     \n10 -69.9  12.5     1    10 Aruba  <NA>     \n# ‚Ä¶ with 99,328 more rows"
  },
  {
    "objectID": "slides/lec-25.html#draw-world-map",
    "href": "slides/lec-25.html#draw-world-map",
    "title": "MultiLR: Predictive models",
    "section": "Draw world map",
    "text": "Draw world map\n\n\nworld_map <- ggplot() +\n  geom_polygon(\n    data = world, \n    aes(\n      x = long, y = lat, group = group),\n      color = \"white\", fill = \"gray50\", \n      size = 0.05, alpha = 0.2\n    ) +\n  theme_minimal() +\n  coord_quickmap() +\n  labs(x = NULL, y = NULL)\n\nworld_map"
  },
  {
    "objectID": "slides/lec-25.html#add-volcanoes",
    "href": "slides/lec-25.html#add-volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Add volcanoes",
    "text": "Add volcanoes\n\n\nworld_map +\n  geom_point(\n    data = volcano,\n    aes(\n      x = longitude, y = latitude, \n      color = volcano_type, \n      shape = volcano_type),\n    alpha = 0.5\n  ) +\n  scale_color_OkabeIto() +\n  labs(color = NULL, shape = NULL)"
  },
  {
    "objectID": "slides/lec-25.html#your-turn",
    "href": "slides/lec-25.html#your-turn",
    "title": "MultiLR: Predictive models",
    "section": "Your turn",
    "text": "Your turn\n\nüìã github.com/STA210-Summer22/ae-11-volcanoes - Exercise 1"
  },
  {
    "objectID": "slides/lec-25.html#split-into-testingtraining",
    "href": "slides/lec-25.html#split-into-testingtraining",
    "title": "MultiLR: Predictive models",
    "section": "Split into testing/training",
    "text": "Split into testing/training\n\nset.seed(1234)\n\nvolcano_split <- initial_split(volcano)\nvolcano_train <- training(volcano_split)\nvolcano_test  <- testing(volcano_split)"
  },
  {
    "objectID": "slides/lec-25.html#create-a-recipe",
    "href": "slides/lec-25.html#create-a-recipe",
    "title": "MultiLR: Predictive models",
    "section": "Create a recipe",
    "text": "Create a recipe\nStart with a model that doesn‚Äôt use geographic information:\n\nvolcano_rec1 <- recipe(volcano_type ~ ., data = volcano_train) %>%\n  step_rm(latitude, longitude) %>%\n  step_other(tectonic_settings) %>%\n  step_other(major_rock_1) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors()) %>%\n  step_center(all_predictors())"
  },
  {
    "objectID": "slides/lec-25.html#specify-a-model",
    "href": "slides/lec-25.html#specify-a-model",
    "title": "MultiLR: Predictive models",
    "section": "Specify a model",
    "text": "Specify a model\n\nvolcano_spec <- multinom_reg() %>%\n  set_engine(\"nnet\")\n\nvolcano_spec\n\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet"
  },
  {
    "objectID": "slides/lec-25.html#create-a-workflow",
    "href": "slides/lec-25.html#create-a-workflow",
    "title": "MultiLR: Predictive models",
    "section": "Create a workflow",
    "text": "Create a workflow\n\nvolcano_wflow1 <- workflow() %>%\n  add_recipe(volcano_rec1) %>%\n  add_model(volcano_spec)\n\nvolcano_wflow1\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: multinom_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_rm()\n‚Ä¢ step_other()\n‚Ä¢ step_other()\n‚Ä¢ step_dummy()\n‚Ä¢ step_zv()\n‚Ä¢ step_center()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet"
  },
  {
    "objectID": "slides/lec-25.html#create-cross-validation-folds",
    "href": "slides/lec-25.html#create-cross-validation-folds",
    "title": "MultiLR: Predictive models",
    "section": "Create cross validation folds",
    "text": "Create cross validation folds\n\nset.seed(9876)\n\nvolcano_folds <- vfold_cv(volcano_train, v = 5)\nvolcano_folds\n\n#  5-fold cross-validation \n# A tibble: 5 √ó 2\n  splits            id   \n  <list>            <chr>\n1 <split [574/144]> Fold1\n2 <split [574/144]> Fold2\n3 <split [574/144]> Fold3\n4 <split [575/143]> Fold4\n5 <split [575/143]> Fold5"
  },
  {
    "objectID": "slides/lec-25.html#fit-resamples",
    "href": "slides/lec-25.html#fit-resamples",
    "title": "MultiLR: Predictive models",
    "section": "Fit resamples",
    "text": "Fit resamples\n\nvolcano_fit_rs1 <- volcano_wflow1 %>%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )\n\nvolcano_fit_rs1\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 √ó 5\n  splits            id    .metrics         .notes           .predictions      \n  <list>            <chr> <list>           <list>           <list>            \n1 <split [574/144]> Fold1 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [144 √ó 7]>\n2 <split [574/144]> Fold2 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [144 √ó 7]>\n3 <split [574/144]> Fold3 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [144 √ó 7]>\n4 <split [575/143]> Fold4 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [143 √ó 7]>\n5 <split [575/143]> Fold5 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [143 √ó 7]>"
  },
  {
    "objectID": "slides/lec-25.html#collect-metrics",
    "href": "slides/lec-25.html#collect-metrics",
    "title": "MultiLR: Predictive models",
    "section": "Collect metrics",
    "text": "Collect metrics\n\ncollect_metrics(volcano_fit_rs1)\n\n# A tibble: 2 √ó 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy multiclass 0.596     5  0.0146 Preprocessor1_Model1\n2 roc_auc  hand_till  0.703     5  0.0244 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/lec-25.html#roc-curve",
    "href": "slides/lec-25.html#roc-curve",
    "title": "MultiLR: Predictive models",
    "section": "ROC curve",
    "text": "ROC curve\nROC curves for multiclass outcomes use a one-vs-all approach: calculate multiple curves, one per level vs.¬†all other levels.\n\nvolcano_fit_rs1 %>%\n  collect_predictions() %>%\n  group_by(id) %>%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %>%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-25.html#roc-curve---under-the-hood",
    "href": "slides/lec-25.html#roc-curve---under-the-hood",
    "title": "MultiLR: Predictive models",
    "section": "ROC curve - under the hood",
    "text": "ROC curve - under the hood\nAn additional column, .level, identifies the ‚Äúone‚Äù column in the one-vs-all calculation:\n\nvolcano_fit_rs1 %>%\n  collect_predictions() %>%\n  group_by(id) %>%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  )\n\n# A tibble: 2,175 √ó 5\n# Groups:   id [5]\n   id    .level        .threshold specificity sensitivity\n   <chr> <chr>              <dbl>       <dbl>       <dbl>\n 1 Fold1 Stratovolcano  -Inf           0                1\n 2 Fold1 Stratovolcano     0.0621      0                1\n 3 Fold1 Stratovolcano     0.0786      0.0123           1\n 4 Fold1 Stratovolcano     0.0869      0.0247           1\n 5 Fold1 Stratovolcano     0.0957      0.0370           1\n 6 Fold1 Stratovolcano     0.104       0.0494           1\n 7 Fold1 Stratovolcano     0.104       0.0617           1\n 8 Fold1 Stratovolcano     0.105       0.0741           1\n 9 Fold1 Stratovolcano     0.105       0.0864           1\n10 Fold1 Stratovolcano     0.107       0.0988           1\n# ‚Ä¶ with 2,165 more rows"
  },
  {
    "objectID": "slides/lec-25.html#your-turn-1",
    "href": "slides/lec-25.html#your-turn-1",
    "title": "MultiLR: Predictive models",
    "section": "Your turn",
    "text": "Your turn\n\nüìã github.com/STA210-Summer22/ae-11-volcanoes - Exercise 2"
  },
  {
    "objectID": "slides/lec-25.html#acknowledgements",
    "href": "slides/lec-25.html#acknowledgements",
    "title": "MultiLR: Predictive models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nInspired by\n\n\nhttps://juliasilge.com/blog/multinomial-volcano-eruptions/\nhttps://juliasilge.com/blog/nber-papers/\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-19.html#announcements",
    "href": "slides/lec-19.html#announcements",
    "title": "Probabilities, odds, odds ratios",
    "section": "Announcements",
    "text": "Announcements\n\nExam 2 scores are posted, please review the feedback asap\nProject proposals due tomorrow (Friday), at 5pm"
  },
  {
    "objectID": "slides/lec-19.html#topics",
    "href": "slides/lec-19.html#topics",
    "title": "Probabilities, odds, odds ratios",
    "section": "Topics",
    "text": "Topics\n\n\nUse the odds ratio to compare the odds of two groups\nInterpret the coefficients of a logistic regression model with\n\na single categorical predictor\na single quantitative predictor\nmultiple predictors"
  },
  {
    "objectID": "slides/lec-19.html#computational-setup",
    "href": "slides/lec-19.html#computational-setup",
    "title": "Probabilities, odds, odds ratios",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-19.html#risk-of-coronary-heart-disease",
    "href": "slides/lec-19.html#risk-of-coronary-heart-disease",
    "title": "Probabilities, odds, odds ratios",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)\neducation: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College"
  },
  {
    "objectID": "slides/lec-19.html#high-risk-vs.-education",
    "href": "slides/lec-19.html#high-risk-vs.-education",
    "title": "Probabilities, odds, odds ratios",
    "section": "High risk vs.¬†education",
    "text": "High risk vs.¬†education\n\n\n\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/lec-19.html#compare-the-odds-for-two-groups",
    "href": "slides/lec-19.html#compare-the-odds-for-two-groups",
    "title": "Probabilities, odds, odds ratios",
    "section": "Compare the odds for two groups",
    "text": "Compare the odds for two groups\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\nWe want to compare the risk of heart disease for those with a High School diploma/GED and those with a college degree.\nWe‚Äôll use the odds to compare the two groups\n\n\\[\n\\text{odds} = \\frac{P(\\text{success})}{P(\\text{failure})} = \\frac{\\text{# of successes}}{\\text{# of failures}}\n\\]"
  },
  {
    "objectID": "slides/lec-19.html#compare-the-odds-for-two-groups-1",
    "href": "slides/lec-19.html#compare-the-odds-for-two-groups-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Compare the odds for two groups",
    "text": "Compare the odds for two groups\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\nOdds of having high risk for the High school or GED group: \\(\\frac{147}{1106} = 0.133\\)\nOdds of having high risk for the College group: \\(\\frac{70}{403} = 0.174\\)\nBased on this, we see those with a college degree had higher odds of having high risk for heart disease than those with a high school diploma or GED."
  },
  {
    "objectID": "slides/lec-19.html#odds-ratio-or",
    "href": "slides/lec-19.html#odds-ratio-or",
    "title": "Probabilities, odds, odds ratios",
    "section": "Odds ratio (OR)",
    "text": "Odds ratio (OR)\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\nLet‚Äôs summarize the relationship between the two groups. To do so, we‚Äôll use the odds ratio (OR).\n\\[\nOR = \\frac{\\text{odds}_1}{\\text{odds}_2} = \\frac{\\omega_1}{\\omega_2}\n\\]"
  },
  {
    "objectID": "slides/lec-19.html#or-college-vs.-high-school-or-ged",
    "href": "slides/lec-19.html#or-college-vs.-high-school-or-ged",
    "title": "Probabilities, odds, odds ratios",
    "section": "OR: College vs.¬†High school or GED",
    "text": "OR: College vs.¬†High school or GED\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\\[OR = \\frac{\\text{odds}_{College}}{\\text{odds}_{HS}} = \\frac{0.174}{0.133} = \\mathbf{1.308}\\]\n\nThe odds of having high risk for heart disease are 1.30 times higher for those with a college degree than those with a high school diploma or GED."
  },
  {
    "objectID": "slides/lec-19.html#or-college-vs.-some-high-school",
    "href": "slides/lec-19.html#or-college-vs.-some-high-school",
    "title": "Probabilities, odds, odds ratios",
    "section": "OR: College vs.¬†Some high school",
    "text": "OR: College vs.¬†Some high school\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\\[OR = \\frac{\\text{odds}_{College}}{\\text{odds}_{Some HS}} = \\frac{70/403}{323/1397} = 0.751\\]\n\nThe odds of having high risk for having heart disease for those with a college degree are 0.751 times the odds of having high risk for heart disease for those with some high school."
  },
  {
    "objectID": "slides/lec-19.html#more-natural-interpretation",
    "href": "slides/lec-19.html#more-natural-interpretation",
    "title": "Probabilities, odds, odds ratios",
    "section": "More natural interpretation",
    "text": "More natural interpretation\n\nIt‚Äôs more natural to interpret the odds ratio with a statement with the odds ratio greater than 1.\nThe odds of having high risk for heart disease are 1.33 times higher for those with some high school than those with a college degree."
  },
  {
    "objectID": "slides/lec-19.html#making-the-table-1",
    "href": "slides/lec-19.html#making-the-table-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Making the table 1",
    "text": "Making the table 1\nFirst, rename the levels of the categorical variables:\n\nheart_disease <- heart_disease %>%\n  mutate(\n    high_risk_names = if_else(high_risk == \"1\", \"High risk\", \"Not high risk\"),\n    education_names = case_when(\n      education == \"1\" ~ \"Some high school\",\n      education == \"2\" ~ \"High school or GED\",\n      education == \"3\" ~ \"Some college or vocational school\",\n      education == \"4\" ~ \"College\"\n    ),\n    education_names = fct_relevel(education_names, \"Some high school\", \"High school or GED\", \"Some college or vocational school\", \"College\")\n  )"
  },
  {
    "objectID": "slides/lec-19.html#making-the-table-2",
    "href": "slides/lec-19.html#making-the-table-2",
    "title": "Probabilities, odds, odds ratios",
    "section": "Making the table 2",
    "text": "Making the table 2\nThen, make the table:\n\nheart_disease %>%\n  count(education_names, high_risk_names) %>%\n  pivot_wider(names_from = high_risk_names, values_from = n) %>%\n  kable(col.names = c(\"Education\", \"High risk\", \"Not high risk\"))"
  },
  {
    "objectID": "slides/lec-19.html#deeper-look-into-the-code",
    "href": "slides/lec-19.html#deeper-look-into-the-code",
    "title": "Probabilities, odds, odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease %>%\n  count(education_names, high_risk_names)\n\n# A tibble: 8 √ó 3\n  education_names                   high_risk_names     n\n  <fct>                             <chr>           <int>\n1 Some high school                  High risk         323\n2 Some high school                  Not high risk    1397\n3 High school or GED                High risk         147\n4 High school or GED                Not high risk    1106\n5 Some college or vocational school High risk          88\n6 Some college or vocational school Not high risk     601\n7 College                           High risk          70\n8 College                           Not high risk     403"
  },
  {
    "objectID": "slides/lec-19.html#deeper-look-into-the-code-1",
    "href": "slides/lec-19.html#deeper-look-into-the-code-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease %>%\n  count(education_names, high_risk_names) %>%\n  pivot_wider(names_from = high_risk_names, values_from = n)\n\n# A tibble: 4 √ó 3\n  education_names                   `High risk` `Not high risk`\n  <fct>                                   <int>           <int>\n1 Some high school                          323            1397\n2 High school or GED                        147            1106\n3 Some college or vocational school          88             601\n4 College                                    70             403"
  },
  {
    "objectID": "slides/lec-19.html#deeper-look-into-the-code-2",
    "href": "slides/lec-19.html#deeper-look-into-the-code-2",
    "title": "Probabilities, odds, odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease %>%\n  count(education_names, high_risk_names) %>%\n  pivot_wider(names_from = high_risk_names, values_from = n) %>%\n  kable()\n\n\n\n\neducation_names\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/lec-19.html#deeper-look-into-the-code-3",
    "href": "slides/lec-19.html#deeper-look-into-the-code-3",
    "title": "Probabilities, odds, odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease %>%\n  count(education_names, high_risk_names) %>%\n  pivot_wider(names_from = high_risk_names, values_from = n) %>%\n  kable(col.names = c(\"Education\", \"High risk\", \"Not high risk\"))\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/lec-19.html#categorical-predictor",
    "href": "slides/lec-19.html#categorical-predictor",
    "title": "Probabilities, odds, odds ratios",
    "section": "Categorical predictor",
    "text": "Categorical predictor\nRecall: Education - 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\n\nheart_edu_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ education, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_edu_fit) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046"
  },
  {
    "objectID": "slides/lec-19.html#interpreting-education4---log-odds",
    "href": "slides/lec-19.html#interpreting-education4---log-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpreting education4 - log-odds",
    "text": "Interpreting education4 - log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046\n\n\n\n\n\n\nThe log-odds of having high risk for heart disease are expected to be 0.286 less for those with a college degree compared to those with some high school (the baseline group)."
  },
  {
    "objectID": "slides/lec-19.html#interpreting-education4---odds",
    "href": "slides/lec-19.html#interpreting-education4---odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpreting education4 - odds",
    "text": "Interpreting education4 - odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046\n\n\n\n\n\n\nThe odds of having high risk for heart disease for those with a college degree are expected to be 0.751 (exp(-0.286)) times the odds for those with some high school."
  },
  {
    "objectID": "slides/lec-19.html#coefficients-odds-ratios",
    "href": "slides/lec-19.html#coefficients-odds-ratios",
    "title": "Probabilities, odds, odds ratios",
    "section": "Coefficients + odds ratios",
    "text": "Coefficients + odds ratios\nThe model coefficient, -0.286, is the expected change in the log-odds when going from the Some high school group to the College group.\n\nTherefore, \\(e^{-0.286}\\) = 0.751 is the expected change in the odds when going from the Some high school group to the College group.\n\n\n\\[\nOR  = e^{\\hat{\\beta}_j} = \\exp\\{\\hat{\\beta}_j\\}\n\\]"
  },
  {
    "objectID": "slides/lec-19.html#quantitative-predictor",
    "href": "slides/lec-19.html#quantitative-predictor",
    "title": "Probabilities, odds, odds ratios",
    "section": "Quantitative predictor",
    "text": "Quantitative predictor\n\nheart_age_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_age_fit) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0"
  },
  {
    "objectID": "slides/lec-19.html#interpreting-age-log-odds",
    "href": "slides/lec-19.html#interpreting-age-log-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpreting age: log-odds",
    "text": "Interpreting age: log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0\n\n\n\n\n\n\nFor each additional year in age, the log-odds of having high risk for heart disease are expected to increase by 0.076."
  },
  {
    "objectID": "slides/lec-19.html#interpreting-age-odds",
    "href": "slides/lec-19.html#interpreting-age-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpreting age: odds",
    "text": "Interpreting age: odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0\n\n\n\n\n\n\n\nFor each additional year in age, the odds of having high risk for heart disease are expected to multiply by a factor of 1.08 (exp(0.076)).\nAlternate interpretation: For each additional year in age, the odds of having high risk for heart disease are expected to increase by 8%."
  },
  {
    "objectID": "slides/lec-19.html#multiple-predictors",
    "href": "slides/lec-19.html#multiple-predictors",
    "title": "Probabilities, odds, odds ratios",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\nheart_edu_age_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ education + age, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_edu_age_fit) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000"
  },
  {
    "objectID": "slides/lec-19.html#interpretation-in-terms-of-log-odds",
    "href": "slides/lec-19.html#interpretation-in-terms-of-log-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpretation in terms of log-odds",
    "text": "Interpretation in terms of log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\neducation4: The log-odds of having high risk for heart disease are expected to be 0.020 less for those with a college degree compared to those with some high school, holding age constant."
  },
  {
    "objectID": "slides/lec-19.html#interpretation-in-terms-of-log-odds-1",
    "href": "slides/lec-19.html#interpretation-in-terms-of-log-odds-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpretation in terms of log-odds",
    "text": "Interpretation in terms of log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\nage: For each additional year in age, the log-odds of having high risk for heart disease are expected to increase by 0.073, holding education level constant."
  },
  {
    "objectID": "slides/lec-19.html#interpretation-in-terms-of-odds",
    "href": "slides/lec-19.html#interpretation-in-terms-of-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpretation in terms of odds",
    "text": "Interpretation in terms of odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\neducation4: The odds of having high risk for heart disease for those with a college degree are expected to be 0.98 (exp(-0.020)) times the odds for those with some high school, holding age constant."
  },
  {
    "objectID": "slides/lec-19.html#interpretation-in-terms-of-odds-1",
    "href": "slides/lec-19.html#interpretation-in-terms-of-odds-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpretation in terms of odds",
    "text": "Interpretation in terms of odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\nage: For each additional year in age, the odds having high risk for heart disease are expected to multiply by a factor of 1.08 (exp(0.073)), holding education level constant."
  },
  {
    "objectID": "slides/lec-19.html#recap",
    "href": "slides/lec-19.html#recap",
    "title": "Probabilities, odds, odds ratios",
    "section": "Recap",
    "text": "Recap\n\nUse the odds ratio to compare the odds of two groups\nInterpret the coefficients of a logistic regression model with\n\na single categorical predictor\na single quantitative predictor\nmultiple predictors\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-18.html#announcements",
    "href": "slides/lec-18.html#announcements",
    "title": "Logistic regression",
    "section": "Announcements",
    "text": "Announcements\n\nAny questions on project proposals?"
  },
  {
    "objectID": "slides/lec-18.html#topics",
    "href": "slides/lec-18.html#topics",
    "title": "Logistic regression",
    "section": "Topics",
    "text": "Topics\n\nLogistic regression for binary response variable\nRelationship between odds and probabilities\nUse logistic regression model to calculate predicted odds and probabilities"
  },
  {
    "objectID": "slides/lec-18.html#computational-setup",
    "href": "slides/lec-18.html#computational-setup",
    "title": "Logistic regression",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(Stat2Data)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-18.html#types-of-outcome-variables",
    "href": "slides/lec-18.html#types-of-outcome-variables",
    "title": "Logistic regression",
    "section": "Types of outcome variables",
    "text": "Types of outcome variables\nQuantitative outcome variable:\n\nSales price of a house in Levittown, NY\nModel: Expected sales price given the number of bedrooms, lot size, etc.\n\n\nCategorical outcone variable:\n\nHigh risk of coronary heart disease\nModel: Probability an adult is high risk of heart disease given their age, total cholesterol, etc."
  },
  {
    "objectID": "slides/lec-18.html#models-for-categorical-outcomes",
    "href": "slides/lec-18.html#models-for-categorical-outcomes",
    "title": "Logistic regression",
    "section": "Models for categorical outcomes",
    "text": "Models for categorical outcomes\n\n\nLogistic regression\n2 Outcomes\n1: Yes, 0: No\n\nMultinomial logistic regression\n3+ Outcomes\n1: Democrat, 2: Republican, 3: Independent"
  },
  {
    "objectID": "slides/lec-18.html#election-forecasts",
    "href": "slides/lec-18.html#election-forecasts",
    "title": "Logistic regression",
    "section": "2020 election forecasts",
    "text": "2020 election forecasts\n\nSource: FiveThirtyEight Election Forcasts"
  },
  {
    "objectID": "slides/lec-18.html#nba-finals-predictions",
    "href": "slides/lec-18.html#nba-finals-predictions",
    "title": "Logistic regression",
    "section": "NBA finals predictions",
    "text": "NBA finals predictions\n\nSource: FiveThirtyEight 2019-20 NBA Predictions"
  },
  {
    "objectID": "slides/lec-18.html#do-teenagers-get-7-hours-of-sleep",
    "href": "slides/lec-18.html#do-teenagers-get-7-hours-of-sleep",
    "title": "Logistic regression",
    "section": "Do teenagers get 7+ hours of sleep?",
    "text": "Do teenagers get 7+ hours of sleep?\n\n\nStudents in grades 9 - 12 surveyed about health risk behaviors including whether they usually get 7 or more hours of sleep.\nSleep7\n1: yes\n0: no\n\n\ndata(YouthRisk2009)\nsleep <- YouthRisk2009 %>%\n  as_tibble() %>%\n  filter(!is.na(Age), !is.na(Sleep7))\nsleep %>%\n  relocate(Age, Sleep7)\n\n# A tibble: 446 √ó 6\n     Age Sleep7 Sleep           SmokeLife SmokeDaily MarijuaEver\n   <int>  <int> <fct>           <fct>     <fct>            <int>\n 1    16      1 8 hours         Yes       Yes                  1\n 2    17      0 5 hours         Yes       Yes                  1\n 3    18      0 5 hours         Yes       Yes                  1\n 4    17      1 7 hours         Yes       No                   1\n 5    15      0 4 or less hours No        No                   0\n 6    17      0 6 hours         No        No                   0\n 7    17      1 7 hours         No        No                   0\n 8    16      1 8 hours         Yes       No                   0\n 9    16      1 8 hours         No        No                   0\n10    18      0 4 or less hours Yes       Yes                  1\n# ‚Ä¶ with 436 more rows"
  },
  {
    "objectID": "slides/lec-18.html#plot-the-data",
    "href": "slides/lec-18.html#plot-the-data",
    "title": "Logistic regression",
    "section": "Plot the data",
    "text": "Plot the data\n\nggplot(sleep, aes(x = Age, y = Sleep7)) +\n  geom_point() + \n  labs(y = \"Getting 7+ hours of sleep\")"
  },
  {
    "objectID": "slides/lec-18.html#lets-fit-a-linear-regression-model",
    "href": "slides/lec-18.html#lets-fit-a-linear-regression-model",
    "title": "Logistic regression",
    "section": "Let‚Äôs fit a linear regression model",
    "text": "Let‚Äôs fit a linear regression model\nOutcome: \\(Y\\) = 1: yes, 0: no"
  },
  {
    "objectID": "slides/lec-18.html#lets-use-proportions",
    "href": "slides/lec-18.html#lets-use-proportions",
    "title": "Logistic regression",
    "section": "Let‚Äôs use proportions",
    "text": "Let‚Äôs use proportions\nOutcome: Probability of getting 7+ hours of sleep"
  },
  {
    "objectID": "slides/lec-18.html#what-happens-if-we-zoom-out",
    "href": "slides/lec-18.html#what-happens-if-we-zoom-out",
    "title": "Logistic regression",
    "section": "What happens if we zoom out?",
    "text": "What happens if we zoom out?\nOutcome: Probability of getting 7+ hours of sleep\n\nüõë This model produces predictions outside of 0 and 1."
  },
  {
    "objectID": "slides/lec-18.html#lets-try-another-model",
    "href": "slides/lec-18.html#lets-try-another-model",
    "title": "Logistic regression",
    "section": "Let‚Äôs try another model",
    "text": "Let‚Äôs try another model\n\n‚úÖ This model (called a logistic regression model) only produces predictions between 0 and 1."
  },
  {
    "objectID": "slides/lec-18.html#the-code",
    "href": "slides/lec-18.html#the-code",
    "title": "Logistic regression",
    "section": "The code",
    "text": "The code\n\nggplot(sleep_age, aes(x = Age, y = prop)) +\n  geom_point() + \n  geom_hline(yintercept = c(0,1), lty = 2) + \n  stat_smooth(method =\"glm\", method.args = list(family = \"binomial\"), \n              fullrange = TRUE, se = FALSE) +\n  labs(y = \"P(7+ hours of sleep)\") +\n  xlim(1, 40) +\n  ylim(-0.5, 1.5)"
  },
  {
    "objectID": "slides/lec-18.html#different-types-of-models",
    "href": "slides/lec-18.html#different-types-of-models",
    "title": "Logistic regression",
    "section": "Different types of models",
    "text": "Different types of models\n\n\n\n\n\n\n\n\nMethod\nOutcome\nModel\n\n\n\n\nLinear regression\nQuantitative\n\\(Y = \\beta_0 + \\beta_1~ X\\)\n\n\nLinear regression (transform Y)\nQuantitative\n\\(\\log(Y) = \\beta_0 + \\beta_1~ X\\)\n\n\nLogistic regression\nBinary\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1 ~ X\\)"
  },
  {
    "objectID": "slides/lec-18.html#binary-response-variable",
    "href": "slides/lec-18.html#binary-response-variable",
    "title": "Logistic regression",
    "section": "Binary response variable",
    "text": "Binary response variable\n\n\\(Y = 1: \\text{ yes}, 0: \\text{ no}\\)\n\\(\\pi\\): probability that \\(Y=1\\), i.e., \\(P(Y = 1)\\)\n\\(\\frac{\\pi}{1-\\pi}\\): odds that \\(Y = 1\\)\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\): log odds\nGo from \\(\\pi\\) to \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\) using the logit transformation"
  },
  {
    "objectID": "slides/lec-18.html#odds",
    "href": "slides/lec-18.html#odds",
    "title": "Logistic regression",
    "section": "Odds",
    "text": "Odds\nSuppose there is a 70% chance it will rain tomorrow\n\nProbability it will rain is \\(\\mathbf{p = 0.7}\\)\nProbability it won‚Äôt rain is \\(\\mathbf{1 - p = 0.3}\\)\nOdds it will rain are 7 to 3, 7:3, \\(\\mathbf{\\frac{0.7}{0.3} \\approx 2.33}\\)"
  },
  {
    "objectID": "slides/lec-18.html#are-teenagers-getting-enough-sleep",
    "href": "slides/lec-18.html#are-teenagers-getting-enough-sleep",
    "title": "Logistic regression",
    "section": "Are teenagers getting enough sleep?",
    "text": "Are teenagers getting enough sleep?\n\nsleep %>%\n  count(Sleep7) %>%\n  mutate(p = round(n / sum(n), 3))\n\n# A tibble: 2 √ó 3\n  Sleep7     n     p\n   <int> <int> <dbl>\n1      0   150 0.336\n2      1   296 0.664\n\n\n\n\\(P(\\text{7+ hours of sleep}) = P(Y = 1) = p = 0.664\\)\n\n\n\\(P(\\text{< 7 hours of sleep}) = P(Y = 0) = 1 - p = 0.336\\)\n\n\n\\(P(\\text{odds of 7+ hours of sleep}) = \\frac{0.664}{0.336} = 1.976\\)"
  },
  {
    "objectID": "slides/lec-18.html#from-odds-to-probabilities",
    "href": "slides/lec-18.html#from-odds-to-probabilities",
    "title": "Logistic regression",
    "section": "From odds to probabilities",
    "text": "From odds to probabilities\n\n\nodds\n\\[\\omega = \\frac{\\pi}{1-\\pi}\\]\n\nprobability\n\\[\\pi = \\frac{\\omega}{1 + \\omega}\\]"
  },
  {
    "objectID": "slides/lec-18.html#logistic-regression",
    "href": "slides/lec-18.html#logistic-regression",
    "title": "Logistic regression",
    "section": "Logistic regression",
    "text": "Logistic regression"
  },
  {
    "objectID": "slides/lec-18.html#from-odds-to-probabilities-1",
    "href": "slides/lec-18.html#from-odds-to-probabilities-1",
    "title": "Logistic regression",
    "section": "From odds to probabilities",
    "text": "From odds to probabilities\n\nLogistic model: log odds = \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\)\nOdds = \\(\\exp\\big\\{\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\big\\} = \\frac{\\pi}{1-\\pi}\\)\nCombining (1) and (2) with what we saw earlier\n\n\\[\\text{probability} = \\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\\]"
  },
  {
    "objectID": "slides/lec-18.html#logistic-regression-model",
    "href": "slides/lec-18.html#logistic-regression-model",
    "title": "Logistic regression",
    "section": "Logistic regression model",
    "text": "Logistic regression model\nLogit form: \\[\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\]\n\nProbability form:\n$$\n= \n$$"
  },
  {
    "objectID": "slides/lec-18.html#risk-of-coronary-heart-disease",
    "href": "slides/lec-18.html#risk-of-coronary-heart-disease",
    "title": "Logistic regression",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use age to predict if a randomly selected adult is high risk of having coronary heart disease in the next 10 years.\nhigh_risk:\n\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\n\nage: Age at exam time (in years)"
  },
  {
    "objectID": "slides/lec-18.html#data-heart",
    "href": "slides/lec-18.html#data-heart",
    "title": "Logistic regression",
    "section": "Data: heart",
    "text": "Data: heart\n\nheart_disease <- read_csv(here::here(\"slides\", \"data/framingham.csv\")) %>%\n  select(age, TenYearCHD) %>%\n  drop_na() %>%\n  mutate(high_risk = as.factor(TenYearCHD)) %>%\n  select(age, high_risk)\n\nheart_disease\n\n# A tibble: 4,240 √ó 2\n     age high_risk\n   <dbl> <fct>    \n 1    39 0        \n 2    46 0        \n 3    48 0        \n 4    61 1        \n 5    46 0        \n 6    43 0        \n 7    63 1        \n 8    45 0        \n 9    52 0        \n10    43 0        \n# ‚Ä¶ with 4,230 more rows"
  },
  {
    "objectID": "slides/lec-18.html#high-risk-vs.-age",
    "href": "slides/lec-18.html#high-risk-vs.-age",
    "title": "Logistic regression",
    "section": "High risk vs.¬†age",
    "text": "High risk vs.¬†age\n\nggplot(heart_disease, aes(x = high_risk, y = age)) +\n  geom_boxplot() +\n  labs(x = \"High risk - 1: yes, 0: no\",\n       y = \"Age\", \n       title = \"Age vs. High risk of heart disease\")"
  },
  {
    "objectID": "slides/lec-18.html#lets-fit-the-model",
    "href": "slides/lec-18.html#lets-fit-the-model",
    "title": "Logistic regression",
    "section": "Let‚Äôs fit the model",
    "text": "Let‚Äôs fit the model\n\nheart_disease_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(high_risk ~ age, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) %>% kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n\n\nage\n0.075\n0.005\n14.178\n0"
  },
  {
    "objectID": "slides/lec-18.html#the-model",
    "href": "slides/lec-18.html#the-model",
    "title": "Logistic regression",
    "section": "The model",
    "text": "The model\n\ntidy(heart_disease_fit) %>% kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n\n\nage\n0.075\n0.005\n14.178\n0\n\n\n\n\n\n. . .\n\n\\[\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.561 + 0.075 \\times \\text{age}\\] where \\(\\hat{\\pi}\\) is the predicted probability of being high risk"
  },
  {
    "objectID": "slides/lec-18.html#predicted-log-odds",
    "href": "slides/lec-18.html#predicted-log-odds",
    "title": "Logistic regression",
    "section": "Predicted log odds",
    "text": "Predicted log odds\n\naugment(heart_disease_fit$fit)\n\n# A tibble: 4,240 √ó 8\n   high_risk   age .fitted .resid .std.resid     .hat .sigma   .cooksd\n   <fct>     <dbl>   <dbl>  <dbl>      <dbl>    <dbl>  <dbl>     <dbl>\n 1 0            39  -2.65  -0.370     -0.370 0.000466  0.895 0.0000165\n 2 0            46  -2.13  -0.475     -0.475 0.000322  0.895 0.0000192\n 3 0            48  -1.98  -0.509     -0.509 0.000288  0.895 0.0000199\n 4 1            61  -1.01   1.62       1.62  0.000706  0.895 0.000968 \n 5 0            46  -2.13  -0.475     -0.475 0.000322  0.895 0.0000192\n 6 0            43  -2.35  -0.427     -0.427 0.000384  0.895 0.0000183\n 7 1            63  -0.858  1.56       1.56  0.000956  0.895 0.00113  \n 8 0            45  -2.20  -0.458     -0.458 0.000342  0.895 0.0000189\n 9 0            52  -1.68  -0.585     -0.585 0.000262  0.895 0.0000244\n10 0            43  -2.35  -0.427     -0.427 0.000384  0.895 0.0000183\n# ‚Ä¶ with 4,230 more rows\n\n\n\nFor observation 1\n\\[\\text{predicted odds} = \\hat{\\omega} = \\frac{\\hat{\\pi}}{1-\\hat{\\pi}} = \\exp\\{-2.650\\} = 0.071\\]"
  },
  {
    "objectID": "slides/lec-18.html#predicted-probabilities",
    "href": "slides/lec-18.html#predicted-probabilities",
    "title": "Logistic regression",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\npredict(heart_disease_fit, new_data = heart_disease, type = \"prob\")\n\n# A tibble: 4,240 √ó 2\n   .pred_0 .pred_1\n     <dbl>   <dbl>\n 1   0.934  0.0660\n 2   0.894  0.106 \n 3   0.878  0.122 \n 4   0.733  0.267 \n 5   0.894  0.106 \n 6   0.913  0.0870\n 7   0.702  0.298 \n 8   0.900  0.0996\n 9   0.843  0.157 \n10   0.913  0.0870\n# ‚Ä¶ with 4,230 more rows\n\n\n\n\\[\\text{predicted probabilities} = \\hat{\\pi} = \\frac{\\exp\\{-2.650\\}}{1 + \\exp\\{-2.650\\}} = 0.066\\]"
  },
  {
    "objectID": "slides/lec-18.html#predicted-classes",
    "href": "slides/lec-18.html#predicted-classes",
    "title": "Logistic regression",
    "section": "Predicted classes",
    "text": "Predicted classes\n\npredict(heart_disease_fit, new_data = heart_disease, type = \"class\")\n\n# A tibble: 4,240 √ó 1\n   .pred_class\n   <fct>      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# ‚Ä¶ with 4,230 more rows"
  },
  {
    "objectID": "slides/lec-18.html#default-prediction",
    "href": "slides/lec-18.html#default-prediction",
    "title": "Logistic regression",
    "section": "Default prediction",
    "text": "Default prediction\nFor a logistic regression, the default prediction is the class.\n\npredict(heart_disease_fit, new_data = heart_disease)\n\n# A tibble: 4,240 √ó 1\n   .pred_class\n   <fct>      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# ‚Ä¶ with 4,230 more rows"
  },
  {
    "objectID": "slides/lec-18.html#observed-vs.-predicted",
    "href": "slides/lec-18.html#observed-vs.-predicted",
    "title": "Logistic regression",
    "section": "Observed vs.¬†predicted",
    "text": "Observed vs.¬†predicted\n\nWhat does the following table show?\n\n\npredict(heart_disease_fit, new_data = heart_disease) %>%\n  bind_cols(heart_disease) %>%\n  count(high_risk, .pred_class)\n\n# A tibble: 2 √ó 3\n  high_risk .pred_class     n\n  <fct>     <fct>       <int>\n1 0         0            3596\n2 1         0             644"
  },
  {
    "objectID": "slides/lec-18.html#recap",
    "href": "slides/lec-18.html#recap",
    "title": "Logistic regression",
    "section": "Recap",
    "text": "Recap\n\nLogistic regression for binary response variable\nRelationship between odds and probabilities\nUsed logistic regression model to calculate predicted odds and probabilities"
  },
  {
    "objectID": "slides/lec-18.html#application-exercise",
    "href": "slides/lec-18.html#application-exercise",
    "title": "Logistic regression",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã github.com/STA210-Summer22/ae-9-odds\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-24.html#topics",
    "href": "slides/lec-24.html#topics",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Topics",
    "text": "Topics\n\n\nPredictions\nModel selection\nChecking conditions"
  },
  {
    "objectID": "slides/lec-24.html#computational-setup",
    "href": "slides/lec-24.html#computational-setup",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(NHANES)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(colorblindr)\nlibrary(pROC)\nlibrary(Stat2Data)\nlibrary(nnet)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-24.html#nhanes-data",
    "href": "slides/lec-24.html#nhanes-data",
    "title": "MultiLR: Prediction + inferential models",
    "section": "NHANES Data",
    "text": "NHANES Data\n\n\nNational Health and Nutrition Examination Survey is conducted by the National Center for Health Statistics (NCHS).\nThe goal is to ‚Äúassess the health and nutritional status of adults and children in the United States‚Äù.\nThis survey includes an interview and a physical examination."
  },
  {
    "objectID": "slides/lec-24.html#variables",
    "href": "slides/lec-24.html#variables",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Variables",
    "text": "Variables\nGoal: Use a person‚Äôs age and whether they do regular physical activity to predict their self-reported health rating.\n\nOutcome: HealthGen: Self-reported rating of participant‚Äôs health in general. Excellent, Vgood, Good, Fair, or Poor.\nPredictors:\n\nAge: Age at time of screening (in years). Participants 80 or older were recorded as 80.\nPhysActive: Participant does moderate to vigorous-intensity sports, fitness or recreational activities."
  },
  {
    "objectID": "slides/lec-24.html#the-data",
    "href": "slides/lec-24.html#the-data",
    "title": "MultiLR: Prediction + inferential models",
    "section": "The data",
    "text": "The data\n\nnhanes_adult <- NHANES %>%\n  filter(Age >= 18) %>%\n  select(HealthGen, Age, PhysActive, Education) %>%\n  drop_na() %>%\n  mutate(obs_num = 1:n())\n\n\nglimpse(nhanes_adult)\n\nRows: 6,465\nColumns: 5\n$ HealthGen  <fct> Good, Good, Good, Good, Vgood, Vgood, Vgood, Vgood, Vgood, ‚Ä¶\n$ Age        <int> 34, 34, 34, 49, 45, 45, 45, 66, 58, 54, 50, 33, 60, 56, 56,‚Ä¶\n$ PhysActive <fct> No, No, No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, ‚Ä¶\n$ Education  <fct> High School, High School, High School, Some College, Colleg‚Ä¶\n$ obs_num    <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ‚Ä¶"
  },
  {
    "objectID": "slides/lec-24.html#model-in-r",
    "href": "slides/lec-24.html#model-in-r",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Model in R",
    "text": "Model in R\n\nhealth_fit <- multinom_reg() %>%\n  set_engine(\"nnet\") %>%\n  fit(HealthGen ~ Age + PhysActive, data = nhanes_adult)\n\nhealth_fit <- repair_call(health_fit, data = nhanes_adult)"
  },
  {
    "objectID": "slides/lec-24.html#model-summary",
    "href": "slides/lec-24.html#model-summary",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Model summary",
    "text": "Model summary\n\ntidy(health_fit) %>% print(n = 12)\n\n# A tibble: 12 √ó 6\n   y.level term            estimate std.error statistic  p.value\n   <chr>   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n 1 Vgood   (Intercept)    1.27        0.154      8.23   1.80e-16\n 2 Vgood   Age           -0.0000361   0.00259   -0.0139 9.89e- 1\n 3 Vgood   PhysActiveYes -0.332       0.0949    -3.50   4.72e- 4\n 4 Good    (Intercept)    1.99        0.150     13.3    2.81e-40\n 5 Good    Age           -0.00304     0.00256   -1.19   2.35e- 1\n 6 Good    PhysActiveYes -1.01        0.0921   -11.0    4.80e-28\n 7 Fair    (Intercept)    1.03        0.174      5.94   2.89e- 9\n 8 Fair    Age            0.00113     0.00302    0.373  7.09e- 1\n 9 Fair    PhysActiveYes -1.66        0.109    -15.2    4.14e-52\n10 Poor    (Intercept)   -1.34        0.299     -4.47   7.65e- 6\n11 Poor    Age            0.0193      0.00505    3.83   1.30e- 4\n12 Poor    PhysActiveYes -2.67        0.236    -11.3    1.20e-29"
  },
  {
    "objectID": "slides/lec-24.html#calculating-probabilities",
    "href": "slides/lec-24.html#calculating-probabilities",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Calculating probabilities",
    "text": "Calculating probabilities\n\nFor categories \\(2,\\ldots,K\\), the probability that the \\(i^{th}\\) observation is in the \\(j^{th}\\) category is\n\\[\n\\hat{\\pi}_{ij} = \\frac{e^{\\hat{\\beta}_{0j} + \\hat{\\beta}_{1j}x_{i1} + \\dots + \\hat{\\beta}_{pj}x_{ip}}}{1 + \\sum\\limits_{k=2}^K e^{\\hat{\\beta}_{0k} + \\hat{\\beta}_{1k}x_{i1} + \\dots \\hat{\\beta}_{pk}x_{ip}}}\n\\]\nFor the baseline category, \\(k=1\\), we calculate the probability \\(\\hat{\\pi}_{i1}\\) as\n\\[\n\\hat{\\pi}_{i1} = 1- \\sum\\limits_{k=2}^K \\hat{\\pi}_{ik}\n\\]"
  },
  {
    "objectID": "slides/lec-24.html#predicted-health-rating",
    "href": "slides/lec-24.html#predicted-health-rating",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Predicted health rating",
    "text": "Predicted health rating\nWe can use our model to predict a person‚Äôs perceived health rating given their age and whether they exercise.\n\nhealth_aug <- augment(health_fit, new_data = nhanes_adult)\nhealth_aug\n\n# A tibble: 6,465 √ó 11\n   HealthGen   Age PhysActive Education      obs_num .pred_class .pred_Excellent\n   <fct>     <int> <fct>      <fct>            <int> <fct>                 <dbl>\n 1 Good         34 No         High School          1 Good                 0.0687\n 2 Good         34 No         High School          2 Good                 0.0687\n 3 Good         34 No         High School          3 Good                 0.0687\n 4 Good         49 No         Some College         4 Good                 0.0691\n 5 Vgood        45 Yes        College Grad         5 Vgood                0.155 \n 6 Vgood        45 Yes        College Grad         6 Vgood                0.155 \n 7 Vgood        45 Yes        College Grad         7 Vgood                0.155 \n 8 Vgood        66 Yes        Some College         8 Vgood                0.157 \n 9 Vgood        58 Yes        College Grad         9 Vgood                0.156 \n10 Fair         54 Yes        9 - 11th Grade      10 Vgood                0.156 \n# ‚Ä¶ with 6,455 more rows, and 4 more variables: .pred_Vgood <dbl>,\n#   .pred_Good <dbl>, .pred_Fair <dbl>, .pred_Poor <dbl>"
  },
  {
    "objectID": "slides/lec-24.html#actual-vs.-predicted-health-rating",
    "href": "slides/lec-24.html#actual-vs.-predicted-health-rating",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Actual vs.¬†predicted health rating",
    "text": "Actual vs.¬†predicted health rating\nFor each observation, the predicted perceived health rating is the category with the highest predicted probability.\n\nhealth_aug %>% select(contains(\"pred\"))\n\n# A tibble: 6,465 √ó 6\n   .pred_class .pred_Excellent .pred_Vgood .pred_Good .pred_Fair .pred_Poor\n   <fct>                 <dbl>       <dbl>      <dbl>      <dbl>      <dbl>\n 1 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 2 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 3 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 4 Good                 0.0691       0.244      0.435     0.205     0.0467 \n 5 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 6 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 7 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 8 Vgood                0.157        0.400      0.342     0.0904    0.0102 \n 9 Vgood                0.156        0.397      0.349     0.0890    0.00872\n10 Vgood                0.156        0.396      0.352     0.0883    0.00804\n# ‚Ä¶ with 6,455 more rows"
  },
  {
    "objectID": "slides/lec-24.html#confusion-matrix",
    "href": "slides/lec-24.html#confusion-matrix",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nhealth_conf <- health_aug %>% \n  count(HealthGen, .pred_class, .drop = FALSE) %>%\n  pivot_wider(names_from = .pred_class, values_from = n)\n\nhealth_conf\n\n# A tibble: 5 √ó 6\n  HealthGen Excellent Vgood  Good  Fair  Poor\n  <fct>         <int> <int> <int> <int> <int>\n1 Excellent         0   528   210     0     0\n2 Vgood             0  1341   743     0     0\n3 Good              0  1226  1316     0     0\n4 Fair              0   296   625     0     0\n5 Poor              0    24   156     0     0"
  },
  {
    "objectID": "slides/lec-24.html#actual-vs.-predicted-health-rating-1",
    "href": "slides/lec-24.html#actual-vs.-predicted-health-rating-1",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Actual vs.¬†predicted health rating",
    "text": "Actual vs.¬†predicted health rating\n\nWhy do you think no observations were predicted to have a rating of ‚ÄúExcellent‚Äù, ‚ÄúFair‚Äù, or ‚ÄúPoor‚Äù?"
  },
  {
    "objectID": "slides/lec-24.html#comparing-nested-models",
    "href": "slides/lec-24.html#comparing-nested-models",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced model includes predictors \\(x_1, \\ldots, x_q\\)\nFull model includes predictors \\(x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p\\)\n\nWe want to test the following hypotheses:\n\n\\(H_0: \\beta_{q+1} = \\dots = \\beta_p = 0\\)\n\\(H_A: \\text{ at least 1 }\\beta_j \\text{ is not } 0\\)\n\nTo do so, we will use the drop-in-deviance test (very similar to logistic regression)"
  },
  {
    "objectID": "slides/lec-24.html#add-education-to-the-model",
    "href": "slides/lec-24.html#add-education-to-the-model",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Add Education to the model?",
    "text": "Add Education to the model?\n\nWe consider adding the participants‚Äô Education level to the model.\n\nEducation takes values 8thGrade, 9-11thGrade, HighSchool, SomeCollege, and CollegeGrad\n\nModels we‚Äôre testing:\n\nReduced model: Age, PhysActive\nFull model: Age, PhysActive, Education\n\n\n\n\\[\n\\begin{align}\n&H_0: \\beta_{9-11thGrade} = \\beta_{HighSchool} = \\beta_{SomeCollege} = \\beta_{CollegeGrad} = 0\\\\\n&H_a: \\text{ at least one }\\beta_j \\text{ is not equal to }0\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lec-24.html#add-education-to-the-model-1",
    "href": "slides/lec-24.html#add-education-to-the-model-1",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Add Education to the model?",
    "text": "Add Education to the model?\n\nreduced_fit <- multinom_reg() %>%\n  set_engine(\"nnet\") %>%\n  fit(HealthGen ~ Age + PhysActive,\n  data = nhanes_adult)\n\nfull_fit <- multinom_reg() %>%\n  set_engine(\"nnet\") %>%\n  fit(HealthGen ~ Age + PhysActive + Education,\n  data = nhanes_adult)\n  \nreduced_fit <- repair_call(reduced_fit, data = nhanes_adult)\nfull_fit <- repair_call(full_fit, data = nhanes_adult)"
  },
  {
    "objectID": "slides/lec-24.html#add-education-to-the-model-2",
    "href": "slides/lec-24.html#add-education-to-the-model-2",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Add Education to the model?",
    "text": "Add Education to the model?\n\nanova(reduced_fit$fit, full_fit$fit, test = \"Chisq\") %>%\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nResid. df\nResid. Dev\nTest\nDf\nLR stat.\nPr(Chi)\n\n\n\n\nAge + PhysActive\n25848\n16994.23\n\nNA\nNA\nNA\n\n\nAge + PhysActive + Education\n25832\n16505.10\n1 vs 2\n16\n489.132\n0\n\n\n\n\n\n\nAt least one coefficient associated with Education is non-zero. Therefore, we will include Education in the model."
  },
  {
    "objectID": "slides/lec-24.html#model-with-education",
    "href": "slides/lec-24.html#model-with-education",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Model with Education",
    "text": "Model with Education\n\ntidy(full_fit, conf.int = T) %>% print(n = 28)\n\n# A tibble: 28 √ó 8\n   y.level term         estimate std.error statistic  p.value conf.low conf.high\n   <chr>   <chr>           <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n 1 Vgood   (Intercept)   5.82e-1   0.301      1.93   5.36e- 2 -0.00914   1.17   \n 2 Vgood   Age           1.12e-3   0.00266    0.419  6.75e- 1 -0.00411   0.00634\n 3 Vgood   PhysActiveY‚Ä¶ -2.64e-1   0.0985    -2.68   7.33e- 3 -0.457    -0.0711 \n 4 Vgood   Education9 ‚Ä¶  7.68e-1   0.308      2.49   1.27e- 2  0.164     1.37   \n 5 Vgood   EducationHi‚Ä¶  7.01e-1   0.280      2.51   1.21e- 2  0.153     1.25   \n 6 Vgood   EducationSo‚Ä¶  7.88e-1   0.271      2.90   3.71e- 3  0.256     1.32   \n 7 Vgood   EducationCo‚Ä¶  4.08e-1   0.268      1.52   1.28e- 1 -0.117     0.933  \n 8 Good    (Intercept)   2.04e+0   0.272      7.51   5.77e-14  1.51      2.57   \n 9 Good    Age          -1.72e-3   0.00263   -0.651  5.15e- 1 -0.00688   0.00345\n10 Good    PhysActiveY‚Ä¶ -7.58e-1   0.0961    -7.88   3.16e-15 -0.946    -0.569  \n11 Good    Education9 ‚Ä¶  3.60e-1   0.275      1.31   1.90e- 1 -0.179     0.899  \n12 Good    EducationHi‚Ä¶  8.52e-2   0.247      0.345  7.30e- 1 -0.399     0.569  \n13 Good    EducationSo‚Ä¶ -1.13e-2   0.239     -0.0472 9.62e- 1 -0.480     0.457  \n14 Good    EducationCo‚Ä¶ -8.91e-1   0.236     -3.77   1.65e- 4 -1.35     -0.427  \n15 Fair    (Intercept)   2.12e+0   0.288      7.35   1.91e-13  1.55      2.68   \n16 Fair    Age           3.35e-4   0.00312    0.107  9.14e- 1 -0.00578   0.00645\n17 Fair    PhysActiveY‚Ä¶ -1.19e+0   0.115    -10.4    3.50e-25 -1.42     -0.966  \n18 Fair    Education9 ‚Ä¶ -2.24e-1   0.279     -0.802  4.22e- 1 -0.771     0.323  \n19 Fair    EducationHi‚Ä¶ -8.32e-1   0.252     -3.31   9.44e- 4 -1.33     -0.339  \n20 Fair    EducationSo‚Ä¶ -1.34e+0   0.246     -5.46   4.71e- 8 -1.82     -0.861  \n21 Fair    EducationCo‚Ä¶ -2.51e+0   0.253     -9.91   3.67e-23 -3.00     -2.01   \n22 Poor    (Intercept)  -2.00e-1   0.411     -0.488  6.26e- 1 -1.01      0.605  \n23 Poor    Age           1.79e-2   0.00509    3.53   4.21e- 4  0.00797   0.0279 \n24 Poor    PhysActiveY‚Ä¶ -2.27e+0   0.242     -9.38   6.81e-21 -2.74     -1.79   \n25 Poor    Education9 ‚Ä¶ -3.60e-1   0.353     -1.02   3.08e- 1 -1.05      0.332  \n26 Poor    EducationHi‚Ä¶ -1.15e+0   0.334     -3.44   5.86e- 4 -1.81     -0.494  \n27 Poor    EducationSo‚Ä¶ -1.07e+0   0.316     -3.40   6.77e- 4 -1.69     -0.454  \n28 Poor    EducationCo‚Ä¶ -2.32e+0   0.366     -6.34   2.27e-10 -3.04     -1.60"
  },
  {
    "objectID": "slides/lec-24.html#compare-nhanes-models-using-aic",
    "href": "slides/lec-24.html#compare-nhanes-models-using-aic",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Compare NHANES models using AIC",
    "text": "Compare NHANES models using AIC\nReduced model:\n\nglance(reduced_fit)$AIC\n\n[1] 17018.23\n\n\n\nFull model:\n\nglance(full_fit)$AIC\n\n[1] 16561.1"
  },
  {
    "objectID": "slides/lec-24.html#conditions-for-inference",
    "href": "slides/lec-24.html#conditions-for-inference",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Conditions for inference",
    "text": "Conditions for inference\nWe want to check the following conditions for inference for the multinomial logistic regression model:\n\nLinearity: Is there a linear relationship between the log-odds and the predictor variables?\nRandomness: Was the sample randomly selected? Or can we reasonably treat it as random?\nIndependence: Are the observations independent?"
  },
  {
    "objectID": "slides/lec-24.html#checking-linearity",
    "href": "slides/lec-24.html#checking-linearity",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking linearity",
    "text": "Checking linearity\nSimilar to logistic regression, we will check linearity by examining empirical logit plots between each level of the response and the quantitative predictor variables.\n\nnhanes_adult <- nhanes_adult %>%\n  mutate(\n    Excellent = factor(if_else(HealthGen == \"Excellent\", \"1\", \"0\")),\n    Vgood = factor(if_else(HealthGen == \"Vgood\", \"1\", \"0\")),\n    Good = factor(if_else(HealthGen == \"Good\", \"1\", \"0\")),\n    Fair = factor(if_else(HealthGen == \"Fair\", \"1\", \"0\")),\n    Poor = factor(if_else(HealthGen == \"Poor\", \"1\", \"0\"))\n  )"
  },
  {
    "objectID": "slides/lec-24.html#checking-linearity-1",
    "href": "slides/lec-24.html#checking-linearity-1",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nemplogitplot1(Excellent ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Excellent vs. Age\")\n\nemplogitplot1(Vgood ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Vgood vs. Age\")"
  },
  {
    "objectID": "slides/lec-24.html#checking-linearity-2",
    "href": "slides/lec-24.html#checking-linearity-2",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nemplogitplot1(Good ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Good vs. Age\")\n\nemplogitplot1(Fair ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Fair vs. Age\")"
  },
  {
    "objectID": "slides/lec-24.html#checking-linearity-3",
    "href": "slides/lec-24.html#checking-linearity-3",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nemplogitplot1(Poor ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Poor vs. Age\")\n\n\n\n‚úÖ The linearity condition is satisfied. There is a linear relationship between the empirical logit and the quantitative predictor variable, Age."
  },
  {
    "objectID": "slides/lec-24.html#checking-randomness",
    "href": "slides/lec-24.html#checking-randomness",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking randomness",
    "text": "Checking randomness\nWe can check the randomness condition based on the context of the data and how the observations were collected.\n\nWas the sample randomly selected?\nIf the sample was not randomly selected, ask whether there is reason to believe the observations in the sample differ systematically from the population of interest.\n\n\n‚úÖ The randomness condition is satisfied. We do not have reason to believe that the participants in this study differ systematically from adults in the U.S.."
  },
  {
    "objectID": "slides/lec-24.html#checking-independence",
    "href": "slides/lec-24.html#checking-independence",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking independence",
    "text": "Checking independence\nWe can check the independence condition based on the context of the data and how the observations were collected.\nIndependence is most often violated if the data were collected over time or there is a strong spatial relationship between the observations.\n\n‚úÖ The independence condition is satisfied. It is reasonable to conclude that the participants‚Äô health and behavior characteristics are independent of one another."
  },
  {
    "objectID": "slides/lec-24.html#recap",
    "href": "slides/lec-24.html#recap",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Recap",
    "text": "Recap\n\nPredictions\nModel selection for inference\nChecking conditions for inference\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-2.html#announcements",
    "href": "slides/lec-2.html#announcements",
    "title": "Simple Linear Regression",
    "section": "Announcements",
    "text": "Announcements\n\nIf you‚Äôre just joining the class, welcome! Go to the course website and review content you‚Äôve missed, read the syllabus, and complete the Getting to know you survey.\nLab 1 is due Friday, at 11:59pm, on Gradescope."
  },
  {
    "objectID": "slides/lec-2.html#review-on-lab-1",
    "href": "slides/lec-2.html#review-on-lab-1",
    "title": "Simple Linear Regression",
    "section": "Review on Lab 1",
    "text": "Review on Lab 1\n\nGet used to R and R studio\nGet used to use R with Git : clone, commit, pull, push\nLearn the components of drawing a plot\n\ndata\nmapping (x,y,color,alpha)\ngeometry (histogram, point, density)\nlab (label for x,y-axises, title, etc.)\nCheck cheatsheet"
  },
  {
    "objectID": "slides/lec-2.html#review-on-lab-1-1",
    "href": "slides/lec-2.html#review-on-lab-1-1",
    "title": "Simple Linear Regression",
    "section": "Review on Lab 1",
    "text": "Review on Lab 1\n\nQuestions:\n\nChoice of binwidth for histogram: Pick the one and reveal the main trend. Too small: too noisy; Too large: ignore trend\nDensity Plot (Smooth version of histogram. eg. Normal Distribution)"
  },
  {
    "objectID": "slides/lec-2.html#outline",
    "href": "slides/lec-2.html#outline",
    "title": "Simple Linear Regression",
    "section": "Outline",
    "text": "Outline\n\nUse simple linear regression to describe the relationship between a quantitative predictor and quantitative outcome variable\nEstimate the slope and intercept of the regression line using the least squares method\nInterpret the slope and intercept of the regression line"
  },
  {
    "objectID": "slides/lec-2.html#computational-setup",
    "href": "slides/lec-2.html#computational-setup",
    "title": "Simple Linear Regression",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(tidymodels)      # for modeling\nlibrary(fivethirtyeight) # for the fandango dataset\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/lec-2.html#movie-ratings",
    "href": "slides/lec-2.html#movie-ratings",
    "title": "Simple Linear Regression",
    "section": "Movie ratings",
    "text": "Movie ratings\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango‚Äôs\nIn the fivethirtyeight package: fandango\nContains every film that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/lec-2.html#data-prep",
    "href": "slides/lec-2.html#data-prep",
    "title": "Simple Linear Regression",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores <- fandango %>%\n  rename(\n    critics = rottentomatoes, \n    audience = rottentomatoes_user\n  )"
  },
  {
    "objectID": "slides/lec-2.html#data-overview",
    "href": "slides/lec-2.html#data-overview",
    "title": "Simple Linear Regression",
    "section": "Data overview",
    "text": "Data overview\n\nglimpse(movie_scores)\n\nRows: 146\nColumns: 23\n$ film                       <chr> \"Avengers: Age of Ultron\", \"Cinderella\", \"A‚Ä¶\n$ year                       <dbl> 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2‚Ä¶\n$ critics                    <int> 74, 85, 80, 18, 14, 63, 42, 86, 99, 89, 84,‚Ä¶\n$ audience                   <int> 86, 80, 90, 84, 28, 62, 53, 64, 82, 87, 77,‚Ä¶\n$ metacritic                 <int> 66, 67, 64, 22, 29, 50, 53, 81, 81, 80, 71,‚Ä¶\n$ metacritic_user            <dbl> 7.1, 7.5, 8.1, 4.7, 3.4, 6.8, 7.6, 6.8, 8.8‚Ä¶\n$ imdb                       <dbl> 7.8, 7.1, 7.8, 5.4, 5.1, 7.2, 6.9, 6.5, 7.4‚Ä¶\n$ fandango_stars             <dbl> 5.0, 5.0, 5.0, 5.0, 3.5, 4.5, 4.0, 4.0, 4.5‚Ä¶\n$ fandango_ratingvalue       <dbl> 4.5, 4.5, 4.5, 4.5, 3.0, 4.0, 3.5, 3.5, 4.0‚Ä¶\n$ rt_norm                    <dbl> 3.70, 4.25, 4.00, 0.90, 0.70, 3.15, 2.10, 4‚Ä¶\n$ rt_user_norm               <dbl> 4.30, 4.00, 4.50, 4.20, 1.40, 3.10, 2.65, 3‚Ä¶\n$ metacritic_norm            <dbl> 3.30, 3.35, 3.20, 1.10, 1.45, 2.50, 2.65, 4‚Ä¶\n$ metacritic_user_nom        <dbl> 3.55, 3.75, 4.05, 2.35, 1.70, 3.40, 3.80, 3‚Ä¶\n$ imdb_norm                  <dbl> 3.90, 3.55, 3.90, 2.70, 2.55, 3.60, 3.45, 3‚Ä¶\n$ rt_norm_round              <dbl> 3.5, 4.5, 4.0, 1.0, 0.5, 3.0, 2.0, 4.5, 5.0‚Ä¶\n$ rt_user_norm_round         <dbl> 4.5, 4.0, 4.5, 4.0, 1.5, 3.0, 2.5, 3.0, 4.0‚Ä¶\n$ metacritic_norm_round      <dbl> 3.5, 3.5, 3.0, 1.0, 1.5, 2.5, 2.5, 4.0, 4.0‚Ä¶\n$ metacritic_user_norm_round <dbl> 3.5, 4.0, 4.0, 2.5, 1.5, 3.5, 4.0, 3.5, 4.5‚Ä¶\n$ imdb_norm_round            <dbl> 4.0, 3.5, 4.0, 2.5, 2.5, 3.5, 3.5, 3.5, 3.5‚Ä¶\n$ metacritic_user_vote_count <int> 1330, 249, 627, 31, 88, 34, 17, 124, 62, 54‚Ä¶\n$ imdb_user_vote_count       <int> 271107, 65709, 103660, 3136, 19560, 39373, ‚Ä¶\n$ fandango_votes             <int> 14846, 12640, 12055, 1793, 1021, 397, 252, ‚Ä¶\n$ fandango_difference        <dbl> 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5‚Ä¶"
  },
  {
    "objectID": "slides/lec-2.html#data-visualization",
    "href": "slides/lec-2.html#data-visualization",
    "title": "Simple Linear Regression",
    "section": "Data visualization",
    "text": "Data visualization"
  },
  {
    "objectID": "slides/lec-2.html#fit-a-line",
    "href": "slides/lec-2.html#fit-a-line",
    "title": "Simple Linear Regression",
    "section": "Fit a line",
    "text": "Fit a line\n‚Ä¶ to describe the relationship between the critics and audience score"
  },
  {
    "objectID": "slides/lec-2.html#terminology",
    "href": "slides/lec-2.html#terminology",
    "title": "Simple Linear Regression",
    "section": "Terminology",
    "text": "Terminology\n\n\n\nOutcome, Y: variable describing the outcome of interest\nPredictor, X: variable used to help understand the variability in the outcome"
  },
  {
    "objectID": "slides/lec-2.html#regression-model-1",
    "href": "slides/lec-2.html#regression-model-1",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\nA regression model is a function that describes the relationship between the outcome, \\(Y\\), and the predictor, \\(X\\).\n\\[\\begin{aligned} Y &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/lec-2.html#regression-model-2",
    "href": "slides/lec-2.html#regression-model-2",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n$$\n\\[\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n\n\n&= \\color{purple}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon\n\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "slides/lec-2.html#regression-model-residuals",
    "href": "slides/lec-2.html#regression-model-residuals",
    "title": "Simple Linear Regression",
    "section": "Regression model + residuals",
    "text": "Regression model + residuals\n\n\n\\[\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\color{blue}{\\textbf{Error}} \\\\[8pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[8pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[8pt]\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lec-2.html#simple-linear-regression-1",
    "href": "slides/lec-2.html#simple-linear-regression-1",
    "title": "Simple Linear Regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nUse simple linear regression to model the relationthip between a quantitative outcome (\\(Y\\)) and a single quantitative predictor (\\(X\\)): \\[\\Large{Y = \\beta_0 + \\beta_1 X + \\epsilon}\\]\n\n\n\\(\\beta_1\\): True slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\beta_0\\): True intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\epsilon\\): Error (residual)"
  },
  {
    "objectID": "slides/lec-2.html#simple-linear-regression-2",
    "href": "slides/lec-2.html#simple-linear-regression-2",
    "title": "Simple Linear Regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\\[\\Large{\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X}\\]\n\n\\(\\hat{\\beta}_1\\): Estimated slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\hat{\\beta}_0\\): Estimated intercept of the relationship between \\(X\\) and \\(Y\\)\nNo error term!"
  },
  {
    "objectID": "slides/lec-2.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "href": "slides/lec-2.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "title": "Simple Linear Regression",
    "section": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)",
    "text": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)"
  },
  {
    "objectID": "slides/lec-2.html#residuals",
    "href": "slides/lec-2.html#residuals",
    "title": "Simple Linear Regression",
    "section": "Residuals",
    "text": "Residuals\n\n\\[\\text{residual} = \\text{observed} - \\text{predicted} = y - \\hat{y}\\]"
  },
  {
    "objectID": "slides/lec-2.html#least-squares-line",
    "href": "slides/lec-2.html#least-squares-line",
    "title": "Simple Linear Regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nThe residual for the \\(i^{th}\\) observation is\n\n\\[e_i = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i\\]\n\nThe sum of squared residuals is\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nThe least squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/lec-2.html#properties-of-least-squares-regression",
    "href": "slides/lec-2.html#properties-of-least-squares-regression",
    "title": "Simple Linear Regression",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(X\\) and average \\(Y\\): \\(\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\\)\nThe slope has the same sign as the correlation coefficient: \\(\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}\\)\nThe sum of the residuals is zero: \\(\\sum_{i = 1}^n \\epsilon_i = 0\\)\nThe residuals and \\(X\\) values are uncorrelated"
  },
  {
    "objectID": "slides/lec-2.html#estimating-the-slope",
    "href": "slides/lec-2.html#estimating-the-slope",
    "title": "Simple Linear Regression",
    "section": "Estimating the slope",
    "text": "Estimating the slope\n\\[\\large{\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}}\\]\n\n\n\\[\\begin{aligned}\ns_X &= 30.1688 \\\\\ns_Y &=  20.0244 \\\\\nr &= 0.7814\n\\end{aligned}\\]\n\n$$\n\\[\\begin{aligned}\n\n\n\\hat{\\beta}_1 &= 0.7814 \\times \\frac{20.0244}{30.1688} \\\\\n&= 0.5187\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "slides/lec-2.html#estimating-the-intercept",
    "href": "slides/lec-2.html#estimating-the-intercept",
    "title": "Simple Linear Regression",
    "section": "Estimating the intercept",
    "text": "Estimating the intercept\n\\[\\large{\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}}\\]\n\n\n$$\n\\[\\begin{aligned}\n\n\n&\\bar{x} = 60.8493 \\\\\n&\\bar{y} = 63.8767 \\\\\n&\\hat{\\beta}_1 = 0.5187\n\\end{aligned}\\]\n$$\n\n\\[\\begin{aligned}\\hat{\\beta}_0 &= 63.8767 - 0.5187 \\times 60.8493 \\\\\n&= 32.3142\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lec-2.html#interpreting-the-slope",
    "href": "slides/lec-2.html#interpreting-the-slope",
    "title": "Simple Linear Regression",
    "section": "Interpreting the slope",
    "text": "Interpreting the slope\nPoll: The slope of the model for predicting audience score from critics score is 32.3142. Which of the following is the best interpretation of this value?\n\nFor every one point increase in the critics score, the audience score goes up by 0.5187 points, on average.\nFor every one point increase in the critics score, we expect the audience score to be higher by 0.5187 points, on average.\nFor every one point increase in the critics score, the audience score goes up by 0.5187 points.\nFor every one point increase in the audience score, the critics score goes up by 0.5187 points, on average."
  },
  {
    "objectID": "slides/lec-2.html#interpreting-slope-intercept",
    "href": "slides/lec-2.html#interpreting-slope-intercept",
    "title": "Simple Linear Regression",
    "section": "Interpreting slope & intercept",
    "text": "Interpreting slope & intercept\n\\[\\widehat{\\text{audience}} = 32.3142 + 0.5187 \\times \\text{critics}\\]\n\n\nSlope: For every one point increase in the critics score, we expect the audience score to be higher by 0.5187 points, on average.\nIntercept: If the critics score is 0 points, we expect the audience score to be 32.3142 points, on average."
  },
  {
    "objectID": "slides/lec-2.html#is-the-intercept-meaningful",
    "href": "slides/lec-2.html#is-the-intercept-meaningful",
    "title": "Simple Linear Regression",
    "section": "Is the intercept meaningful?",
    "text": "Is the intercept meaningful?\n‚úÖ The intercept is meaningful in context of the data if\n\nthe predictor can feasibly take values equal to or near zero or\nthe predictor has values near zero in the observed data\n\n\nüõë Otherwise, it might not be meaningful!"
  },
  {
    "objectID": "slides/lec-2.html#making-a-prediction",
    "href": "slides/lec-2.html#making-a-prediction",
    "title": "Simple Linear Regression",
    "section": "Making a prediction",
    "text": "Making a prediction\nSuppose that a movie has a critics score of 50. According to this model, what is the movie‚Äôs predicted audience score?\n\\[\n\\begin{aligned}\n\\widehat{\\text{audience}} &= 32.3142 + 0.5187 \\times \\text{critics} \\\\\n&= 32.3142 + 0.5187 \\times 50 \\\\\n&= 58.2492\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-2.html#extrapolation",
    "href": "slides/lec-2.html#extrapolation",
    "title": "Simple Linear Regression",
    "section": "Extrapolation",
    "text": "Extrapolation\nExtrapolation is prediction outside of the ranged covered by data.\nSuppose that a movie has a critics score of 0. According to this model, what is the movie‚Äôs predicted audience score?"
  },
  {
    "objectID": "slides/lec-2.html#recap-1",
    "href": "slides/lec-2.html#recap-1",
    "title": "Simple Linear Regression",
    "section": "Recap",
    "text": "Recap\n\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative outcome variable.\nUsed the least squares method to estimate the slope and intercept.\nWe interpreted the slope and intercept.\n\n\nSlope: For every one unit increase in \\(x\\), we expect y to be higher/lower by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units.\n\n\nPredicted the response given a value of the predictor variable.\nDefined extrapolation and why we should avoid it.\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation¬†1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\qquad(1)\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation¬†2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\qquad(2)\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation¬†1 and Equation¬†2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\qquad(3)\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n$$\n = = - \n$$ {#eq-matrix_mean}"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n$$\n{i=1}^{n} e{i}^2 = ^T = ( - )^T( - )\n$$ {#eq-sum_sq_resid}\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n$$\n( - )^T( - ) = (^T - ^T - ({T}T + {T}T )\n$$ {#eq-model_equation}\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e.¬†\\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, ?@eq-model_equation becomes\n$$\n^T - 2 T{T} + {T}T \n$$ {#eq-model_equation}\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes ?@eq-sum_sq_resid, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n$$\n\\[\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\\]\n$$ {#eq-ete}\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n$$\n\\[\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\\]\n$$ {#eq-expected_error}\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e.¬†\\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write ?@eq-expected_error as\n$$\nE[^T] =\n\\[\\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix}\\]\n= ^2 \n$$ {#eq-expected_error2}\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n$$\n\\[\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\\]\n$$ {#eq-est-beta}\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n$$\n\\[\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\\]\n$$ {#eq-var-cov}"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook‚Äôs distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation¬†1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\qquad(1)\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation¬†2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\qquad(2)\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Equation¬†1 and Equation¬†2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\qquad(3)\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n$$\n = = - \n$$ {#eq-matrix_mean}"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n$$\n = (T){-1}^T\n$$ {#eq-beta-hat}\nCombining ?@eq-matrix_mean and ?@eq-beta-hat, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n$$\n\\[\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\\]\n$$ {#eq-y-hat}\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus ?@eq-y-hat becomes\n$$\n = \n$$ {#eq-y-hat-matrix}\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n$$\nh_{ii} = + \n$$\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e.¬†outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from ?@eq-matrix_mean using ?@eq-y-hat-matrix.\n$$\n\\[\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\\]\n$$ {#eq-resid-hat}\nNote that the identity matrix and hat matrix are idempotent, i.e.¬†\\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and ?@eq-resid-hat, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n$$\n\\[\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\\]\n$$ {#eq-resid-var}\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n$$\nstd.res_i = \n$$\nThe expected value of the residuals is 0, i.e.¬†\\(E(e_i) = 0\\). From ?@eq-resid-var), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n$$\nstd.res_i = \n$$ {#eq-std-resid}"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook‚Äôs Distance",
    "text": "Cook‚Äôs Distance\nCook‚Äôs distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook‚Äôs distance for the \\(i^{th}\\) observation can be written as\n$$\nD_i = \n$$ {#eq-cooksd}\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook‚Äôs Distance can be calculated without deleting observations one at a time, since ?@eq-cooksd-v2 below is mathematically equivalent to ?@eq-cooksd.\n$$\nD_i = std.res_i^2= \n$$ {#eq-cooksd-v2}"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "This document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation¬†1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\qquad(1)\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation¬†1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation¬†2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\qquad(2)\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation¬†3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\qquad(3)\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation¬†4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\qquad(4)\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that‚Äôs more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\qquad(5)\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\qquad(6)\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation¬†5 and Equation¬†6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\qquad(7)\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation¬†7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\qquad(8)\\]"
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Equation¬†1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\qquad(1)\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in Equation¬†2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\qquad(2)\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in Equation¬†2 for interpretations and predictions, we will use Equation¬†3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\qquad(3)\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Equation¬†2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\qquad(4)\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e.¬†\\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in Equation¬†5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\qquad(5)\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e.¬†\\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike‚Äôs Information Criterion (AIC) and Schwarz‚Äôs Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\qquad(1)\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation¬†1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation¬†1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\qquad(2)\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation¬†2, i.e.¬†maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e.¬†the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\qquad(3)\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\qquad(4)\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation¬†4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike‚Äôs Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\qquad(5)\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let‚Äôs focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\qquad(6)\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.1 ‚îÄ‚îÄ\n\n\n‚úì ggplot2 3.3.5     ‚úì purrr   0.3.4\n‚úì tibble  3.1.6     ‚úì dplyr   1.0.7\n‚úì tidyr   1.1.4     ‚úì stringr 1.4.0\n‚úì readr   2.1.1     ‚úì forcats 0.5.1\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(tidymodels)\n\nRegistered S3 method overwritten by 'tune':\n  method                   from   \n  required_pkgs.model_spec parsnip\n\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 0.1.4 ‚îÄ‚îÄ\n\n\n‚úì broom        0.7.10         ‚úì rsample      0.1.1     \n‚úì dials        0.0.10         ‚úì tune         0.1.6     \n‚úì infer        1.0.1.9000     ‚úì workflows    0.2.4     \n‚úì modeldata    0.1.1          ‚úì workflowsets 0.1.0     \n‚úì parsnip      0.1.7          ‚úì yardstick    0.0.9     \n‚úì recipes      0.2.0          \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n‚Ä¢ Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(knitr)"
  },
  {
    "objectID": "computing-pipelines.html#simple-linear-regression",
    "href": "computing-pipelines.html#simple-linear-regression",
    "title": "Pipelines",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nModel fitting\nFit model:\n\npenguins_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(body_mass_g ~ flipper_length_mm, data = penguins)\n\nTidy model output:\n\ntidy(penguins_fit)\n\n# A tibble: 2 √ó 5\n  term              estimate std.error statistic   p.value\n  <chr>                <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\nFormat model output as table:\n\ntidy(penguins_fit) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5780.831\n305.815\n-18.903\n0\n\n\nflipper_length_mm\n49.686\n1.518\n32.722\n0\n\n\n\n\n\nAugment data with model:\n\naugment(penguins_fit$fit)\n\n# A tibble: 342 √ó 9\n   .rownames body_mass_g flipper_length_‚Ä¶ .fitted  .resid    .hat .sigma .cooksd\n   <chr>           <int>            <int>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>\n 1 1                3750              181   3212.  538.   0.00881   394. 8.34e-3\n 2 2                3800              186   3461.  339.   0.00622   394. 2.33e-3\n 3 3                3250              195   3908. -658.   0.00344   393. 4.83e-3\n 4 5                3450              193   3808. -358.   0.00385   394. 1.60e-3\n 5 6                3650              190   3659.   -9.43 0.00469   395. 1.35e-6\n 6 7                3625              181   3212.  413.   0.00881   394. 4.91e-3\n 7 8                4675              195   3908.  767.   0.00344   393. 6.56e-3\n 8 9                3475              193   3808. -333.   0.00385   394. 1.39e-3\n 9 10               4250              190   3659.  591.   0.00469   394. 5.31e-3\n10 11               3300              186   3461. -161.   0.00622   395. 5.23e-4\n# ‚Ä¶ with 332 more rows, and 1 more variable: .std.resid <dbl>\n\n\n\n\nStatistical inference"
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export‚Ä¶ If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you‚Äôve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, you‚Äôll upload your PDF and them mark the page(s) where each question can be found. It‚Äôs OK if a question spans multiple pages, just mark them all. It‚Äôs also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, I‚Äôd rather you didn‚Äôt, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we‚Äôre using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you‚Äôre working in the containers we have provided for you. If you‚Äôre working on your local setup, we can‚Äôt guarantee being able to resolve your issues, though we‚Äôre happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.1.2: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I‚Äôd like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "exams/exam-1.html",
    "href": "exams/exam-1.html",
    "title": "Exam 1",
    "section": "",
    "text": "Grading (50 pts)\n\n\n\nPart\nPoints\n\n\n\n\nPart 1 - Conceptual (on Sakai)\n10\n\n\nPart 2 - Applied (on Gradescope)\n40\n\n\nTotal\n50"
  },
  {
    "objectID": "exams/exam-3.html",
    "href": "exams/exam-3.html",
    "title": "Exam 3",
    "section": "",
    "text": "Grading (50 pts)\n\n\n\nPart\nPoints\n\n\n\n\nPart 1 - Conceptual (on Sakai)\n10\n\n\nPart 2 - Applied (on Gradescope)\n40\n\n\nTotal\n50"
  },
  {
    "objectID": "exams/exam-2.html",
    "href": "exams/exam-2.html",
    "title": "Exam 2",
    "section": "",
    "text": "Grading (50 pts)\n\n\n\nPart\nPoints\n\n\n\n\nPart 1 - Conceptual (on Sakai)\n10\n\n\nPart 2 - Applied (on Gradescope)\n40\n\n\nTotal\n50"
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Project description",
    "section": "",
    "text": "Topic ideas due Sun, May 29\nProposal due Wed, June 8\nDraft report due Sun, June 19\nProject peer review of drafts due Mon, June 20\nFinal report due Wed, June 22\nVideo presentation + slides and final GitHub repo due Wed, June 22\nPresentation comments due Wed, June 22"
  },
  {
    "objectID": "project-description.html#introduction",
    "href": "project-description.html#introduction",
    "title": "Project description",
    "section": "Introduction",
    "text": "Introduction\nTL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group‚Äôs interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio, and all components of the project must be reproducible (with the exception of the presentation).\n\nLogistics\nYou will work on the project with your lab groups.\nThe four primary deliverables for the final project are\n\nA written, reproducible report detailing your analysis\nA GitHub repository corresponding to your report\nSlides + a video presentation\nFormal peer review on another team‚Äôs project"
  },
  {
    "objectID": "project-description.html#topic-ideas",
    "href": "project-description.html#topic-ideas",
    "title": "Project description",
    "section": "Topic ideas",
    "text": "Topic ideas\nIdentify 2-3 data sets you‚Äôre interested in potentially using for the final project. If you‚Äôre unsure where to find data, you can use the list of potential data sources in the Tips + Resources section as a starting point. It may also help to think of topics you‚Äôre interested in investigating and find data sets on those topics.\nThe purpose of submitting project ideas is to give you time to find data for the project and to make sure you have a data set that can help you be successful in the project. Therefore, you must use one of the data sets submitted as a topic idea, unless otherwise notified by the teaching team.\nThe data sets should meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns\nAt least 6 of the columns must be useful and unique predictor variables.\n\nIdentifier variables such as ‚Äúname‚Äù, ‚Äúsocial security number‚Äù, etc. are not useful predictor variables.\nIf you have multiple columns with the same information (e.g.¬†‚Äústate abbreviation‚Äù and ‚Äústate name‚Äù), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nObservations should reasonably meet the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nYou may not use data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\nPlease ask TA or me if you‚Äôre unsure whether your data set meets the criteria.\nFor each data set, include the following:\n\nIntroduction and data\n\nState the source of the data set.\nDescribe when and how it was originally collected (by the original data curator, not necessarily how you found the data)\nDescribe the observations and the general characteristics being measured in the data\n\n\n\nResearch question\n\nDescribe a research question you‚Äôre interested in answering using this data.\n\n\n\nGlimpse of data\n\nUse the glimpse function to provide an overview of each data set\n\nSubmit the PDF of the topic ideas to Gradescope. Mark all pages associated with each data set."
  },
  {
    "objectID": "project-description.html#project-proposal",
    "href": "project-description.html#project-proposal",
    "title": "Project description",
    "section": "Project proposal",
    "text": "Project proposal\nThe purpose of the project proposal is to help you think about your analysis strategy early.\nInclude the following in the proposal:\n\nSection 1 - Introduction\nThe introduction section includes\n\nan introduction to the subject matter you‚Äôre investigating\nthe motivation for your research question (citing any relevant literature)\nthe general research question you wish to explore\nyour hypotheses regarding the research question of interest.\n\n\n\nSection 2 - Data description\nIn this section, you will describe the data set you wish to explore. This includes\n\ndescription of the observations in the data set,\ndescription of how the data was originally collected (not how you found the data but how the original curator of the data collected it).\n\n\n\nSection 3 - Analysis approach\nIn this section, you will provide a brief overview of your analysis approach. This includes:\n\nDescription of the response variable.\nVisualization and summary statistics for the response variable.\nList of variables that will be considered as predictors\nRegression model technique (multiple linear regression and logistic regression)\n\n\n\nData dictionary (aka code book)\nSubmit a data dictionary for all the variables in your data set in the README of your project repo, in the data folder. Link to this file from your proposal writeup.\n\n\nSubmission\nPush all of your final changes to the GitHub repo, and submit the PDF of your proposal to Gradescope.\n\n\nProposal grading\n\n\n\nTotal\n10 pts\n\n\n\n\nIntroduction\n3 pts\n\n\nData description\n2 pts\n\n\nAnalysis plan\n4 pts\n\n\nData dictionary\n1 pts\n\n\n\nEach component will be graded as follows:\n\nMeets expectations (full credit): All required elements are completed and are accurate. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nClose to expectations (half credit): There are some elements missing and/or inaccurate. There are some issues with formatting.\nDoes not meet expectations (no credit): Major elements missing. Work is not neatly formatted and would not be presentable in a professional setting."
  },
  {
    "objectID": "project-description.html#draft-report",
    "href": "project-description.html#draft-report",
    "title": "Project description",
    "section": "Draft report",
    "text": "Draft report\nThe purpose of the draft and peer review is to give you an opportunity to get early feedback on your analysis. Therefore, the draft and peer review will focus primarily on the exploratory data analysis, modeling, and initial interpretations.\nWrite the draft in the written-report.qmd file in your project repo. You do not need to submit the draft on Gradescope.\nBelow is a brief description of the sections to focus on in the draft:\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won‚Äôt fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you‚Äôre fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.\n\n\nResults\nIn this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics.\nThis section also includes initial interpretations and conclusions drawn from the model."
  },
  {
    "objectID": "project-description.html#peer-review",
    "href": "project-description.html#peer-review",
    "title": "Project description",
    "section": "Peer review",
    "text": "Peer review\nCritically reviewing others‚Äô work is a crucial part of the scientific process, and STA 210 is no exception. Each lab team will be assigned two other teams‚Äôs projects to review. Each team should push their draft to their GitHub repo by the due date. One lab in the following week will be dedicated to the peer review, and all reviews will be due by the end of that lab session.\nDuring the peer review process, you will be provided read-only access to your partner teams‚Äô GitHub repos. Provide your review in the form of GitHub issues to your partner team‚Äôs GitHub repo using the issue template provided. The peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the partner team‚Äôs report: the research context and motivation, exploratory data analysis, modeling, interpretations, and conclusions.\n\nProcess and questions\nSpend ~30 mins to review the other team‚Äôs project.\n\nOpen the repo of the team you‚Äôre reviewing, read their project draft, and browser around the rest of their repo.\nThen, go to the Issues tab in that repo, click on New issue, and click on Get started for the Peer review issue. Fill out this issue, answering the following questions:\n\nPeer review by: [NAME OF TEAM DOING THE REVIEW]\nNames of team members that participated in this review: [FULL NAMES OF TEAM MEMBERS DOING THE REVIEW]\nDescribe the goal of the project.\nDescribe the data used or collected, if any. If the proposal does not include the use of a specific dataset, comment on whether the project would be strengthened by the inclusion of a dataset.\nDescribe the approaches, tools, and methods that will be used.\nIs there anything that is unclear from the proposal?\nProvide constructive feedback on how the team might be able to improve their project. Make sure your feedback includes at least one comment on the statistical modeling aspect of the project, but do feel free to comment on aspects beyond the modeling.\nWhat aspect of this project are you most interested in and would like to see highlighted in the presentation.\nProvide constructive feedback on any issues with file and/or code organization.\n(Optional) Any further comments or feedback?"
  },
  {
    "objectID": "project-description.html#written-report",
    "href": "project-description.html#written-report",
    "title": "Project description",
    "section": "Written report",
    "text": "Written report\nYour written report must be completed in the written-report.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\nBefore you finalize your write up, make sure the printing of code chunks is off with the option echo = FALSE.\nYou will submit the PDF of your final report on Gradescope.\nThe PDF you submit must match the files in your GitHub repository exactly. The mandatory components of the report are below. You are free to add additional sections as necessary. The report, including visualizations, should be no more than 10 pages long. is no minimum page requirement; however, you should comprehensively address all of the analysis and report.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\nThe written report is worth 40 points, broken down as follows\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction/data\n6 pts\n\n\nMethodology\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion + conclusion\n6 pts\n\n\nOrganization + formatting\n4 pts\n\n\n\nClick here for a PDF of the written report rubric.\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won‚Äôt fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\nGrading criteria\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The explanatory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you‚Äôre fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\n\nGrading criteria\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to select the final model; the approach is clearly described in the report. The model selection process took into account potential interaction effects and addressed any violations in model conditions. The model conditions and diagnostics are thoroughly and accurately assessed for their model. If violations of model conditions are still present, there was a reasonable attempt to address the violations based on the course content.\n\n\n\nResults\nThis is where you will output the final model with any relevant model fit statistics.\nDescribe the key results from the model. The goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\n\nGrading criteria\nThe model fit is clearly assessed, and interesting findings from the model are clearly described. Interpretations of model coefficients are used to support the key findings and conclusions, rather than merely listing the interpretation of every model coefficient. If the primary modeling objective is prediction, the model‚Äôs predictive power is thoroughly assessed.\n\n\n\nDiscussion + Conclusion\nIn this section you‚Äôll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\n\nGrading criteria\nOverall conclusions from analysis are clearly described, and the model results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\nOrganization + formatting\nThis is an assessment of the overall presentation and formatting of the written report.\n\nGrading criteria\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages."
  },
  {
    "objectID": "project-description.html#video-presentation-slides",
    "href": "project-description.html#video-presentation-slides",
    "title": "Project description",
    "section": "Video presentation + slides",
    "text": "Video presentation + slides\n\nSlides\nIn addition to the written report, your team will also create presentation slides and record a video presentation that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and discuss the primary conclusions. These slides should serve as a brief visual addition to your written report and will be graded for content and quality.\nFor submission, convert these slides to a .pdf document, and submit the PDF of the slides on Gradescope.\nThe slide deck should have no more than 6 content slides + 1 title slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 6 slides.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3: Highlights from EDA\nSlide 4: Final model\nSlide 5: Interesting findings from the model\nSlide 6: Conclusions + future work\n\n\n\nVideo presentation\nFor the video presentation, you can speak over your slide deck, similar to the lecture content videos. The video presentation must be no longer than 8 minutes. It is fine if the video is shorter than 8 minutes, but it cannot exceed 8 minutes. You may use can use any platform that works best for your group to record your presentation. Below are a few resources on recording videos:\n\nRecording presentations in Zoom\nApple Quicktime for screen recording\nWindows 10 built-in screen recording functionality\nKap for screen recording\n\nOnce your video is ready, upload the video to Warpwire, then embed the video in an new discussion post on Conversations.\n\nTo upload your video to Warpwire:\n\nClick the Warpwire tab in the course Sakai site.\nClick the ‚Äú+‚Äù and select ‚ÄúUpload files‚Äù.\nLocate the video on your computer and click to upload.\nOnce you‚Äôve uploaded the video to Warpwire, click to share the video and copy the video‚Äôs URL. You will need this when you post the video in the discussion forum.\n\n\n\nTo post the video to the discussion forum\n\nClick the Presentations tab in the course Sakai site.\nClick the Presentations topic.\nClick ‚ÄúStart a new conversation‚Äù.\nMake the title ‚ÄúYour Team Name: Project Title‚Äù. For example, ‚ÄúTeaching Team: Our Awesome Presentation‚Äù.\nClick the Warpwire icon (between the table and shopping cart icons).\nSelect your video, then click ‚ÄúInsert 1 item.‚Äù This will embed your video in the conversation.\nUnder the video, paste the URL to your video.\nYou‚Äôre done!"
  },
  {
    "objectID": "project-description.html#presentation-comments",
    "href": "project-description.html#presentation-comments",
    "title": "Project description",
    "section": "Presentation comments",
    "text": "Presentation comments\nWatch the other group‚Äôs video, then click ‚ÄúReply‚Äù to post a question for the group. You may not post a question that‚Äôs already been asked on the discussion thread. Additionally, the question should be (i) substantive (i.e.¬†it shouldn‚Äôt be ‚ÄúWhy did you use a bar plot instead of a pie chart‚Äù?), (ii) demonstrate your understanding of the content from the course, and (iii) relevant to that group‚Äôs specific presentation, i.e demonstrating that you‚Äôve watched the presentation.\nThis portion of the project will be assessed individually."
  },
  {
    "objectID": "project-description.html#reproducibility-organization",
    "href": "project-description.html#reproducibility-organization",
    "title": "Project description",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization\nAll written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nThe GitHub repo should have the following structure:\n\nREADME: Short project description and data dictionary\nwritten-report.qmd & written-report.pdf: Final written report\n/data: Folder that contains the data set for the final project.\n/previous-work: Folder that contains the topic-ideas and project-proposal files.\n/presentation: Folder with the presentation slides.\n\nIf your presentation slides are online, you can put a link to the slides in a README.md file in the presentation folder.\n\n\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo. The repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable."
  },
  {
    "objectID": "project-description.html#peer-teamwork-evaluation",
    "href": "project-description.html#peer-teamwork-evaluation",
    "title": "Project description",
    "section": "Peer teamwork evaluation",
    "text": "Peer teamwork evaluation\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member by assigning a contribution percentage for each team member. Filling out the survey is a prerequisite for getting credit on the team member evaluation. If you are suggesting that an individual did less than half the expected contribution given your team size (e.g., for a team of four students, if a student contributed less than 12.5% of the total effort), please provide some explanation. If any individual gets an average peer score indicating that this was the case, their grade will be assessed accordingly.\nIf you have concerns with the teamwork and/or contribution from any team members, please email me by the project video deadline. You only need to email me if you have concerns. Otherwise, I will assume everyone on the team equally contributed and will receive full credit for the teamwork portion of the grade."
  },
  {
    "objectID": "project-description.html#overall-grading",
    "href": "project-description.html#overall-grading",
    "title": "Project description",
    "section": "Overall grading",
    "text": "Overall grading\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nTopic ideas\n5 pts\n\n\nProject proposal\n10 pts\n\n\nPeer review\n10 pts\n\n\nWritten report\n40 pts\n\n\nSlides + video presentation\n20 pts\n\n\nReproducibility + organization\n5 pts\n\n\nVideo comments\n5 pts\n\n\nPeer teamwork evaluation\n5 pts\n\n\n\nNote: No late project reports or videos are accepted.\n\nGrading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.\n\n\n\nLate work policy\nThere is no late work accepted on this project. Be sure to turn in your work early to avoid any technological mishaps."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus."
  },
  {
    "objectID": "course-syllabus.html#course-info",
    "href": "course-syllabus.html#course-info",
    "title": "Syllabus",
    "section": "Course info",
    "text": "Course info\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nLectures\nWeekdays\n11:00 am - 12:15 pm\nOld Chemistry 101\n\n\nLabs\nMon and Wed\n2:00 pm - 3:15 pm\nOld Chemistry 101\n\n\nYunran‚Äôs OH\nThursdays\n2:00 pm - 3:00 pm\nOld Chemistry 025\n\n\n\nMondays\n7:00 pm - 8:00 pm\nZoom\n\n\nJoseph‚Äôs OH\nTue and Thur\n8:00 pm - 9:00 pm\nZoom"
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nanalyze real-world data to answer questions about multivariable relationships.\nfit and evaluate linear and logistic regression models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\ncommunicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "course-syllabus.html#community",
    "href": "course-syllabus.html#community",
    "title": "Syllabus",
    "section": "Community",
    "text": "Community\n\nDuke Community Standard\nAs a student in this course, you have agreed to uphold the Duke Community Standard as well as the practices specific to this course.\n\n\n\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students‚Äô learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Duke‚Äôs Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don‚Äôt hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean is an excellent resource.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nWhere to get help\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. Please use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the course forum Conversations. There is a chance another student has already asked a similar question, so please check the other posts in Conversations before adding a new question. If you know the answer to a question posted in the discussion forum, I encourage you to respond!\nEmails should be reserved for questions not appropriate for the public forum. If you email me, please include ‚ÄúSTA 210‚Äù in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Emails sent Friday evening - Sunday may be answered on Monday.\n\nCheck out the Support page for more resources."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nWhile there is no official textbook for the course, we will be assigning readings from the following textbooks.\n\nR for Data Science by Garret Grolemund and Hadley Wickham\nIntroduction to Modern Statistics by Mine √áetinkaya-Rundel and Johanna Hardin\nTidy modeling with R by Max Kuhn and Julia Silge\nBeyond Multiple Linear Regression by Paul Roback and Julie Legler"
  },
  {
    "objectID": "course-syllabus.html#lectures-and-labs",
    "href": "course-syllabus.html#lectures-and-labs",
    "title": "Syllabus",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nThe goal of both the lectures and the labs is for them to be as interactive as possible. My role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. You are expected to attend all lecture and lab sessions and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded. In addition to application exercises will be periodic activities help build a learning community. These will be short, fun activities that will help everyone in the class connect throughout the semester.\nYou are expected to bring a laptop to each class so that you can take part in the in-class exercises. Please make sure your laptop is fully charged before you come to class as the number of outlets in the classroom will not be sufficient to accommodate everyone. More information on loaner laptops can be found here."
  },
  {
    "objectID": "course-syllabus.html#teams",
    "href": "course-syllabus.html#teams",
    "title": "Syllabus",
    "section": "Teams",
    "text": "Teams\nYou will be assigned to a team at the beginning of each week. You are encouraged to sit with your teammates in lecture and you will also work with them in the lab sessions. All team members are expected to contribute equally to the completion of the labs and project and you will be asked to evaluate your team members throughout the semester. Failure to adequately contribute to an assignment will result in a penalty to your mark relative to the team‚Äôs overall mark.\nYou are expected to make use of the provided GitHub repository as their central collaborative platform. Commits to this repository will be used as a metric (one of several) of each team member‚Äôs relative contribution for each project."
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nAssessment for the course is comprised of six components: application exercises, homework assignments, labs, exams, projects, and teamwork.\n\nApplication exercises\nParts of some lectures will be dedicated to working on Application Exercises (AEs). These exercises which give you an opportunity to practice apply the statistical concepts and code introduced in the readings and lectures. These AEs are released on Monday, Wednesday and Friday. They are due within two days of the corresponding lecture period. Specifically, AEs from Monday and Wednesday lectures are due Wednesday and Friday by 11:59 pm ET respectively. AEs from Friday lectures are due Sunday by 11:59 pm ET.\nBecause these AEs are for practice, they will be graded based on completion, i.e., a good-faith effort has been made in attempting all parts. Successful on-time completion of at least 80% of AEs will result in full credit for AEs in the final course grade.\n\n\nLabs\nIn labs, you will apply the concepts discussed in lecture to various data analysis scenarios, with a focus on the computation. Most lab assignments will be completed in teams, and all team members are expected to contribute equally to the completion of each assignment. You are expected to use the team‚Äôs GitHub repository on the course‚Äôs GitHub organization as the central platform for collaboration. Commits to this repository will be used as a metric of each team member‚Äôs relative contribution for each lab, and there will be periodic peer evaluation on the team collaboration. Lab assignments will be completed using Quarto, correspond to an appropriate GitHub repository, and submitted for grading in Gradescope.\n\n\nHomework\nIn homework, you will apply what you‚Äôve learned during lecture and lab to complete data analysis tasks. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. Similar to lab assignments, homework must be typed up using Quarto and GitHub and submitted as a PDF in Gradescope.\nOne homework assignment will be dedicated to a statistics experience. The statistics experience is an opportunity to engage with statistics and data science outside of the classroom through podcasts, books, seminars, data analysis competitions, and other activities. As you complete these experiences, the goal is to consider how the material you‚Äôre learning in the course connects with society more broadly.\n\n\nExams\nThere will be three, take-home, open-note exams. Through these exams you have the opportunity to demonstrate what you‚Äôve learned in the course thus far. The exams will focus on the conceptual understanding of the content, and they may also include small analysis and computational tasks. The content of the exam will be related to the content in the prepare, practice, and perform assignments. More details about the exams will be given during the semester.\n\n\nProject\nThe purpose of the project is to apply what you‚Äôve learned throughout the semester to analyze an interesting, data-driven research question. The project will be completed with your teams, and each team will present their work in class and in writing during the final exam period. More information about the project will be provided during the semester."
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nApplication exercises\n3%\n\n\nHomework\n35% (7% x 5)\n\n\nProject\n15%\n\n\nLab\n14% (2.5% x 6)\n\n\nExam 01\n10%\n\n\nExam 02\n10%\n\n\nExam 03\n10%\n\n\nTeamwork\n2%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n>= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n< 60"
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic integrity\nTL;DR: Don‚Äôt cheat!\nAll students must adhere to the Duke Community Standard (DCS): Duke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nTo uphold the Duke Community Standard:\nStudents affirm their commitment to uphold the values of the Duke University community by signing a pledge that states:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors;\nI will act if the Standard is compromised\n\nRegardless of course delivery format, it is your responsibility to understand and follow Duke policies regarding academic integrity, including doing one‚Äôs own work, following proper citation of sources, and adhering to guidance around group work projects. Ignoring these requirements is a violation of the Duke Community Standard. If you have any questions about how to follow these requirements, please contact Jeanna McCullers (jeanna.mccullers@duke.edu), Director of the Office of Student Conduct.\n\n\nCollaboration policy\nOnly work that is clearly assigned as team work should be completed collaboratively.\n\nThe homework assignments must be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what‚Äôs the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the teaching assistants.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\n\n\n\nPolicy on sharing and reusing code\nI am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course‚Äôs policy is that you may make use of any online resources (e.g.RStudio Community, StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. On individual assignments you may not directly share code with another student in this class, and on team assignments you may not directly share code with another team in this class.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework and lab assignment will be dropped to accommodate such circumstances.\n\nHomework and labs may be submitted up to 1 day late. There will be a 5% deduction for each 6-hour period the assignment is late.\nThere is no late work accepted for application exercises, since these are designed to help you prepare for labs and homework.\nThe late work policy for exams will be provided with the exam instructions.\nThe late work policy for the project will be provided with the project instructions.\n\n\n\nRegrade request policy\nRegrade requests must be submitted on Gradescope within 24 hours of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the final project presentations.\n\n\nAttendance policy\nResponsibility for class attendance rests with individual students. Since regular and punctual class attendance is expected, students must accept the consequences of failure to attend. More details on Trinity attendance policies are available here.\nHowever, there may be many reasons why you cannot be in class on a given day, particularly with possible extra personal and academic stress and health concerns. If you miss a lecture, make sure to review the material before the next class session. Lab time is dedicated to working on your lab assignments and collaborating with your teammates on your project. If you miss a lab session, make sure to communicate with your team about how you can make up your contribution. Given the technologies we use in the course, this is straightforward to do asynchronously. If you know you‚Äôre going to miss a lab session and you‚Äôre feeling well enough to do so, notify your teammates ahead of time. Overall these policies are put in place to ensure communication between team members, respect for each others‚Äô time, and also to give you a safety net in the case of illness or other reasons that keep you away from attending class.\n\n\nAttendance policy related to COVID symptoms, exposure, or infection\nStudent health, safety, and well-being are the university‚Äôs top priorities. Please wear a mask when attending the class. To help ensure your well-being and the well-being of those around you, please do not come to class if you have symptoms related to COVID-19, have had a known exposure to COVID-19, or have tested positive for COVID-19. If any of these situations apply to you, you must follow university guidance related to the ongoing COVID-19 pandemic and current health and safety protocols. If you are experiencing any COVID-19 symptoms, contact student health at 919-681-9355. To keep the university community as safe and healthy as possible, you will be expected to follow these guidelines. Please reach out to me and your academic dean as soon as possible if you need to quarantine or isolate so that we can discuss arrangements for your continued participation in class.\n\n\nInclement weather policy\nIn the event of inclement weather or other connectivity-related events that prohibit class attendance, I will notify you how we will make up missed course content and work. This might entail holding the class on Zoom synchronously or watching a recording of the class."
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nMay 11: Classes begin (Monday meeting schedule).\nMay 12: Regular class meeting schedule begins.\nMay 13: Drop/add for term 1 ends.\nMay 30: Memorial Day holiday. No classes are held.\nJune 8: Last day to withdraw with W.\nJune 15: Last lab.\nJune 17: Classes end.\nJune 20: Juneteenth holiday. No classes/office hour are held.\nJune 21: Reading period.\nJune 22: Final project presentation. Due date for final project report.\n\nClick here for the full Duke academic calendar."
  },
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it‚Äôs been resolved. If there‚Äôs a deadline coming up soon, post on the course forum to let us know that there‚Äôs an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don‚Äôt anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you‚Äôve tried and the errors you see (including verbatim errors and/or screenshots).\nEither way you can also fill out the form here, which will notify our the R TA for the department as well as our undergraduate coordinator. They‚Äôll be able to help diagnose the issue."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 210: Regression Analysis",
    "section": "",
    "text": "Week\nDate\nTopic\nPrepare\nSlides\nAE\nLab\nHW\nExam\nProject\n\n\n\n\n1\nWed, May 11\nWelcome to STA 210\nüìñ\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n\n\nLab 1 - Meet the toolkit\n\nüñ•Ô∏è\n\nüíª\n\n\n\n\n\n\nThur, May 12\nSimple linear regression (SLR)\n\nüñ•Ô∏è\n\n\n\n\n\n\n\n*\nFri, May 13\nSLR: Model fitting in R with tidymodels\n\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n\n\nReleased: HW 5\n\n\n\n\n‚úçÔ∏è\n\n\n\n\n\n\nDue: Lab 1 + AE 0\n\n\n\nüíª üóùÔ∏è\n\n\n\n\n\n*\nSun, May 15\nDue: AE 1\n\n\n\n\n\n\n\n\n\n2\nMon, May 16\nSLR: Prediction + model evaluation\nüìñ\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n\n\nLab: Intro to HW5 + AE2 (continued)\n\n\nüìã\n\n‚úçÔ∏è\n\n\n\n\n\n\nReleased: HW 1\n\n\n\n\n‚úçÔ∏è\n\n\n\n\n\nTue, May 17\nSLR: Simulation-based inference\n\nüñ•Ô∏è\n\n\n\n\n\n\n\n*\nWed, May 18\nSLR: Mathematical models for inference\n\nüñ•Ô∏è\n\n\n\n\n\n\n\n\n\nLab 2 - College scorecard\n\n\n\nüíª\n\n\n\n\n\n\n\nDue: HW 1 + AE 2\n\n\n\n\n‚úçÔ∏è üóùÔ∏è\n\n\n\n\n\nThur, May 19\nSLR: Model diagnostics\n\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n*\nFri, May 20\nExam 1 review\n\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n\n\nReleased: Exam 1\n\n\n\n\n\n‚úÖ\n\n\n\n\n\nDue: Lab 2 + AE 3\n\n\n\nüíª üóùÔ∏è\n\n\n\n\n\n*\nSun, May 22\nDue: AE 4\n\n\n\n\n\n\n\n\n\n3*\nMon, May 23\nMultiple linear regression (MLR)\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\n\n\n\nLab: Project topic ideas\n\n\n\n\n\n\nüìÇ\n\n\n\n\nDue: Exam 1\n\n\n\n\n\n‚úÖüóùÔ∏è\n\n\n\n\nTue, May 24\nMLR: Types of predictors\n\nüñ•Ô∏è\n\n\n\n\n\n\n\n\nWed, May 25\nMLR: Types of predictors\n\nüñ•Ô∏è\n\n\n\n\n\n\n\n\n\nReleased: HW 2\n\n\n\n\n‚úçÔ∏è\n\n\n\n\n\n\nLab 3 - Coffee ratings\n\n\n\nüíª\n\n\n\n\n\n\nThur, May 26\nMLR: Model comparison\n\nüñ•Ô∏è\n\n\n\n\n\n\n\n*\nFri, May 27\nMLR: Feature engineering\n\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n\n\nDue: Lab 3\n\n\n\nüíª üóùÔ∏è\n\n\n\n\n\n*\nSun, May 29\nDue: HW 2 + Project topic ideas + AE 5\n\n\n\n\n‚úçÔ∏è üóùÔ∏è\n\nüìÇ\n\n\n4\nMon, May 30\nMemorial Day holiday\nüìñ\n\n\n\n\n\n\n\n\n\nTue, May 31\nMLR: Cross validation\n\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n*\nWed, June 1\nMLR: Inference\n\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n\n\nLab: Lab 4 + project proposals\n\n\n\nüíª\n\n\nüìÇ\n\n\n\n\nDue: AE 6\n\n\n\n\n\n\n\n\n\n\nThur, June 2\nMLR: Inference conditions + multicollinearity\n\nüñ•Ô∏è\n\n\n\n\n\n\n\n*\nFri, June 3\nExam 2 review\n\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n\n\nReleased: Exam 2\n\n\n\n\n\n‚úÖ\n\n\n\n\n\nDue: Lab 4 + AE 8\n\n\n\nüíª üóùÔ∏è\n\n\n\n\n\n*\nSun, June 5\nDue: AE 7\n\n\n\n\n\n\n\n\n\n5*\nMon, June 6\nLogistic regression (LR)\nüìñ\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n\n\nLab: Work on project proposals\n\n\n\n\n\n\nüìÇ\n\n\n\n\nDue: Exam 2\n\n\n\n\n\n‚úÖ\n\n\n\n\nTue, June 7\nProbabilities, odds, and odds ratios\n\nüñ•Ô∏è\n\n\n\n\n\n\n\n*\nWed, June 8\nLR: Prediction / classification\n\nüñ•Ô∏è\n\n\n\n\n\n\n\n\n\nLab 5: General Social Survey+ project drafts\n\n\n\nüíª\n\n\nüìÇ\n\n\n\n\nDue: Project proposals + AE 9\n\n\n\n\n\n\nüìÇ\n\n\n\n\nReleased: HW 3\n\n\n\n\n‚úçÔ∏è\n\n\n\n\n\nThur, June 9\nLR: Model comparison\n\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n*\nFri, June 10\nLR: Inference + conditions\n\nüñ•Ô∏è\n\n\n\n\n\n\n\n\n\nDue: Lab 5 + AE 10\n\n\n\nüíª üóùÔ∏è\n\n\n\n\n\n*\nSun, June 12\nDue: HW 3\n\n\n\n\n‚úçÔ∏è üóùÔ∏è\n\n\n\n\n6*\nMon, June 13\nMultinomial Logistic Regression (MultiLR)\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\n\n\n\nLab: Work on project drafts\n\n\n\n\n\n\nüìÇ\n\n\n\n\nDue: HW 5\n\n\n\n\n‚úçÔ∏è\n\n\n\n\n\n\nReleased: HW 4\n\n\n\n\n‚úçÔ∏è\n\n\n\n\n\nTue, June 14\nMultiLR: Prediction + inferential models\n\nüñ•Ô∏è\n\n\n\n\n\n\n\n*\nWed, June 15\nMultiLR: Predictive models\n\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n\n\nLab 6: Why Many Americans Don‚Äôt Vote\n\n\n\nüíª\n\n\n\n\n\n\n\nDue: HW 4\n\n\n\n\n‚úçÔ∏è üóùÔ∏è\n\n\n\n\n\nThur, June 16\nMultiLR: Predictive models (cont.)\n\nüñ•Ô∏è\n\n\n\n\n\n\n\n*\nFri, June 17\nExam 3 review\n\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n\n\nWrap-up\n\nüñ•Ô∏è\nüìã\n\n\n\n\n\n\n\n\nDue: Lab 6 +AE 11\n\n\n\nüíª üóùÔ∏è\n\n\n\n\n\n\n\nReleased: Exam 3\n\n\n\n\n\n‚úÖ\n\n\n\n*\nSun, June 19\nDue: project drafts+ AE 12 + AE 13\n\n\n\n\n\n\nüìÇ\n\n\n7*\nMon, June 20\nJuneteenth holiday. No class\nüìñ\n\n\n\n\n\n\n\n\n\n\nDue: Exam 3\n\n\n\n\n\n‚úÖ\n\n\n\n\n\nProject peer review of drafts\n\n\n\n\n\n\nüìÇ\n\n\n\n\nDue: Peer review\n\n\n\n\n\n\nüìÇ\n\n\n\nTue, June 21\nReading day\n\n\n\n\n\n\n\n\n\n*\nWed, June 22\nDue: Video presentation + repo\n\n\n\n\n\n\nüìÇ\n\n\n\n\nDue: Video comments\n\n\n\n\n\n\nüìÇ\n\n\n\n\nDue: Project write-up\n\n\n\n\n\n\nüìÇ\n\n\n\nThur, June 23\nFinal grades"
  },
  {
    "objectID": "slides/lec-5.html#announcements",
    "href": "slides/lec-5.html#announcements",
    "title": "SLR: Simulation based-inference",
    "section": "Announcements",
    "text": "Announcements\n\nHW 1 posted on Monday, due Wednesday, May 18, 11:59pm.\nLab 2 posted on Monday, due Friday, May 20, 11:59pm.\nAE 2 posted on Monday, due Wednesday, May 18, 11:59pm."
  },
  {
    "objectID": "slides/lec-5.html#tips-for-collaboration-on-github",
    "href": "slides/lec-5.html#tips-for-collaboration-on-github",
    "title": "SLR: Simulation based-inference",
    "section": "Tips for collaboration on Github",
    "text": "Tips for collaboration on Github\n\nFrustrating, but part of our goal for the class\nWidely-used for version control and collaboration\nMerge issue"
  },
  {
    "objectID": "slides/lec-5.html#merge-issue-on-github",
    "href": "slides/lec-5.html#merge-issue-on-github",
    "title": "SLR: Simulation based-inference",
    "section": "Merge Issue on Github",
    "text": "Merge Issue on Github\n ## Merge Issue on Github\n\nHaving merge issue: pull - re-edit, merge issue - commit - push\nTo avoid merge conflicts:\n\nWork on different parts of the lab. Do not edit the same part simultaneously.\nCreate your own rmd file using your own name, and only edit your file. Then merge all changes.\nAlways pull before you start any new logical work on your code.\nTalk to your team members when you need to make changes related to their part"
  },
  {
    "objectID": "slides/lec-5.html#computational-setup",
    "href": "slides/lec-5.html#computational-setup",
    "title": "SLR: Simulation based-inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\nlibrary(knitr)       # for pretty tables\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/lec-5.html#terminology",
    "href": "slides/lec-5.html#terminology",
    "title": "SLR: Simulation based-inference",
    "section": "Terminology",
    "text": "Terminology\n\nOutcome: y\nPredictor: x\nObserved y, \\(y\\): truth\nPredicted y, \\(\\hat{y}\\): fitted, estimated\nResidual: \\(y-\\hat{y}\\)"
  },
  {
    "objectID": "slides/lec-5.html#model-evaluation",
    "href": "slides/lec-5.html#model-evaluation",
    "title": "SLR: Simulation based-inference",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nEvaluating models‚Äô performance for fitting\n\nR-squared, RMSE\n\nInterested in performance for prediction for a new observation\nEvaluating predictive performance: Splitting the data\nQuantifying variability of of estimates: Bootstrap the data"
  },
  {
    "objectID": "slides/lec-5.html#model-evaluation-1",
    "href": "slides/lec-5.html#model-evaluation-1",
    "title": "SLR: Simulation based-inference",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nEvaluating predictive performance: Split the data into testing and training sets, build models using only the training set, and evaluate their performance on the testing set, and repeat to see how your model holds up to ‚Äúnew‚Äù data\nQuantifying variability of of estimates: Bootstrap the data, fit a model, obtain coefficient estimates and/or measures of strength of fit, and repeat many times to see how your model holds up to ‚Äúnew‚Äù data"
  },
  {
    "objectID": "slides/lec-5.html#uninsurance-vs.-hs-graduation-in-nc",
    "href": "slides/lec-5.html#uninsurance-vs.-hs-graduation-in-nc",
    "title": "SLR: Simulation based-inference",
    "section": "Uninsurance vs.¬†HS graduation in NC",
    "text": "Uninsurance vs.¬†HS graduation in NC"
  },
  {
    "objectID": "slides/lec-5.html#uninsurance-vs.-hs-graduation-in-ny",
    "href": "slides/lec-5.html#uninsurance-vs.-hs-graduation-in-ny",
    "title": "SLR: Simulation based-inference",
    "section": "Uninsurance vs.¬†HS graduation in NY",
    "text": "Uninsurance vs.¬†HS graduation in NY\n\n\nCode\ncounty_2019_ny <- county_2019 %>%\n  as_tibble() %>%\n  filter(state == \"New York\") %>%\n  select(name, hs_grad, uninsured)\n\nggplot(county_2019_ny,\n       aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"New York counties, 2015 - 2019\"\n  ) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"pink\")"
  },
  {
    "objectID": "slides/lec-5.html#data-splitting",
    "href": "slides/lec-5.html#data-splitting",
    "title": "SLR: Simulation based-inference",
    "section": "Data splitting",
    "text": "Data splitting"
  },
  {
    "objectID": "slides/lec-5.html#bootstrapping",
    "href": "slides/lec-5.html#bootstrapping",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "slides/lec-5.html#comparing-ny-and-nc",
    "href": "slides/lec-5.html#comparing-ny-and-nc",
    "title": "SLR: Simulation based-inference",
    "section": "Comparing NY and NC",
    "text": "Comparing NY and NC\n\nWhy are the fits from the NY models more variable than those from the NC models?\n\n\nSample size\nOutliers or influential points"
  },
  {
    "objectID": "slides/lec-5.html#data-sale-prices-of-houses-in-duke-forest",
    "href": "slides/lec-5.html#data-sale-prices-of-houses-in-duke-forest",
    "title": "SLR: Simulation based-inference",
    "section": "Data: Sale prices of houses in Duke Forest",
    "text": "Data: Sale prices of houses in Duke Forest\n\n\n\nData on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\nScraped from Zillow\nSource: openintro::duke_forest"
  },
  {
    "objectID": "slides/lec-5.html#exploratory-analysis",
    "href": "slides/lec-5.html#exploratory-analysis",
    "title": "SLR: Simulation based-inference",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis"
  },
  {
    "objectID": "slides/lec-5.html#modeling",
    "href": "slides/lec-5.html#modeling",
    "title": "SLR: Simulation based-inference",
    "section": "Modeling",
    "text": "Modeling\n\ndf_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %>%\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\n\n\nIntercept: Duke Forest houses that are 0 square feet are expected to sell, on average, for $116,652.\nSlope: For each additional square foot, the model predicts (we expect) the sale price of Duke Forest houses to be higher, on average, by $159."
  },
  {
    "objectID": "slides/lec-5.html#sample-to-population",
    "href": "slides/lec-5.html#sample-to-population",
    "title": "SLR: Simulation based-inference",
    "section": "Sample to population",
    "text": "Sample to population\n\nFor each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159.\n\n\nThis estimate is valid for the single sample of 98 houses.\nBut what if we‚Äôre not interested quantifying the relationship between the size and price of a house in this single sample?\nWhat if we want to say something about the relationship between these variables for all houses in Duke Forest?"
  },
  {
    "objectID": "slides/lec-5.html#statistical-inference-1",
    "href": "slides/lec-5.html#statistical-inference-1",
    "title": "SLR: Simulation based-inference",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nStatistical inference allows provide methods and tools for us to use the single sample we have observed to make valid statements (inferences) about the population it comes from\nFor our inferences to be valid, the sample should be random and representative of the population we‚Äôre interested in"
  },
  {
    "objectID": "slides/lec-5.html#inference-for-simple-linear-regression",
    "href": "slides/lec-5.html#inference-for-simple-linear-regression",
    "title": "SLR: Simulation based-inference",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the slope, \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/lec-5.html#confidence-interval",
    "href": "slides/lec-5.html#confidence-interval",
    "title": "SLR: Simulation based-inference",
    "section": "Confidence interval",
    "text": "Confidence interval\n\nA plausible range of values for a population parameter is called a confidence interval\nUsing only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net\n\nWe can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\nSimilarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter"
  },
  {
    "objectID": "slides/lec-5.html#confidence-interval-for-the-slope-1",
    "href": "slides/lec-5.html#confidence-interval-for-the-slope-1",
    "title": "SLR: Simulation based-inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\nA confidence interval will allow us to make a statement like ‚ÄúFor each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, give or take X dollars.‚Äù\n\nShould X be $10? $100? $1000? (width of interval)\nIf we were to take another sample of 98 would we expect the slope calculated based on that sample to be exactly $159? Off by $10? $100? $1000?\nThe answer depends on how variable (from one sample to another sample) the sample statistic (the slope) is\nWe need a way to quantify the variability of the sample statistic"
  },
  {
    "objectID": "slides/lec-5.html#quantify-the-variability-of-the-slope",
    "href": "slides/lec-5.html#quantify-the-variability-of-the-slope",
    "title": "SLR: Simulation based-inference",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor estimation\n\nTwo approaches:\n\nVia simulation (what we‚Äôll do today)\nVia mathematical models (what we‚Äôll do in the next class)\n\nBootstrapping to quantify the variability of the slope for the purpose of estimation:\n\nBootstrap new samples from the original sample\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-1",
    "href": "slides/lec-5.html#bootstrap-sample-1",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 1",
    "text": "Bootstrap sample 1"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-2",
    "href": "slides/lec-5.html#bootstrap-sample-2",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 2",
    "text": "Bootstrap sample 2"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-3",
    "href": "slides/lec-5.html#bootstrap-sample-3",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 3",
    "text": "Bootstrap sample 3"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-4",
    "href": "slides/lec-5.html#bootstrap-sample-4",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 4",
    "text": "Bootstrap sample 4"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-5",
    "href": "slides/lec-5.html#bootstrap-sample-5",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 5",
    "text": "Bootstrap sample 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nso on and so forth‚Ä¶"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-samples-1---5",
    "href": "slides/lec-5.html#bootstrap-samples-1---5",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap samples 1 - 5",
    "text": "Bootstrap samples 1 - 5"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-samples-1---100",
    "href": "slides/lec-5.html#bootstrap-samples-1---100",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap samples 1 - 100",
    "text": "Bootstrap samples 1 - 100"
  },
  {
    "objectID": "slides/lec-5.html#slopes-of-bootstrap-samples",
    "href": "slides/lec-5.html#slopes-of-bootstrap-samples",
    "title": "SLR: Simulation based-inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, give or take ___ dollars."
  },
  {
    "objectID": "slides/lec-5.html#slopes-of-bootstrap-samples-1",
    "href": "slides/lec-5.html#slopes-of-bootstrap-samples-1",
    "title": "SLR: Simulation based-inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, give or take ___ dollars."
  },
  {
    "objectID": "slides/lec-5.html#confidence-level",
    "href": "slides/lec-5.html#confidence-level",
    "title": "SLR: Simulation based-inference",
    "section": "Confidence level",
    "text": "Confidence level\n\nHow confident are you that the true slope is between $0 and $250? How about $150 and $170? How about $90 and $210?"
  },
  {
    "objectID": "slides/lec-5.html#confidence-interval-1",
    "href": "slides/lec-5.html#confidence-interval-1",
    "title": "SLR: Simulation based-inference",
    "section": "95% confidence interval",
    "text": "95% confidence interval\n\n\nA 95% confidence interval is bounded by the middle 95% of the bootstrap distribution\nWe are 95% confident that For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $90.43 to $205.77."
  },
  {
    "objectID": "slides/lec-5.html#computing-the-ci-for-the-slope-i",
    "href": "slides/lec-5.html#computing-the-ci-for-the-slope-i",
    "title": "SLR: Simulation based-inference",
    "section": "Computing the CI for the slope I",
    "text": "Computing the CI for the slope I\nCalculate the observed slope:\n\nobserved_fit <- duke_forest %>%\n  specify(price ~ area) %>%\n  fit()\n\nobserved_fit\n\n# A tibble: 2 √ó 2\n  term      estimate\n  <chr>        <dbl>\n1 intercept  116652.\n2 area          159."
  },
  {
    "objectID": "slides/lec-5.html#computing-the-ci-for-the-slope-ii",
    "href": "slides/lec-5.html#computing-the-ci-for-the-slope-ii",
    "title": "SLR: Simulation based-inference",
    "section": "Computing the CI for the slope II",
    "text": "Computing the CI for the slope II\nTake 100 bootstrap samples and fit models to each one:\n\n# #| code-line-numbers: \"1,5,6\"\n\nset.seed(1120)\n\nboot_fits <- duke_forest %>%\n  specify(price ~ area) %>%\n  generate(reps = 100, type = \"bootstrap\") %>%\n  fit()\n\nboot_fits\n\n# A tibble: 200 √ó 3\n# Groups:   replicate [100]\n   replicate term      estimate\n       <int> <chr>        <dbl>\n 1         1 intercept   47819.\n 2         1 area          191.\n 3         2 intercept  144645.\n 4         2 area          134.\n 5         3 intercept  114008.\n 6         3 area          161.\n 7         4 intercept  100639.\n 8         4 area          166.\n 9         5 intercept  215264.\n10         5 area          125.\n# ‚Ä¶ with 190 more rows"
  },
  {
    "objectID": "slides/lec-5.html#computing-the-ci-for-the-slope-iii",
    "href": "slides/lec-5.html#computing-the-ci-for-the-slope-iii",
    "title": "SLR: Simulation based-inference",
    "section": "Computing the CI for the slope III",
    "text": "Computing the CI for the slope III\nPercentile method: Compute the 95% CI as the middle 95% of the bootstrap distribution:\n\n# #| code-line-numbers: \"5\"\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          92.1     223.\n2 intercept -36765.   296528."
  },
  {
    "objectID": "slides/lec-5.html#computing-the-ci-for-the-slope-iv",
    "href": "slides/lec-5.html#computing-the-ci-for-the-slope-iv",
    "title": "SLR: Simulation based-inference",
    "section": "Computing the CI for the slope IV",
    "text": "Computing the CI for the slope IV\nStandard error method: Alternatively, compute the 95% CI as the point estimate \\(\\pm\\) ~2 standard deviations of the bootstrap distribution:\n\n# #| code-line-numbers: \"5\"\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"se\"\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          90.8     228.\n2 intercept -56788.   290093."
  },
  {
    "objectID": "slides/lec-5.html#precision-vs.-accuracy",
    "href": "slides/lec-5.html#precision-vs.-accuracy",
    "title": "SLR: Simulation based-inference",
    "section": "Precision vs.¬†accuracy",
    "text": "Precision vs.¬†accuracy\n\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?"
  },
  {
    "objectID": "slides/lec-5.html#precision-vs.-accuracy-1",
    "href": "slides/lec-5.html#precision-vs.-accuracy-1",
    "title": "SLR: Simulation based-inference",
    "section": "Precision vs.¬†accuracy",
    "text": "Precision vs.¬†accuracy\n\nHow can we get best of both worlds ‚Äì high precision and high accuracy?"
  },
  {
    "objectID": "slides/lec-5.html#changing-confidence-level",
    "href": "slides/lec-5.html#changing-confidence-level",
    "title": "SLR: Simulation based-inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\nHow would you modify the following code to calculate a 90% confidence interval? How would you modify it for a 99% confidence interval?\n\n\n# #| code-line-numbers: \"|4\"\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          92.1     223.\n2 intercept -36765.   296528."
  },
  {
    "objectID": "slides/lec-5.html#changing-confidence-level-1",
    "href": "slides/lec-5.html#changing-confidence-level-1",
    "title": "SLR: Simulation based-inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\n## confidence level: 90%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.90, type = \"percentile\"\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          104.     212.\n2 intercept  -24380.  256730.\n\n## confidence level: 99%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.99, type = \"percentile\"\n)\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          56.3     226.\n2 intercept -61950.   370395."
  },
  {
    "objectID": "slides/lec-5.html#recap",
    "href": "slides/lec-5.html#recap",
    "title": "SLR: Simulation based-inference",
    "section": "Recap",
    "text": "Recap\n\nPopulation: Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = \\(N\\))\nSample: Subset of the population, ideally random and representative (sample size = \\(n\\))\nSample statistic \\(\\ne\\) population parameter, but if the sample is good, it can be a good estimate\nStatistical inference: Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process\nWe report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population\nSince we can‚Äôt continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability"
  },
  {
    "objectID": "slides/lec-5.html#sampling-is-natural",
    "href": "slides/lec-5.html#sampling-is-natural",
    "title": "SLR: Simulation based-inference",
    "section": "Sampling is natural",
    "text": "Sampling is natural\n\n\nWhen you taste a spoonful of soup and decide the spoonful you tasted isn‚Äôt salty enough, that‚Äôs exploratory analysis\nIf you generalize and conclude that your entire soup needs salt, that‚Äôs an inference\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population)"
  },
  {
    "objectID": "slides/lec-5.html#statistical-significance",
    "href": "slides/lec-5.html#statistical-significance",
    "title": "SLR: Simulation based-inference",
    "section": "Statistical significance",
    "text": "Statistical significance\n\n\nDo the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?"
  },
  {
    "objectID": "slides/lec-5.html#hypotheses",
    "href": "slides/lec-5.html#hypotheses",
    "title": "SLR: Simulation based-inference",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nWe want to answer the question ‚ÄúDo the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?‚Äù\nNull hypothesis - \\(H_0: \\beta_1 = 0\\), there is no linear relationship between area and price\nAlternative hypothesis - \\(H_A: \\beta_1 \\ne 0\\), there is a linear relationship between area and price"
  },
  {
    "objectID": "slides/lec-5.html#hypothesis-testing-as-a-court-trial",
    "href": "slides/lec-5.html#hypothesis-testing-as-a-court-trial",
    "title": "SLR: Simulation based-inference",
    "section": "Hypothesis testing as a court trial",
    "text": "Hypothesis testing as a court trial\n\nNull hypothesis, \\(H_0\\): Defendant is innocent\nAlternative hypothesis, \\(H_A\\): Defendant is guilty\nPresent the evidence: Collect data\nJudge the evidence: ‚ÄúCould these data plausibly have happened by chance if the null hypothesis were true?‚Äù\n\nYes: Fail to reject \\(H_0\\)\nNo: Reject \\(H_0\\)"
  },
  {
    "objectID": "slides/lec-5.html#hypothesis-testing-framework",
    "href": "slides/lec-5.html#hypothesis-testing-framework",
    "title": "SLR: Simulation based-inference",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\nStart with a null hypothesis, \\(H_0\\) that represents the status quo\nSet an alternative hypothesis, \\(H_A\\) that represents the research question, i.e.¬†what we‚Äôre testing for\nConduct a hypothesis test under the assumption that the null hypothesis is true and calculate a p-value (probability of observed or more extreme outcome given that the null hypothesis is true)\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-6.html#computational-setup",
    "href": "slides/lec-6.html#computational-setup",
    "title": "SLR: Mathematical models for inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-6.html#data-duke-forest-houses",
    "href": "slides/lec-6.html#data-duke-forest-houses",
    "title": "SLR: Mathematical models for inference",
    "section": "Data: Duke Forest houses",
    "text": "Data: Duke Forest houses"
  },
  {
    "objectID": "slides/lec-6.html#the-regression-model",
    "href": "slides/lec-6.html#the-regression-model",
    "title": "SLR: Mathematical models for inference",
    "section": "The regression model",
    "text": "The regression model\n\ndf_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %>%\n  kable(digits = 2)\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    116652.33 \n    53302.46 \n    2.19 \n    0.03 \n  \n  \n    area \n    159.48 \n    18.17 \n    8.78 \n    0.00 \n  \n\n\n\n\n\n\n\n\n\n\nIntercept: Duke Forest houses that are 0 square feet are expected to sell, on average, for $116,652.\nSlope: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159."
  },
  {
    "objectID": "slides/lec-6.html#inference-for-simple-linear-regression",
    "href": "slides/lec-6.html#inference-for-simple-linear-regression",
    "title": "SLR: Mathematical models for inference",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the interval, \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/lec-6.html#confidence-interval-via-bootstrapping",
    "href": "slides/lec-6.html#confidence-interval-via-bootstrapping",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval via bootstrapping",
    "text": "Confidence interval via bootstrapping\n\nBootstrap new samples from the original sample\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/lec-6.html#bootstrapping-pipeline-i",
    "href": "slides/lec-6.html#bootstrapping-pipeline-i",
    "title": "SLR: Mathematical models for inference",
    "section": "Bootstrapping pipeline I",
    "text": "Bootstrapping pipeline I\n\n# #| code-line-numbers: \"|1|3|4\"\n#| \nset.seed(119)\n\nduke_forest %>%\n  specify(price ~ area)\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98 √ó 2\n     price  area\n     <dbl> <dbl>\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# ‚Ä¶ with 88 more rows"
  },
  {
    "objectID": "slides/lec-6.html#bootstrapping-pipeline-ii",
    "href": "slides/lec-6.html#bootstrapping-pipeline-ii",
    "title": "SLR: Mathematical models for inference",
    "section": "Bootstrapping pipeline II",
    "text": "Bootstrapping pipeline II\n\n# #| code-line-numbers: \"|5\"\n\nset.seed(119)\n\nduke_forest %>%\n  specify(price ~ area) %>%\n  generate(reps = 1000, type = \"bootstrap\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98,000 √ó 3\n# Groups:   replicate [1,000]\n   replicate   price  area\n       <int>   <dbl> <dbl>\n 1         1  535000  2334\n 2         1  520000  2637\n 3         1  540000  2165\n 4         1  155000  1620\n 5         1  567000  3931\n 6         1  420000  1745\n 7         1  400000  4769\n 8         1  579000  2926\n 9         1  615000  2203\n10         1 1030000  4475\n# ‚Ä¶ with 97,990 more rows"
  },
  {
    "objectID": "slides/lec-6.html#bootstrapping-pipeline-iii",
    "href": "slides/lec-6.html#bootstrapping-pipeline-iii",
    "title": "SLR: Mathematical models for inference",
    "section": "Bootstrapping pipeline III",
    "text": "Bootstrapping pipeline III\n\n# #| code-line-numbers: \"|6\"\n\nset.seed(119)\n\nduke_forest %>%\n  specify(price ~ area) %>%\n  generate(reps = 1000, type = \"bootstrap\") %>%\n  fit()\n\n# A tibble: 2,000 √ó 3\n# Groups:   replicate [1,000]\n   replicate term      estimate\n       <int> <chr>        <dbl>\n 1         1 intercept  200401.\n 2         1 area          122.\n 3         2 intercept  120000.\n 4         2 area          156.\n 5         3 intercept  190879.\n 6         3 area          126.\n 7         4 intercept  206842.\n 8         4 area          127.\n 9         5 intercept  211231.\n10         5 area          124.\n# ‚Ä¶ with 1,990 more rows"
  },
  {
    "objectID": "slides/lec-6.html#bootstrapping-pipeline-iv",
    "href": "slides/lec-6.html#bootstrapping-pipeline-iv",
    "title": "SLR: Mathematical models for inference",
    "section": "Bootstrapping pipeline IV",
    "text": "Bootstrapping pipeline IV\n\n# #| code-line-numbers: \"|3\"\n\nset.seed(119)\n\nboot_dist <- duke_forest %>%\n  specify(price ~ area) %>%\n  generate(reps = 1000, type = \"bootstrap\") %>%\n  fit()"
  },
  {
    "objectID": "slides/lec-6.html#visualize-the-bootstrap-distribution",
    "href": "slides/lec-6.html#visualize-the-bootstrap-distribution",
    "title": "SLR: Mathematical models for inference",
    "section": "Visualize the bootstrap distribution",
    "text": "Visualize the bootstrap distribution\n\n# #| code-line-numbers: \"|2\"\n\nboot_dist %>%\n  filter(term == \"area\") %>%\n  ggplot(aes(x = estimate)) +\n  geom_histogram(binwidth = 10)"
  },
  {
    "objectID": "slides/lec-6.html#compute-the-ci",
    "href": "slides/lec-6.html#compute-the-ci",
    "title": "SLR: Mathematical models for inference",
    "section": "Compute the CI",
    "text": "Compute the CI\nTwo methods:\n\n\nPercentile method\nStandard error method"
  },
  {
    "objectID": "slides/lec-6.html#but-first",
    "href": "slides/lec-6.html#but-first",
    "title": "SLR: Mathematical models for inference",
    "section": "But first‚Ä¶",
    "text": "But first‚Ä¶\n\nobs_fit <- duke_forest %>%\n  specify(price ~ area) %>%\n  fit()\n\nobs_fit\n\n# A tibble: 2 √ó 2\n  term      estimate\n  <chr>        <dbl>\n1 intercept  116652.\n2 area          159."
  },
  {
    "objectID": "slides/lec-6.html#percentile-method",
    "href": "slides/lec-6.html#percentile-method",
    "title": "SLR: Mathematical models for inference",
    "section": "Percentile method",
    "text": "Percentile method\nThough do not need to use point estimate for percentile method, do include it or will return error.\n\n# #| code-line-numbers: \"|4\"\n\nboot_dist %>%\n  get_confidence_interval(\n    level = 0.95,\n    type = \"percentile\",\n    point_estimate = obs_fit\n  )\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          91.0     215.\n2 intercept -22046.   289004."
  },
  {
    "objectID": "slides/lec-6.html#standard-error-method",
    "href": "slides/lec-6.html#standard-error-method",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard error method",
    "text": "Standard error method\n\nUse bootstrap to estimate se\nNormal assumption: \\(\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\\)\n\n\n# #| code-line-numbers: \"|4\"\n\nboot_dist %>%\n  get_confidence_interval(\n    level = 0.95,\n    type = \"se\",\n    point_estimate = obs_fit\n  )\n\n# A tibble: 2 √ó 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          96.1     223.\n2 intercept -39805.   273109."
  },
  {
    "objectID": "slides/lec-6.html#research-question-and-hypotheses",
    "href": "slides/lec-6.html#research-question-and-hypotheses",
    "title": "SLR: Mathematical models for inference",
    "section": "Research question and hypotheses",
    "text": "Research question and hypotheses\n\n‚ÄúDo the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?‚Äù\nNull hypothesis - \\(H_0: \\beta_1 = 0\\), there is no linear relationship between area and price\nAlternative hypothesis - \\(H_A: \\beta_1 \\ne 0\\), there is a linear relationship between area and price"
  },
  {
    "objectID": "slides/lec-6.html#hypothesis-testing-framework",
    "href": "slides/lec-6.html#hypothesis-testing-framework",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\nStart with a null hypothesis, \\(H_0\\) that represents the status quo\nSet an alternative hypothesis, \\(H_A\\) that represents the research question, i.e.¬†what we‚Äôre testing for\nConduct a hypothesis test under the assumption that the null hypothesis is true and calculate a p-value (probability of observed or more extreme outcome given that the null hypothesis is true)\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative"
  },
  {
    "objectID": "slides/lec-6.html#hypothesis-testing",
    "href": "slides/lec-6.html#hypothesis-testing",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\n\nTwo approaches:\n\nVia simulation (CI: bootstrap; HT: permutation)\nVia mathematical models\n\nRandomizing to quantify the variability of the slope for the purpose of testing, under the assumption that the null hypothesis is true:\n\nSimulate new samples from the original sample via permutation\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the permuted slopes to conduct a hypothesis test"
  },
  {
    "objectID": "slides/lec-6.html#permutation-described",
    "href": "slides/lec-6.html#permutation-described",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation, described",
    "text": "Permutation, described\n\n\n\nSet the null hypothesis to be true, and measure the natural variability in the data due to sampling but not due to variables being correlated by permuting permute one variable to eliminate any existing relationship between the variables\nEach price value is randomly assigned to area of a given house, i.e.¬†area and price are no longer matched for a given house\n\n\n\n\n# A tibble: 98 √ó 3\n   price_Observed price_Permuted  area\n            <dbl>          <dbl> <dbl>\n 1        1520000         342500  6040\n 2        1030000         750000  4475\n 3         420000         645000  1745\n 4         680000         697500  2091\n 5         428500         428500  1772\n 6         456000         481000  1950\n 7        1270000         610000  3909\n 8         557450         680000  2841\n 9         697500         485000  3924\n10         650000         105000  2173\n# ‚Ä¶ with 88 more rows"
  },
  {
    "objectID": "slides/lec-6.html#permutation-visualized",
    "href": "slides/lec-6.html#permutation-visualized",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation, visualized",
    "text": "Permutation, visualized\n\n\n\nEach of the observed values for area (and for price) exist in both the observed data plot as well as the permuted price plot\nThe permutation removes the linear relationship between area and price"
  },
  {
    "objectID": "slides/lec-6.html#permutation-repeated",
    "href": "slides/lec-6.html#permutation-repeated",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation, repeated",
    "text": "Permutation, repeated\nRepeated permutations allow for quantifying the variability in the slope under the condition that there is no linear relationship (i.e., that the null hypothesis is true)"
  },
  {
    "objectID": "slides/lec-6.html#concluding-the-hypothesis-test",
    "href": "slides/lec-6.html#concluding-the-hypothesis-test",
    "title": "SLR: Mathematical models for inference",
    "section": "Concluding the hypothesis test",
    "text": "Concluding the hypothesis test\n\nIs the observed slope of \\(\\hat{\\beta_1} = 159\\) (or an even more extreme slope) a likely outcome under the null hypothesis that \\(\\beta = 0\\)? What does this mean for our original question: ‚ÄúDo the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?‚Äù"
  },
  {
    "objectID": "slides/lec-6.html#permutation-pipeline-i",
    "href": "slides/lec-6.html#permutation-pipeline-i",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation pipeline I",
    "text": "Permutation pipeline I\n\n# #| code-line-numbers: \"|1|3|4\"\n#| \nset.seed(1125)\n\nduke_forest %>%\n  specify(price ~ area)\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98 √ó 2\n     price  area\n     <dbl> <dbl>\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# ‚Ä¶ with 88 more rows"
  },
  {
    "objectID": "slides/lec-6.html#permutation-pipeline-ii",
    "href": "slides/lec-6.html#permutation-pipeline-ii",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation pipeline II",
    "text": "Permutation pipeline II\n\n# #| code-line-numbers: \"|5\"\n\nset.seed(1125)\n\nduke_forest %>%\n  specify(price ~ area) %>%\n  hypothesize(null = \"independence\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\nNull Hypothesis: independence\n# A tibble: 98 √ó 2\n     price  area\n     <dbl> <dbl>\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# ‚Ä¶ with 88 more rows"
  },
  {
    "objectID": "slides/lec-6.html#permutation-pipeline-iii",
    "href": "slides/lec-6.html#permutation-pipeline-iii",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation pipeline III",
    "text": "Permutation pipeline III\n\n# #| code-line-numbers: \"|6\"\n\nset.seed(1125)\n\nduke_forest %>%\n  specify(price ~ area) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\nNull Hypothesis: independence\n# A tibble: 98,000 √ó 3\n# Groups:   replicate [1,000]\n     price  area replicate\n     <dbl> <dbl>     <int>\n 1  465000  6040         1\n 2  481000  4475         1\n 3 1020000  1745         1\n 4  520000  2091         1\n 5  592000  1772         1\n 6  650000  1950         1\n 7  473000  3909         1\n 8  705000  2841         1\n 9  785000  3924         1\n10  671500  2173         1\n# ‚Ä¶ with 97,990 more rows"
  },
  {
    "objectID": "slides/lec-6.html#permutation-pipeline-iv",
    "href": "slides/lec-6.html#permutation-pipeline-iv",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation pipeline IV",
    "text": "Permutation pipeline IV\n\n# #| code-line-numbers: \"|7\"\n\nset.seed(1125)\n\nduke_forest %>%\n  specify(price ~ area) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\") %>%\n  fit()\n\n# A tibble: 2,000 √ó 3\n# Groups:   replicate [1,000]\n   replicate term       estimate\n       <int> <chr>         <dbl>\n 1         1 intercept 553355.  \n 2         1 area           2.35\n 3         2 intercept 635824.  \n 4         2 area         -27.3 \n 5         3 intercept 536072.  \n 6         3 area           8.57\n 7         4 intercept 598649.  \n 8         4 area         -13.9 \n 9         5 intercept 556202.  \n10         5 area           1.33\n# ‚Ä¶ with 1,990 more rows"
  },
  {
    "objectID": "slides/lec-6.html#permutation-pipeline-v",
    "href": "slides/lec-6.html#permutation-pipeline-v",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation pipeline V",
    "text": "Permutation pipeline V\n\n# #| code-line-numbers: \"|3\"\n\nset.seed(1125)\n\nnull_dist <- duke_forest %>%\n  specify(price ~ area) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\") %>%\n  fit()"
  },
  {
    "objectID": "slides/lec-6.html#visualize-the-null-distribution",
    "href": "slides/lec-6.html#visualize-the-null-distribution",
    "title": "SLR: Mathematical models for inference",
    "section": "Visualize the null distribution",
    "text": "Visualize the null distribution\n\n# #| code-line-numbers: \"|2\"\n\nnull_dist %>%\n  filter(term == \"area\") %>%\n  ggplot(aes(x = estimate)) +\n  geom_histogram(binwidth = 10, color = \"white\")"
  },
  {
    "objectID": "slides/lec-6.html#reason-around-the-p-value",
    "href": "slides/lec-6.html#reason-around-the-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Reason around the p-value",
    "text": "Reason around the p-value\n\nIn a world where the there is no relationship between the area of a Duke Forest house and in its price (\\(\\beta_1 = 0\\)), what is the probability that we observe a sample of 98 houses where the slope fo the model predicting price from area is 159 or even more extreme?"
  },
  {
    "objectID": "slides/lec-6.html#compute-the-p-value",
    "href": "slides/lec-6.html#compute-the-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Compute the p-value",
    "text": "Compute the p-value\n\nWhat does this warning mean?\n\n\nget_p_value(\n  null_dist,\n  obs_stat = obs_fit,\n  direction = \"two-sided\"\n)\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step. See\n`?get_p_value()` for more information.\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step. See\n`?get_p_value()` for more information.\n\n\n# A tibble: 2 √ó 2\n  term      p_value\n  <chr>       <dbl>\n1 area            0\n2 intercept       0"
  },
  {
    "objectID": "slides/lec-6.html#the-regression-model-revisited",
    "href": "slides/lec-6.html#the-regression-model-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\ndf_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %>%\n  kable(digits = 3)\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    116652.325 \n    53302.463 \n    2.188 \n    0.031 \n  \n  \n    area \n    159.483 \n    18.171 \n    8.777 \n    0.000"
  },
  {
    "objectID": "slides/lec-6.html#ht-and-ci-recapped",
    "href": "slides/lec-6.html#ht-and-ci-recapped",
    "title": "SLR: Mathematical models for inference",
    "section": "HT and CI, recapped",
    "text": "HT and CI, recapped\n\nHypothesis test:\n\n\nDo the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?\nNull hypothesis - \\(H_0: \\beta_1 = 0\\), there is no linear relationship between area and price.\nAlternative hypothesis - \\(H_A: \\beta_1 \\ne 0\\), there is a linear relationship between area and price.\n\n\nConfidence interval: Provide a plausible range of values for \\(\\beta_1\\) at a given confidence level."
  },
  {
    "objectID": "slides/lec-6.html#ht-and-ci-revisited",
    "href": "slides/lec-6.html#ht-and-ci-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "HT and CI, revisited",
    "text": "HT and CI, revisited\n\nEarlier we computed a CI and conducted a HT via simulation:\n\nCI: Bootstrap the observed sample to simulate the distribution of the slope\nHT: Permute the observed sample to simulate the distribution of the slope under the assumption that the null hypothesis is true\n\nNow we‚Äôll do these based on theoretical results, i.e., by using the Central Limit Theorem"
  },
  {
    "objectID": "slides/lec-6.html#mathematical-representation-of-the-model",
    "href": "slides/lec-6.html#mathematical-representation-of-the-model",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation of the model",
    "text": "Mathematical representation of the model\n\\[\n\\begin{aligned}\nY &= Model + Error \\\\\n&= f(X) + \\epsilon \\\\\n&= \\mu_{Y|X} + \\epsilon \\\\\n&= \\beta_0 + \\beta_1 X + \\epsilon\n\\end{aligned}\n\\]\nwhere the errors are independent and normally distributed:\n\nindependent: Knowing the error term for one observation doesn‚Äôt tell you anything about the error term for another observation\nnormally distributed: \\(\\epsilon \\sim N(0, \\sigma_\\epsilon^2)\\)"
  },
  {
    "objectID": "slides/lec-6.html#mathematical-representation-visualized",
    "href": "slides/lec-6.html#mathematical-representation-visualized",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean: \\(\\beta_0 + \\beta_1 X\\), the predicted value based on the regression model\nVariance: \\(\\sigma_\\epsilon^2\\), constant across the range of \\(X\\)\n\nHow do we estimate \\(\\sigma_\\epsilon^2\\)?"
  },
  {
    "objectID": "slides/lec-6.html#regression-standard-error",
    "href": "slides/lec-6.html#regression-standard-error",
    "title": "SLR: Mathematical models for inference",
    "section": "Regression standard error",
    "text": "Regression standard error\nOnce we fit the model, we can use the residuals to estimate the regression standard error (the spread of the distribution of the response, for a given value of the predictor variable):\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{\\sum_\\limits{i=1}^ne_i^2}{n-2}}\n\\]\n\n\n\n\nWhy divide by \\(n - 2\\)? (degree of freedom) (unbiased)\nWhy do we care about the value of the regression standard error?"
  },
  {
    "objectID": "slides/lec-6.html#standard-error-of-hatbeta_1",
    "href": "slides/lec-6.html#standard-error-of-hatbeta_1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard error of \\(\\hat{\\beta}_1\\)",
    "text": "Standard error of \\(\\hat{\\beta}_1\\)\n\\[\nSE_{\\hat{\\beta}_1} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{(n-1)s_X^2}}\n\\]\n\nor‚Ä¶\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    116652.33 \n    53302.46 \n    2.19 \n    0.03 \n  \n  \n    area \n    159.48 \n    18.17 \n    8.78 \n    0.00 \n  \n\n\n\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-7.html#computational-setup",
    "href": "slides/lec-7.html#computational-setup",
    "title": "SLR: Model diagnostics",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-7.html#the-regression-model-revisited",
    "href": "slides/lec-7.html#the-regression-model-revisited",
    "title": "SLR: Model diagnostics",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\ndf_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %>%\n  kable(digits = 2)\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    116652.33 \n    53302.46 \n    2.19 \n    0.03 \n  \n  \n    area \n    159.48 \n    18.17 \n    8.78 \n    0.00"
  },
  {
    "objectID": "slides/lec-7.html#ht-for-the-slope",
    "href": "slides/lec-7.html#ht-for-the-slope",
    "title": "SLR: Model diagnostics",
    "section": "HT for the slope",
    "text": "HT for the slope\nHypotheses: \\(H_0: \\beta_1 = 0\\) vs.¬†\\(H_A: \\beta_1 \\ne 0\\)\nTest statistic: Number of standard errors the estimate is away from the null\n\\[\nT = \\frac{\\text{Estimate - Null}}{\\text{Standard error}} \\\\\n\\]\n\np-value: Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\np-value = P(|t| > \\text{test statistic}),\n\\]\ncalculated from a \\(t\\) distribution with \\(n - 2\\) degrees of freedom"
  },
  {
    "objectID": "slides/lec-7.html#ht-test-statistic",
    "href": "slides/lec-7.html#ht-test-statistic",
    "title": "SLR: Model diagnostics",
    "section": "HT: Test statistic",
    "text": "HT: Test statistic\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    116652.33 \n    53302.46 \n    2.19 \n    0.03 \n  \n  \n    area \n    159.48 \n    18.17 \n    8.78 \n    0.00 \n  \n\n\n\n\n\n\\[\nt = \\frac{\\hat{\\beta}_1 - 0}{SE_{\\hat{\\beta}_1}} = \\frac{159.48 - 0}{18.17} = 8.78\n\\]"
  },
  {
    "objectID": "slides/lec-7.html#ht-p-value",
    "href": "slides/lec-7.html#ht-p-value",
    "title": "SLR: Model diagnostics",
    "section": "HT: p-value",
    "text": "HT: p-value\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    116652.33 \n    53302.46 \n    2.19 \n    0.03 \n  \n  \n    area \n    159.48 \n    18.17 \n    8.78 \n    0.00"
  },
  {
    "objectID": "slides/lec-7.html#understanding-the-p-value",
    "href": "slides/lec-7.html#understanding-the-p-value",
    "title": "SLR: Model diagnostics",
    "section": "Understanding the p-value",
    "text": "Understanding the p-value\n\n\n\nMagnitude of p-value\nInterpretation\n\n\n\n\np-value < 0.01\nstrong evidence against \\(H_0\\)\n\n\n0.01 < p-value < 0.05\nmoderate evidence against \\(H_0\\)\n\n\n0.05 < p-value < 0.1\nweak evidence against \\(H_0\\)\n\n\np-value > 0.1\neffectively no evidence against \\(H_0\\)\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThese are general guidelines. The strength of evidence depends on the context of the problem."
  },
  {
    "objectID": "slides/lec-7.html#ht-conclusion-in-context",
    "href": "slides/lec-7.html#ht-conclusion-in-context",
    "title": "SLR: Model diagnostics",
    "section": "HT: Conclusion, in context",
    "text": "HT: Conclusion, in context\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    116652.33 \n    53302.46 \n    2.19 \n    0.03 \n  \n  \n    area \n    159.48 \n    18.17 \n    8.78 \n    0.00 \n  \n\n\n\n\n\n\nThe data provide convincing evidence that the population slope \\(\\beta_1\\) is different from 0.\nThe data provide convincing evidence of a linear relationship between area and price of houses in Duke Forest."
  },
  {
    "objectID": "slides/lec-7.html#ci-for-the-slope",
    "href": "slides/lec-7.html#ci-for-the-slope",
    "title": "SLR: Model diagnostics",
    "section": "CI for the slope",
    "text": "CI for the slope\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE_{\\hat{\\beta}_1}\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-2\\) degrees of freedom"
  },
  {
    "objectID": "slides/lec-7.html#ci-critical-value",
    "href": "slides/lec-7.html#ci-critical-value",
    "title": "SLR: Model diagnostics",
    "section": "CI: Critical value",
    "text": "CI: Critical value\nqt quantile of t-dist\n\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(duke_forest) - 2)\n\n[1] 1.984984\n\n# confidence level: 90%\nqt(0.95, df = nrow(duke_forest) - 2)\n\n[1] 1.660881\n\n# confidence level: 99%\nqt(0.995, df = nrow(duke_forest) - 2)\n\n[1] 2.628016"
  },
  {
    "objectID": "slides/lec-7.html#ci-for-the-slope-calculation",
    "href": "slides/lec-7.html#ci-for-the-slope-calculation",
    "title": "SLR: Model diagnostics",
    "section": "95% CI for the slope: Calculation",
    "text": "95% CI for the slope: Calculation\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    116652.33 \n    53302.46 \n    2.19 \n    0.03 \n  \n  \n    area \n    159.48 \n    18.17 \n    8.78 \n    0.00 \n  \n\n\n\n\n\n\\[\\hat{\\beta}_1 = 159.48 \\hspace{15mm} t^* = 1.98 \\hspace{15mm} SE_{\\hat{\\beta}_1} = 18.17\\]\n\n\\[159.48 \\pm 1.98 \\times 18.17 = (123.50, 195.46)\\]"
  },
  {
    "objectID": "slides/lec-7.html#ci-for-the-slope-computation",
    "href": "slides/lec-7.html#ci-for-the-slope-computation",
    "title": "SLR: Model diagnostics",
    "section": "95% CI for the slope: Computation",
    "text": "95% CI for the slope: Computation\n\ntidy(df_fit, conf.int = TRUE, conf.level = 0.95) %>% \n  kable(digits = 2)\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n  \n \n\n  \n    (Intercept) \n    116652.33 \n    53302.46 \n    2.19 \n    0.03 \n    10847.77 \n    222456.88 \n  \n  \n    area \n    159.48 \n    18.17 \n    8.78 \n    0.00 \n    123.41 \n    195.55"
  },
  {
    "objectID": "slides/lec-7.html#confidence-interval-for-predictions",
    "href": "slides/lec-7.html#confidence-interval-for-predictions",
    "title": "SLR: Model diagnostics",
    "section": "Confidence interval for predictions",
    "text": "Confidence interval for predictions\n\nSuppose we want to answer the question ‚ÄúWhat is the predicted sale price of a Duke Forest house that is 2,800 square feet?‚Äù\nWe said reporting a single estimate for the slope is not wise, and we should report a plausible range instead\nSimilarly, reporting a single prediction for a new value is not wise, and we should report a plausible range instead"
  },
  {
    "objectID": "slides/lec-7.html#two-types-of-predictions",
    "href": "slides/lec-7.html#two-types-of-predictions",
    "title": "SLR: Model diagnostics",
    "section": "Two types of predictions",
    "text": "Two types of predictions\n\nPrediction for the mean: ‚ÄúWhat is the average predicted sale price of Duke Forest houses that are 2,800 square feet?‚Äù\nPrediction for an individual observation: ‚ÄúWhat is the predicted sale price of a Duke Forest house that is 2,800 square feet?‚Äù\n\n\n\nWhich would you expect to be more variable? The average prediction or the prediction for an individual observation? Based on your answer, how would you expect the widths of plausible ranges for these two predictions to compare?"
  },
  {
    "objectID": "slides/lec-7.html#uncertainty-in-predictions",
    "href": "slides/lec-7.html#uncertainty-in-predictions",
    "title": "SLR: Model diagnostics",
    "section": "Uncertainty in predictions",
    "text": "Uncertainty in predictions\nConfidence interval for the mean outcome: \\[\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE}_{\\hat{\\boldsymbol{\\mu}}}}\\]\n\nPrediction interval for an individual observation: \\[\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE_{\\hat{y}}}}\\]"
  },
  {
    "objectID": "slides/lec-7.html#standard-errors",
    "href": "slides/lec-7.html#standard-errors",
    "title": "SLR: Model diagnostics",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\n\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{1 + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/lec-7.html#standard-errors-1",
    "href": "slides/lec-7.html#standard-errors-1",
    "title": "SLR: Model diagnostics",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\mathbf{\\color{purple}{\\Large{1}}} + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/lec-7.html#confidence-interval",
    "href": "slides/lec-7.html#confidence-interval",
    "title": "SLR: Model diagnostics",
    "section": "Confidence interval",
    "text": "Confidence interval\nThe 95% confidence interval for the mean outcome:\n\nnew_house <- tibble(area = 2800)\n\npredict(df_fit, new_data = new_house, type = \"conf_int\", level = 0.95)\n\n# A tibble: 1 √ó 2\n  .pred_lower .pred_upper\n        <dbl>       <dbl>\n1     529351.     597060.\n\n\n\n\n\n\nWe are 95% confident that mean sale price of Duke Forest houses that are 2,800 square feet is between $529,351 and $597,060."
  },
  {
    "objectID": "slides/lec-7.html#prediction-interval",
    "href": "slides/lec-7.html#prediction-interval",
    "title": "SLR: Model diagnostics",
    "section": "Prediction interval",
    "text": "Prediction interval\nThe 95% prediction interval for the individual outcome:\n\npredict(df_fit, new_data = new_house, type = \"pred_int\", level = 0.95)\n\n# A tibble: 1 √ó 2\n  .pred_lower .pred_upper\n        <dbl>       <dbl>\n1     226438.     899973.\n\n\n\n\n\n\nWe are 95% confident that predicted sale price of a Duke Forest house that is 2,800 square feet is between $226,438 and $899,973."
  },
  {
    "objectID": "slides/lec-7.html#comparing-intervals",
    "href": "slides/lec-7.html#comparing-intervals",
    "title": "SLR: Model diagnostics",
    "section": "Comparing intervals",
    "text": "Comparing intervals"
  },
  {
    "objectID": "slides/lec-7.html#extrapolation",
    "href": "slides/lec-7.html#extrapolation",
    "title": "SLR: Model diagnostics",
    "section": "Extrapolation",
    "text": "Extrapolation\n\n\n\nCalculate the prediction interval for the sale price of a ‚Äútiny house‚Äù in Duke Forest that is 225 square feet."
  },
  {
    "objectID": "slides/lec-7.html#model-conditions-1",
    "href": "slides/lec-7.html#model-conditions-1",
    "title": "SLR: Model diagnostics",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable, i.e.¬†the errors are homeoscedastic\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/lec-7.html#linearity",
    "href": "slides/lec-7.html#linearity",
    "title": "SLR: Model diagnostics",
    "section": "Linearity",
    "text": "Linearity\n‚úÖ The residuals vs.¬†fitted values plot should not show a random scatter of residuals (no distinguishable pattern or structure)"
  },
  {
    "objectID": "slides/lec-7.html#residuals-vs.-fitted-values",
    "href": "slides/lec-7.html#residuals-vs.-fitted-values",
    "title": "SLR: Model diagnostics",
    "section": "Residuals vs.¬†fitted values",
    "text": "Residuals vs.¬†fitted values\n\ndf_aug <- augment(df_fit$fit)\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/lec-7.html#application-exercise",
    "href": "slides/lec-7.html#application-exercise",
    "title": "SLR: Model diagnostics",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã github.com/STA210-Summer22/ae-3-duke-forest"
  },
  {
    "objectID": "slides/lec-7.html#non-linear-relationships",
    "href": "slides/lec-7.html#non-linear-relationships",
    "title": "SLR: Model diagnostics",
    "section": "Non-linear relationships",
    "text": "Non-linear relationships"
  },
  {
    "objectID": "slides/lec-7.html#constant-variance",
    "href": "slides/lec-7.html#constant-variance",
    "title": "SLR: Model diagnostics",
    "section": "Constant variance",
    "text": "Constant variance\n‚úÖ The vertical spread of the residuals should be relatively constant across the plot"
  },
  {
    "objectID": "slides/lec-7.html#non-constant-variance",
    "href": "slides/lec-7.html#non-constant-variance",
    "title": "SLR: Model diagnostics",
    "section": "Non-constant variance",
    "text": "Non-constant variance"
  },
  {
    "objectID": "slides/lec-7.html#normality",
    "href": "slides/lec-7.html#normality",
    "title": "SLR: Model diagnostics",
    "section": "Normality",
    "text": "Normality"
  },
  {
    "objectID": "slides/lec-7.html#independence",
    "href": "slides/lec-7.html#independence",
    "title": "SLR: Model diagnostics",
    "section": "Independence",
    "text": "Independence\n\nWe can often check the independence assumption based on the context of the data and how the observations were collected\nIf the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected\n\n\n‚úÖ If this is a random sample of Duke Houses, the error for one house does not tell us anything about the error for another use"
  },
  {
    "objectID": "slides/lec-7.html#recap",
    "href": "slides/lec-7.html#recap",
    "title": "SLR: Model diagnostics",
    "section": "Recap",
    "text": "Recap\nUsed residual plots to check conditions for SLR:\n\n\n\n\nLinearity\nConstant variance\n\n\n\n\n\nNormality\nIndependence\n\n\n\n\n\nWhich of these conditions are required for fitting a SLR? Which for simulation-based inference for the slope for an SLR? Which for inference with mathematical models?\n\n\nfitting: linearity + independence\ninference (simulation-based): linearity + independence\ninference (mathematical): ‚Ä¶ + normality + constant variance\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html",
    "href": "ae/ae-4-exam-1-review.html",
    "title": "AE 4: Exam 1 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-4-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#packages",
    "href": "ae/ae-4-exam-1-review.html#packages",
    "title": "AE 4: Exam 1 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggfortify)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "href": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "title": "AE 4: Exam 1 Review",
    "section": "Restaurant tips",
    "text": "Restaurant tips\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St.¬†Olaf who worked at a local restaurant.1\nThe variables we‚Äôll focus on for this analysis are\n\nTip: amount of the tip\nParty: number of people in the party\n\nView the data set to see the remaining variables.\n\ntips <- read_csv(\"data/tip-data.csv\")"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "href": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "title": "AE 4: Exam 1 Review",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\n\nVisualize, summarize, and describe the relationship between Party and Tip.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#modeling",
    "href": "ae/ae-4-exam-1-review.html#modeling",
    "title": "AE 4: Exam 1 Review",
    "section": "Modeling",
    "text": "Modeling\nLet‚Äôs start by fitting a model using Party to predict the Tip at this restaurant.\n\nWrite the statistical model.\nFit the regression line and write the regression equation. Name the model tips_fit and display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret the slope.\nDoes it make sense to interpret the intercept? Explain your reasoning.\nWhat is the R-squared and RMSE for this model? How do you evaluate your model‚Äôs fitting performance? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#inference",
    "href": "ae/ae-4-exam-1-review.html#inference",
    "title": "AE 4: Exam 1 Review",
    "section": "Inference",
    "text": "Inference\n\nInference for the slope\n\nThe following code can be used to create a bootstrap distribution for the slope (and the intercept, though we‚Äôll focus primarily on the slope in our inference). Describe what each line of code does, supplemented by any visualizations that might help with your description.\n\n\nset.seed(1234)\n\nboot_dist <- tips %>%\n  specify(Tip ~ Party) %>%\n  generate(reps = 100, type = \"bootstrap\") %>%\n  fit()\n\n\nUse the bootstrap distribution created in Exercise 7, boot_dist, to construct a 90% confidence interval for the slope using bootstrapping and the percentile method and interpret it in context of the data.\n\n\n# add your code here\n\n\nConduct a hypothesis test at the equivalent significance level using permutation. State the hypotheses and the significance level you‚Äôre using explicitly. Also include a visualization of the null distribution of the slope with the observed slope marked as a vertical line.\n\n\n# add your code here\n\n\nNow repeat Exercises 8 and 9 using approaches based on mathematical models.\n\n\n# add your code here\n\n\n\nInference for a prediction\n\nBased on your model, predict the tip for a party of 4.\n\n\n# add your code here\n\n\nSuppose you‚Äôre asked to construct a confidence and a prediction interval. Which one would you expect to be wider and why? In your answer clearly state the difference between these intervals.\nNow construct the intervals from Exercise 11 and comment on whether your guess is confirmed.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html",
    "href": "ae/ae-3-duke-forest.html",
    "title": "AE 3: Duke Forest houses",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-3-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#packages",
    "href": "ae/ae-3-duke-forest.html#packages",
    "title": "AE 3: Duke Forest houses",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#predict-sale-price-from-area",
    "href": "ae/ae-3-duke-forest.html#predict-sale-price-from-area",
    "title": "AE 3: Duke Forest houses",
    "section": "Predict sale price from area",
    "text": "Predict sale price from area\n\ndf_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %>%\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#model-conditions",
    "href": "ae/ae-3-duke-forest.html#model-conditions",
    "title": "AE 3: Duke Forest houses",
    "section": "Model conditions",
    "text": "Model conditions\n\nExercise 1\nThe following code produces the residuals vs.¬†fitted values plot for this model. Comment out the layer that defines the y-axis limits and re-create the plot. How does the plot change? Why might we want to define the limits explicitly?\n\ndf_aug <- augment(df_fit$fit)\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )\n\n\n\n\n\n\nExercise 2\nImprove how the values on the axes of the plot are displayed by modifying the code below.\nHint: use scale_continuous()\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/lec-9.html#announcements",
    "href": "slides/lec-9.html#announcements",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Announcements",
    "text": "Announcements\n\nExam 1 opens today at 1:00 pm and ends on Monday, May 23 at 11:59pm."
  },
  {
    "objectID": "slides/lec-9.html#feedback-from-submissions-so-far",
    "href": "slides/lec-9.html#feedback-from-submissions-so-far",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Feedback from submissions so far",
    "text": "Feedback from submissions so far\n\nYou must submit a PDF (not HTML) to Gradescope\nYou must tag your pages when you upload to Gradescope ‚Äì if you don‚Äôt know how to do this, please ask well before the deadline!\nYou must not refer to keys distributed in previous semesters of the course ‚Äì much of what we‚Äôre doing is different and some of it is the same. If you need help, please ask!"
  },
  {
    "objectID": "slides/lec-9.html#exam-1",
    "href": "slides/lec-9.html#exam-1",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Exam 1",
    "text": "Exam 1\n\nInstructions can be found at website\nCovers everything we‚Äôve done so far\nAny clarification questions for the exam?\n\nPost on Sakai Conversations, post to ‚ÄúInstructors in this site‚Äù"
  },
  {
    "objectID": "slides/lec-9.html#application-exercise",
    "href": "slides/lec-9.html#application-exercise",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Application Exercise",
    "text": "Application Exercise\n\nüìã github.com/STA210-Summer22/ae-4\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-7.html#previous-questions",
    "href": "slides/lec-7.html#previous-questions",
    "title": "SLR: Model diagnostics",
    "section": "Previous Questions",
    "text": "Previous Questions\n\nInterpretation of CI: In frequentists‚Äô world, we do not say we are 95% confident that the slope falls into [90,200], or the slope is between 90 and 200. Since the slope is a fixed value that always unknown to us instead of a variable! We should interpret it as : if we have repeated trials, 95% of time, caculated CI will cover the true slope. Since the CI here is a variable which vary with realizations."
  },
  {
    "objectID": "slides/lab-3.html#the-data",
    "href": "slides/lab-3.html#the-data",
    "title": "Lab 3 - Coffee ratings",
    "section": "The data",
    "text": "The data"
  },
  {
    "objectID": "slides/lab-3.html#the-data-an-outlier",
    "href": "slides/lab-3.html#the-data-an-outlier",
    "title": "Lab 3 - Coffee ratings",
    "section": "The data + an outlier",
    "text": "The data + an outlier"
  },
  {
    "objectID": "slides/lab-3.html#the-data-influential-point",
    "href": "slides/lab-3.html#the-data-influential-point",
    "title": "Lab 3 - Coffee ratings",
    "section": "The data + influential point",
    "text": "The data + influential point"
  },
  {
    "objectID": "slides/lab-3.html#influential-point",
    "href": "slides/lab-3.html#influential-point",
    "title": "Lab 3 - Coffee ratings",
    "section": "Influential point",
    "text": "Influential point\nAn observation is influential if removing it substantially changes the coefficients of the regression model."
  },
  {
    "objectID": "slides/lab-3.html#influential-points",
    "href": "slides/lab-3.html#influential-points",
    "title": "Lab 3 - Coffee ratings",
    "section": "Influential points",
    "text": "Influential points\n\nInfluential points have a large impact on the coefficients and standard errors used for inference\nThese points can sometimes be identified in a scatterplot if there is only one predictor variable, this is often not the case when there are multiple predictors\nWe will use measures to quantify an individual observation‚Äôs influence on the regression model: leverage, standardized residuals, and Cook‚Äôs distance"
  },
  {
    "objectID": "slides/lab-3.html#remember-augment",
    "href": "slides/lab-3.html#remember-augment",
    "title": "Lab 3 - Coffee ratings",
    "section": "Remember augment()?",
    "text": "Remember augment()?\n\nmtcars_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(mpg ~ disp, data = mtcars)\n\naugment(mtcars_fit$fit)\n\n# A tibble: 32 √ó 9\n   .rownames           mpg  disp .fitted .resid   .hat .sigma .cooksd .std.resid\n   <chr>             <dbl> <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n 1 Mazda RX4          21    160     23.0 -2.01  0.0418   3.29 8.65e-3     -0.630\n 2 Mazda RX4 Wag      21    160     23.0 -2.01  0.0418   3.29 8.65e-3     -0.630\n 3 Datsun 710         22.8  108     25.1 -2.35  0.0629   3.28 1.87e-2     -0.746\n 4 Hornet 4 Drive     21.4  258     19.0  2.43  0.0328   3.27 9.83e-3      0.761\n 5 Hornet Sportabout  18.7  360     14.8  3.94  0.0663   3.22 5.58e-2      1.25 \n 6 Valiant            18.1  225     20.3 -2.23  0.0313   3.28 7.82e-3     -0.696\n 7 Duster 360         14.3  360     14.8 -0.462 0.0663   3.31 7.70e-4     -0.147\n 8 Merc 240D          24.4  147.    23.6  0.846 0.0461   3.30 1.72e-3      0.267\n 9 Merc 230           22.8  141.    23.8 -0.997 0.0482   3.30 2.50e-3     -0.314\n10 Merc 280           19.2  168.    22.7 -3.49  0.0396   3.24 2.48e-2     -1.10 \n# ‚Ä¶ with 22 more rows"
  },
  {
    "objectID": "slides/lab-3.html#model-diagnostics-1",
    "href": "slides/lab-3.html#model-diagnostics-1",
    "title": "Lab 3 - Coffee ratings",
    "section": "Model diagnostics",
    "text": "Model diagnostics\nUse the augment() function to output statistics that can be used to diagnose the model, along with the predicted values and residuals:\n\n\noutcome and predictor variables in the model\n.fitted: predicted values\n.se.fit: standard errors of predicted values\n.resid: residuals\n.hat: leverage\n.sigma: estimate of residual standard deviation when the corresponding observation is dropped from model\n.cooksd: Cook‚Äôs distance\n.std.resid: standardized residuals\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-9.html#interpretation-on-ci",
    "href": "slides/lec-9.html#interpretation-on-ci",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Interpretation on CI",
    "text": "Interpretation on CI\nWe are 95% confident that, as xx increase by 1 unit, the model predicts xx increase/decrease [,] on average.\nWe are 95% confident that mean sale price of Duke Forest houses that are 2,800 square feet is between XX and XX."
  },
  {
    "objectID": "slides/lec-9.html#outliers-and-influential-points",
    "href": "slides/lec-9.html#outliers-and-influential-points",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Outliers and influential points",
    "text": "Outliers and influential points"
  },
  {
    "objectID": "slides/lec-9.html#outliers",
    "href": "slides/lec-9.html#outliers",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Outliers",
    "text": "Outliers\n\nBoxplot\nStandard residuals"
  },
  {
    "objectID": "slides/lec-9.html#identifying-influential-points",
    "href": "slides/lec-9.html#identifying-influential-points",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Identifying influential points",
    "text": "Identifying influential points\n\nLeverage\nStandardized residuals\nCook‚Äôs Distance\n\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(kableExtra)\nlibrary(ggfortify)\nlibrary(viridis)"
  },
  {
    "objectID": "slides/lec-9.html#influential-point",
    "href": "slides/lec-9.html#influential-point",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Influential Point",
    "text": "Influential Point\nAn observation is influential if removing it substantially changes the coefficients of the regression model"
  },
  {
    "objectID": "slides/lec-9.html#influential-points",
    "href": "slides/lec-9.html#influential-points",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Influential points",
    "text": "Influential points\n\nInfluential points have a large impact on the coefficients and standard errors used for inference\nThese points can sometimes be identified in a scatterplot if there is only one predictor variable\n\nThis is often not the case when there are multiple predictors\n\nWe will use measures to quantify an individual observation‚Äôs influence on the regression model\n\nleverage, standardized residuals, and Cook‚Äôs distance"
  },
  {
    "objectID": "slides/lec-9.html#model-diagnostics-in-r",
    "href": "slides/lec-9.html#model-diagnostics-in-r",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Model diagnostics in R",
    "text": "Model diagnostics in R\nUse the augment function to output the model diagnostics (along with the predicted values and residuals)\n\nresponse and predictor variables in the model\n.fitted: predicted values\n.se.fit: standard errors of predicted values\n.resid: residuals\n.hat: leverage\n.sigma: estimate of residual standard deviation when the corresponding observation is dropped from model\n.cooksd: Cook‚Äôs distance\n.std.resid: standardized residuals"
  },
  {
    "objectID": "slides/lec-9.html#example-average-sat-scores-by-state",
    "href": "slides/lec-9.html#example-average-sat-scores-by-state",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Example: Average SAT scores by state",
    "text": "Example: Average SAT scores by state\n\nThis data set contains the average SAT score (out of 1600) and other variables that may be associated with SAT performance for each of the 50 U.S. states. The data is based on test takers for the 1982 exam.\nResponse - .vocab[SAT]: average total SAT score\nPredictor - .vocab[Public]: percentage of test-takers who attended public high schools"
  },
  {
    "objectID": "slides/lec-9.html#footnotedata-comes-from-case1201-data-set-in-the-sleuth3-package",
    "href": "slides/lec-9.html#footnotedata-comes-from-case1201-data-set-in-the-sleuth3-package",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": ".footnote[Data comes from case1201 data set in the Sleuth3 package]",
    "text": ".footnote[Data comes from case1201 data set in the Sleuth3 package]"
  },
  {
    "objectID": "slides/lec-9.html#model",
    "href": "slides/lec-9.html#model",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Model",
    "text": "Model\n\nsat_scores <- Sleuth3::case1201\n\n\nsat_model <- lm(SAT ~ Public, data = sat_scores)\ntidy(sat_model) %>%\n  kable(digits = 3)\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    994.971 \n    84.807 \n    11.732 \n    0.000 \n  \n  \n    Public \n    -0.579 \n    1.037 \n    -0.559 \n    0.579"
  },
  {
    "objectID": "slides/lec-9.html#sat-augmented-data",
    "href": "slides/lec-9.html#sat-augmented-data",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "SAT: Augmented Data",
    "text": "SAT: Augmented Data\n\nsat_aug = augment(sat_model) %>%\n  mutate(obs_num=row_number())\nglimpse(sat_aug)\n\nRows: 50\nColumns: 9\n$ SAT        <int> 1088, 1075, 1068, 1045, 1045, 1033, 1028, 1022, 1017, 1011,‚Ä¶\n$ Public     <dbl> 87.8, 86.2, 88.3, 83.9, 83.6, 93.7, 78.3, 75.2, 97.0, 77.3,‚Ä¶\n$ .fitted    <dbl> 944.1198, 945.0465, 943.8302, 946.3786, 946.5523, 940.7027,‚Ä¶\n$ .resid     <dbl> 143.880224, 129.953547, 124.169810, 98.621450, 98.447698, 9‚Ä¶\n$ .hat       <dbl> 0.02918707, 0.02527061, 0.03063269, 0.02153481, 0.02121224,‚Ä¶\n$ .sigma     <dbl> 68.89683, 69.51144, 69.72849, 70.63271, 70.63847, 70.77489,‚Ä¶\n$ .cooksd    <dbl> 0.0629494764, 0.0441056591, 0.0493526954, 0.0214814500, 0.0‚Ä¶\n$ .std.resid <dbl> 2.0463672, 1.8445751, 1.7673480, 1.3971689, 1.3944776, 1.32‚Ä¶\n$ obs_num    <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ‚Ä¶"
  },
  {
    "objectID": "slides/lec-9.html#leverage",
    "href": "slides/lec-9.html#leverage",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Leverage",
    "text": "Leverage\n\nLeverage: measure of the distance between an observation‚Äôs values of the predictor variables and the average values of the predictor variables for the entire data set\nAn observation has high leverage if its combination of values for the predictor variables is very far from the typical combination of values in the data\nObservations with high leverage should be considered as potential influential points"
  },
  {
    "objectID": "slides/lec-9.html#calculating-leverage",
    "href": "slides/lec-9.html#calculating-leverage",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Calculating leverage",
    "text": "Calculating leverage\nSimple Regression: leverage of the \\(i^{th}\\) observation \\[h_i =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\\]\n\nNote: Leverage only depends on values of the predictor variable(s)"
  },
  {
    "objectID": "slides/lec-9.html#high-leverage",
    "href": "slides/lec-9.html#high-leverage",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "High Leverage",
    "text": "High Leverage\n\nThe sum of the leverages for all points is \\(p + 1\\)\n\\(p\\) is the number of predictors\nIn the case of SLR \\(\\sum_{i=1}^n h_i = 2\\)\nThe ‚Äútypical‚Äù leverage is \\(\\frac{(p+1)}{n}\\)\nAn observation has high leverage if \\[h_i > \\frac{2(p+1)}{n}\\]"
  },
  {
    "objectID": "slides/lec-9.html#high-leverage-1",
    "href": "slides/lec-9.html#high-leverage-1",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "High Leverage",
    "text": "High Leverage\nIf there is point with high leverage, ask\n\nIs there a data entry error?\nIs this observation within the scope of individuals for which you want to make predictions and draw conclusions?\nIs this observation impacting the estimates of the model coefficients, especially for interactions?\n\nJust because a point has high leverage does not necessarily mean it will have a substantial impact on the regression. Therefore we need to check other measures."
  },
  {
    "objectID": "slides/lec-9.html#sat-leverage",
    "href": "slides/lec-9.html#sat-leverage",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "SAT: Leverage",
    "text": "SAT: Leverage"
  },
  {
    "objectID": "slides/lec-9.html#observations-with-high-leverage",
    "href": "slides/lec-9.html#observations-with-high-leverage",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Observations with high leverage",
    "text": "Observations with high leverage\n\n(leverage_threshold <- 2*(1+1)/nrow(sat_aug))\n\n[1] 0.08\n\nautoplot(sat_model,which = 5, ncol = 1) + \n  geom_vline(xintercept = leverage_threshold, color = \"red\")\n\nsat_aug %>% filter(.hat > leverage_threshold) %>% \n  select(SAT, Public)\n\n# A tibble: 2 √ó 2\n    SAT Public\n  <int>  <dbl>\n1   999   61.2\n2   975   44.8\n\n\nWhy do you think these observations have high leverage?"
  },
  {
    "objectID": "slides/lec-9.html#lets-dig-into-the-data",
    "href": "slides/lec-9.html#lets-dig-into-the-data",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Let‚Äôs dig into the data",
    "text": "Let‚Äôs dig into the data"
  },
  {
    "objectID": "slides/lec-9.html#standardized-residuals",
    "href": "slides/lec-9.html#standardized-residuals",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Standardized residuals",
    "text": "Standardized residuals\n\nWhat is the best way to identify outliers (points that don‚Äôt fit the pattern from the regression line)?\nLook for points that have large residuals\nWe want a common scale, so we can more easily identify ‚Äúlarge‚Äù residuals\nWe will look at each residual divided by its standard error"
  },
  {
    "objectID": "slides/lec-9.html#standardized-residuals-1",
    "href": "slides/lec-9.html#standardized-residuals-1",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Standardized residuals",
    "text": "Standardized residuals\n\\[std.res_i = \\frac{y_i - \\hat{y}_i}{\\hat{\\sigma}_\\epsilon\\sqrt{1-h_i}}\\]\n\nStandardized residuals are produced by augment in the column .std.resid"
  },
  {
    "objectID": "slides/lec-9.html#standardized-residuals-2",
    "href": "slides/lec-9.html#standardized-residuals-2",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Standardized residuals",
    "text": "Standardized residuals\nObservations with high leverage tend to have low values of standardized residuals because they pull the regression line towards them\n\nautoplot(sat_model, which = 5, ncol = 1)"
  },
  {
    "objectID": "slides/lec-9.html#using-standardized-residuals",
    "href": "slides/lec-9.html#using-standardized-residuals",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Using standardized residuals",
    "text": "Using standardized residuals\nObservations that have standardized residuals of large magnitude are outliers, since they don‚Äôt fit the pattern determined by the regression model\nAn observation is a potential outlier if its standardized residual is beyond \\(\\pm 3\\).\nMake residual plots with standardized residuals to make it easier to identify outliers\n\nautoplot(sat_model, which = 3, ncol = 1) + \n  geom_hline(yintercept = sqrt(3),color = \"red\",linetype = \"dotted\")"
  },
  {
    "objectID": "slides/lec-9.html#motivating-cooks-distance",
    "href": "slides/lec-9.html#motivating-cooks-distance",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Motivating Cook‚Äôs Distance",
    "text": "Motivating Cook‚Äôs Distance\nAn observation‚Äôs influence on the regression line depends on\n\nHow close it lies to the general trend of the data - (Standardized residual)\nIts leverage - \\(h_i\\)\n\nCook‚Äôs Distance is a statistic that includes both of these components to measure an observation‚Äôs overall impact on the model"
  },
  {
    "objectID": "slides/lec-9.html#cooks-distance",
    "href": "slides/lec-9.html#cooks-distance",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Cook‚Äôs Distance",
    "text": "Cook‚Äôs Distance\nCook‚Äôs distance for the \\(i^{th}\\) observation\nAn observation with large \\(D_i\\) is said to have a strong influence on the predicted values\nAn observation with\n\n\\(D_i > 0.5\\) is moderately influential\n\\(D_i > 1\\) is very influential"
  },
  {
    "objectID": "slides/lec-9.html#cooks-distance-1",
    "href": "slides/lec-9.html#cooks-distance-1",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Cook‚Äôs Distance",
    "text": "Cook‚Äôs Distance\n\nautoplot(sat_model, which = 4, ncol = 1) + \n  geom_hline(yintercept = 0.5, color = \"red\", lty = 2) +\n  geom_hline(yintercept = 1,color = \"red\")"
  },
  {
    "objectID": "slides/lec-9.html#using-these-measures",
    "href": "slides/lec-9.html#using-these-measures",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Using these measures",
    "text": "Using these measures\n\nStandardized residuals, leverage, and Cook‚Äôs Distance should all be examined together\nExamine plots of the measures to identify observations that are outliers, high leverage, and may potentially impact the model."
  },
  {
    "objectID": "slides/lec-9.html#what-to-do-with-outliersinfluential-points",
    "href": "slides/lec-9.html#what-to-do-with-outliersinfluential-points",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "What to do with outliers/influential points?",
    "text": "What to do with outliers/influential points?\nIt is OK to drop an observation based on the predictor variables if‚Ä¶\n\nIt is meaningful to drop the observation given the context of the problem\nYou intended to build a model on a smaller range of the predictor variables. Mention this in the write up of the results and be careful to avoid extrapolation when making predictions"
  },
  {
    "objectID": "slides/lec-9.html#what-to-do-with-outliersinfluential-points-1",
    "href": "slides/lec-9.html#what-to-do-with-outliersinfluential-points-1",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "What to do with outliers/influential points?",
    "text": "What to do with outliers/influential points?\nIt is not OK to drop an observation based on the response variable\n\nThese are legitimate observations and should be in the model\nYou can try transformations or increasing the sample size by collecting more data\n\n‚Äì\nIn either instance, you can try building the model with and without the outliers/influential observations"
  },
  {
    "objectID": "slides/lec-9.html#tips-on-r-programming",
    "href": "slides/lec-9.html#tips-on-r-programming",
    "title": "Model Diagnostics and Exam 1 Review",
    "section": "Tips on R programming",
    "text": "Tips on R programming\n\nlibrary(tidyverse)\ntest <- tibble(a=1,b=2)\ntest\n\n# A tibble: 1 √ó 2\n      a     b\n  <dbl> <dbl>\n1     1     2\n\n\nThe test values are 1 and 2."
  },
  {
    "objectID": "slides/lec-8.html#announcements",
    "href": "slides/lec-8.html#announcements",
    "title": "Multiple linear regression (MLR)",
    "section": "Announcements",
    "text": "Announcements\n\nLab 3:\n\n\nAny questions about lab / teamwork?\nCheck ddl\n\n\nExam 1:\n\n\nMultiple choice questions (mostly conceptual) + open-ended exercises (like lab + homework)\nOpen book, open internet, open questions to me + TA only\nNo communication with others or posting questions on the internet allowed\nWhat can you do to start preparing?\n\nReview readings, assignments, feedback returned\nOrganize your notes\nCome to office hours with questions"
  },
  {
    "objectID": "slides/lec-8.html#computational-setup",
    "href": "slides/lec-8.html#computational-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(patchwork)   # for laying out plots\nlibrary(GGally)      # for pairwise plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-8.html#house-prices-in-levittown",
    "href": "slides/lec-8.html#house-prices-in-levittown",
    "title": "Multiple linear regression (MLR)",
    "section": "House prices in Levittown",
    "text": "House prices in Levittown\n\nThe data set contains the sales price and characteristics of 85 homes in Levittown, NY that sold between June 2010 and May 2011.\nLevittown was built right after WWII and was the first planned suburban community built using mass production techniques.\nThe article ‚ÄúLevittown, the prototypical American suburb ‚Äì a history of cities in 50 buildings, day 25‚Äù gives an overview of Levittown‚Äôs controversial history."
  },
  {
    "objectID": "slides/lec-8.html#analysis-goals",
    "href": "slides/lec-8.html#analysis-goals",
    "title": "Multiple linear regression (MLR)",
    "section": "Analysis goals",
    "text": "Analysis goals\n\nWe would like to use the characteristics of a house to understand variability in the sales price.\nTo do so, we will fit a multiple linear regression model.\nUsing our model, we can answers questions such as\n\n\nWhat is the relationship between the characteristics of a house in Levittown and its sale price?\nGiven its characteristics, what is the expected sale price of a house in Levittown?"
  },
  {
    "objectID": "slides/lec-8.html#the-data",
    "href": "slides/lec-8.html#the-data",
    "title": "Multiple linear regression (MLR)",
    "section": "The data",
    "text": "The data\nhere packge helps you find the path\n\nlevittown <- read_csv(here::here(\"slides/data/homeprices.csv\"))\nlevittown\n\n# A tibble: 85 √ó 7\n   bedrooms bathrooms living_area lot_size year_built property_tax sale_price\n      <dbl>     <dbl>       <dbl>    <dbl>      <dbl>        <dbl>      <dbl>\n 1        4       1          1380     6000       1948         8360     350000\n 2        4       2          1761     7400       1951         5754     360000\n 3        4       2          1564     6000       1948         8982     350000\n 4        5       2          2904     9898       1949        11664     375000\n 5        5       2.5        1942     7788       1948         8120     370000\n 6        4       2          1830     6000       1948         8197     335000\n 7        4       1          1585     6000       1948         6223     295000\n 8        4       1           941     6800       1951         2448     250000\n 9        4       1.5        1481     6000       1948         9087     299990\n10        3       2          1630     5998       1948         9430     375000\n# ‚Ä¶ with 75 more rows"
  },
  {
    "objectID": "slides/lec-8.html#variables",
    "href": "slides/lec-8.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nbedrooms: Number of bedrooms\nbathrooms: Number of bathrooms\nliving_area: Total living area of the house (in square feet)\nlot_size: Total area of the lot (in square feet)\nyear_built: Year the house was built\nproperty_tax: Annual property taxes (in USD)\n\n\nResponse: sale_price: Sales price (in USD)"
  },
  {
    "objectID": "slides/lec-8.html#eda-response-variable",
    "href": "slides/lec-8.html#eda-response-variable",
    "title": "Multiple linear regression (MLR)",
    "section": "EDA: Response variable",
    "text": "EDA: Response variable"
  },
  {
    "objectID": "slides/lec-8.html#eda-predictor-variables",
    "href": "slides/lec-8.html#eda-predictor-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "EDA: Predictor variables",
    "text": "EDA: Predictor variables"
  },
  {
    "objectID": "slides/lec-8.html#eda-response-vs.-predictors",
    "href": "slides/lec-8.html#eda-response-vs.-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "EDA: Response vs.¬†Predictors",
    "text": "EDA: Response vs.¬†Predictors"
  },
  {
    "objectID": "slides/lec-8.html#eda-all-variables",
    "href": "slides/lec-8.html#eda-all-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "EDA: All variables",
    "text": "EDA: All variables\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggpairs(levittown) +\n  theme(\n    axis.text.y = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, size = 10),\n    strip.text.y = element_text(angle = 0, hjust = 0)\n    )"
  },
  {
    "objectID": "slides/lec-8.html#single-vs.-multiple-predictors",
    "href": "slides/lec-8.html#single-vs.-multiple-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Single vs.¬†multiple predictors",
    "text": "Single vs.¬†multiple predictors\nSo far we‚Äôve used a single predictor variable to understand variation in a quantitative response variable\n\nNow we want to use multiple predictor variables to understand variation in a quantitative response variable"
  },
  {
    "objectID": "slides/lec-8.html#multiple-linear-regression-mlr",
    "href": "slides/lec-8.html#multiple-linear-regression-mlr",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\nBased on the analysis goals, we will use a multiple linear regression model of the following form\n\\[\n\\begin{aligned}\\hat{\\text{sale_price}} ~ = & ~\n\\hat{\\beta}_0 + \\hat{\\beta}_1 \\text{bedrooms} + \\hat{\\beta}_2 \\text{bathrooms} + \\hat{\\beta}_3 \\text{living_area} \\\\\n&+ \\hat{\\beta}_4 \\text{lot_size} + \\hat{\\beta}_5 \\text{year_built} + \\hat{\\beta}_6 \\text{property_tax}\\end{aligned}\n\\]\nSimilar to simple linear regression, this model assumes that at each combination of the predictor variables, the values sale_price follow a Normal distribution."
  },
  {
    "objectID": "slides/lec-8.html#regression-model",
    "href": "slides/lec-8.html#regression-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Regression Model",
    "text": "Regression Model\nRecall: The simple linear regression model assumes\n\\[\nY|X\\sim N(\\beta_0 + \\beta_1 X, \\sigma_{\\epsilon}^2)\n\\]\n\nSimilarly: The multiple linear regression model assumes\n\\[\nY|X_1, X_2, \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_{\\epsilon}^2)\n\\]"
  },
  {
    "objectID": "slides/lec-8.html#the-mlr-model",
    "href": "slides/lec-8.html#the-mlr-model",
    "title": "Multiple linear regression (MLR)",
    "section": "The MLR model",
    "text": "The MLR model\nFor a given observation \\((x_{i1}, x_{i2} \\ldots, x_{ip}, y_i)\\)\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i} \\hspace{8mm} \\epsilon_i \\sim N(0,\\sigma_\\epsilon^2)\n\\]"
  },
  {
    "objectID": "slides/lec-8.html#prediction",
    "href": "slides/lec-8.html#prediction",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\nAt any combination of the predictors, the mean value of the response \\(Y\\), is\n\\[\n\\mu_{Y|X_1, \\ldots, X_p} = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n\nUsing multiple linear regression, we can estimate the mean response for any combination of predictors\n\\[\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{1} + \\hat{\\beta}_2 X_2 + \\dots + \\hat{\\beta}_p X_{p}\n\\]"
  },
  {
    "objectID": "slides/lec-8.html#model-fit",
    "href": "slides/lec-8.html#model-fit",
    "title": "Multiple linear regression (MLR)",
    "section": "Model fit",
    "text": "Model fit\n\nprice_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(sale_price ~ bedrooms + bathrooms + living_area + lot_size +\n        year_built + property_tax, data = levittown)\n\ntidy(price_fit) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-7148818.957\n3820093.694\n-1.871\n0.065\n\n\nbedrooms\n-12291.011\n9346.727\n-1.315\n0.192\n\n\nbathrooms\n51699.236\n13094.170\n3.948\n0.000\n\n\nliving_area\n65.903\n15.979\n4.124\n0.000\n\n\nlot_size\n-0.897\n4.194\n-0.214\n0.831\n\n\nyear_built\n3760.898\n1962.504\n1.916\n0.059\n\n\nproperty_tax\n1.476\n2.832\n0.521\n0.604"
  },
  {
    "objectID": "slides/lec-8.html#model-equation",
    "href": "slides/lec-8.html#model-equation",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{price}} = & -7148818.957 - 12291.011 \\times \\text{bedrooms}\\\\[5pt]  \n&+ 51699.236 \\times \\text{bathrooms}  + 65.903 \\times \\text{living area}\\\\[5pt]\n&- 0.897 \\times \\text{lot size} +  3760.898 \\times \\text{year built}\\\\[5pt]\n&+ 1.476 \\times \\text{property tax}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lec-8.html#interpreting-hatbeta_j",
    "href": "slides/lec-8.html#interpreting-hatbeta_j",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_j\\)",
    "text": "Interpreting \\(\\hat{\\beta}_j\\)\n\nThe estimated coefficient \\(\\hat{\\beta}_j\\) is the expected change in the mean of \\(y\\) when \\(x_j\\) increases by one unit, holding the values of all other predictor variables constant.\n\n\n\nExample: The estimated coefficient for living_area is 65.90. This means for each additional square foot of living area, we expect the sale price of a house in Levittown, NY to increase by $65.90, on average, holding all other predictor variables constant."
  },
  {
    "objectID": "slides/lec-8.html#prediction-1",
    "href": "slides/lec-8.html#prediction-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\n\nWhat is the predicted sale price for a house in Levittown, NY with 3 bedrooms, 1 bathroom, 1,050 square feet of living area, 6,000 square foot lot size, built in 1948 with $6,306 in property taxes?\n\n\n\n-7148818.957 - 12291.011 * 3 + 51699.236 * 1 + \n  65.903 * 1050 - 0.897 * 6000 + 3760.898 * 1948 + \n  1.476 * 6306\n\n[1] 265360.4\n\n\n\nThe predicted sale price for a house in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes is $265,360."
  },
  {
    "objectID": "slides/lec-8.html#prediction-revisit",
    "href": "slides/lec-8.html#prediction-revisit",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction, revisit",
    "text": "Prediction, revisit\nJust like with simple linear regression, we can use the predict() function in R to calculate the appropriate intervals for our predicted values:\n\nnew_house <- tibble(\n  bedrooms = 3, bathrooms = 1, \n  living_area = 1050, lot_size = 6000, \n  year_built = 1948, property_tax = 6306\n  )\n\npredict(price_fit, new_house)\n\n# A tibble: 1 √ó 1\n    .pred\n    <dbl>\n1 265360."
  },
  {
    "objectID": "slides/lec-8.html#confidence-interval-for-hatmu_y",
    "href": "slides/lec-8.html#confidence-interval-for-hatmu_y",
    "title": "Multiple linear regression (MLR)",
    "section": "Confidence interval for \\(\\hat{\\mu}_y\\)",
    "text": "Confidence interval for \\(\\hat{\\mu}_y\\)\n\nCalculate a 95% confidence interval for the estimated mean price of houses in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes.\n\n\n\npredict(price_fit, new_house, type = \"conf_int\", level = 0.95)\n\n# A tibble: 1 √ó 2\n  .pred_lower .pred_upper\n        <dbl>       <dbl>\n1     238482.     292239."
  },
  {
    "objectID": "slides/lec-8.html#prediction-interval-for-haty",
    "href": "slides/lec-8.html#prediction-interval-for-haty",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction interval for \\(\\hat{y}\\)",
    "text": "Prediction interval for \\(\\hat{y}\\)\n\nCalculate a 95% prediction interval for an individual house in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes.\n\n\n\npredict(price_fit, new_house, type = \"pred_int\", level = 0.95)\n\n# A tibble: 1 √ó 2\n  .pred_lower .pred_upper\n        <dbl>       <dbl>\n1     167277.     363444."
  },
  {
    "objectID": "slides/lec-8.html#cautions",
    "href": "slides/lec-8.html#cautions",
    "title": "Multiple linear regression (MLR)",
    "section": "Cautions",
    "text": "Cautions\n\nDo not extrapolate! Because there are multiple predictor variables, there is the potential to extrapolate in many directions\nThe multiple regression model only shows association, not causality\n\nTo show causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study"
  },
  {
    "objectID": "slides/lec-8.html#recap",
    "href": "slides/lec-8.html#recap",
    "title": "Multiple linear regression (MLR)",
    "section": "Recap",
    "text": "Recap\n\n\nIntroduced multiple linear regression\nInterpreted a coefficient \\(\\hat{\\beta}_j\\)\nUsed the model to calculate predicted values and the corresponding intervals\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-8.html#annoucement",
    "href": "slides/lec-8.html#annoucement",
    "title": "Multiple linear regression (MLR)",
    "section": "Annoucement",
    "text": "Annoucement\n\nWill be in person on Wednesday IF tested Negative on Monday and Tuesday\nExam 1 due today 11:59pm."
  },
  {
    "objectID": "slides/lec-10.html#announcements",
    "href": "slides/lec-10.html#announcements",
    "title": "Types of predictors",
    "section": "Announcements",
    "text": "Announcements\n\n\nCongratulations on finishing Exam 1!"
  },
  {
    "objectID": "slides/lec-10.html#topics",
    "href": "slides/lec-10.html#topics",
    "title": "Types of predictors",
    "section": "Topics",
    "text": "Topics\n\n\nMean-centering quantitative predictors\nUsing indicator variables for categorical predictors\nUsing interaction terms"
  },
  {
    "objectID": "slides/lec-10.html#computational-setup",
    "href": "slides/lec-10.html#computational-setup",
    "title": "Types of predictors",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(colorblindr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-10.html#data-peer-to-peer-lender",
    "href": "slides/lec-10.html#data-peer-to-peer-lender",
    "title": "Types of predictors",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday‚Äôs data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 √ó 4\n   annual_income debt_to_income verified_income interest_rate\n           <dbl>          <dbl> <fct>                   <dbl>\n 1         59000         0.558  Not Verified            10.9 \n 2         60000         1.31   Not Verified             9.92\n 3         75000         1.06   Verified                26.3 \n 4         75000         0.574  Not Verified             9.92\n 5        254000         0.238  Not Verified             9.43\n 6         67000         1.08   Source Verified          9.92\n 7         28800         0.0997 Source Verified         17.1 \n 8         80000         0.351  Not Verified             6.08\n 9         34000         0.698  Not Verified             7.97\n10         80000         0.167  Source Verified         12.6 \n# ‚Ä¶ with 40 more rows"
  },
  {
    "objectID": "slides/lec-10.html#variables",
    "href": "slides/lec-10.html#variables",
    "title": "Types of predictors",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income: Annual income\ndebt_to_income: Debt-to-income ratio, i.e.¬†the percentage of a borrower‚Äôs total debt divided by their total income\nverified_income: Whether borrower‚Äôs income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nOutcome: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/lec-10.html#outcome-interest_rate",
    "href": "slides/lec-10.html#outcome-interest_rate",
    "title": "Types of predictors",
    "section": "Outcome: interest_rate",
    "text": "Outcome: interest_rate\n\n\n\n\n \n  \n    min \n    median \n    max \n  \n \n\n  \n    5.31 \n    9.93 \n    26.3"
  },
  {
    "objectID": "slides/lec-10.html#predictors",
    "href": "slides/lec-10.html#predictors",
    "title": "Types of predictors",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/lec-10.html#data-manipulation-1-rescale-income",
    "href": "slides/lec-10.html#data-manipulation-1-rescale-income",
    "title": "Types of predictors",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 <- loan50 %>%\n  mutate(annual_income_th = annual_income / 1000)\n\nggplot(loan50, aes(x = annual_income_th)) +\n  geom_histogram(binwidth = 20) +\n  labs(title = \"Annual income (in $1000s)\")"
  },
  {
    "objectID": "slides/lec-10.html#outcome-vs.-predictors",
    "href": "slides/lec-10.html#outcome-vs.-predictors",
    "title": "Types of predictors",
    "section": "Outcome vs.¬†predictors",
    "text": "Outcome vs.¬†predictors"
  },
  {
    "objectID": "slides/lec-10.html#fit-regression-model",
    "href": "slides/lec-10.html#fit-regression-model",
    "title": "Types of predictors",
    "section": "Fit regression model",
    "text": "Fit regression model\n\nint_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(interest_rate ~ debt_to_income + verified_income  + annual_income_th,\n      data = loan50)"
  },
  {
    "objectID": "slides/lec-10.html#summarize-model-results",
    "href": "slides/lec-10.html#summarize-model-results",
    "title": "Types of predictors",
    "section": "Summarize model results",
    "text": "Summarize model results\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n  \n \n\n  \n    (Intercept) \n    10.726 \n    1.507 \n    7.116 \n    0.000 \n    7.690 \n    13.762 \n  \n  \n    debt_to_income \n    0.671 \n    0.676 \n    0.993 \n    0.326 \n    -0.690 \n    2.033 \n  \n  \n    verified_incomeSource Verified \n    2.211 \n    1.399 \n    1.581 \n    0.121 \n    -0.606 \n    5.028 \n  \n  \n    verified_incomeVerified \n    6.880 \n    1.801 \n    3.820 \n    0.000 \n    3.253 \n    10.508 \n  \n  \n    annual_income_th \n    -0.021 \n    0.011 \n    -1.804 \n    0.078 \n    -0.043 \n    0.002 \n  \n\n\n\n\n\n\n\n\nDescribe the subset of borrowers who are expected to get an interest rate of 10.726% based on our model. Is this interpretation meaningful? Why or why not?"
  },
  {
    "objectID": "slides/lec-10.html#mean-centering",
    "href": "slides/lec-10.html#mean-centering",
    "title": "Types of predictors",
    "section": "Mean-centering",
    "text": "Mean-centering\nIf we are interested in interpreting the intercept, we can mean-center the quantitative predictors in the model.\nWe can mean-center a quantitative predictor \\(X_j\\) using the following:\n\\[X_{j_{Cent}} = X_{j}- \\bar{X}_{j}\\]\n\nIf we mean-center all quantitative variables, then the intercept is interpreted as the expected value of the response variable when all quantitative variables are at their mean value."
  },
  {
    "objectID": "slides/lec-10.html#data-manipulation-2-mean-center-numeric-predictors",
    "href": "slides/lec-10.html#data-manipulation-2-mean-center-numeric-predictors",
    "title": "Types of predictors",
    "section": "Data manipulation 2: Mean-center numeric predictors",
    "text": "Data manipulation 2: Mean-center numeric predictors\n\nloan50 <- loan50 %>%\n  mutate(\n    debt_inc_cent = debt_to_income - mean(debt_to_income), \n    annual_income_th_cent = annual_income_th - mean(annual_income_th)\n    )"
  },
  {
    "objectID": "slides/lec-10.html#visualize-mean-centered-predictors",
    "href": "slides/lec-10.html#visualize-mean-centered-predictors",
    "title": "Types of predictors",
    "section": "Visualize mean-centered predictors",
    "text": "Visualize mean-centered predictors"
  },
  {
    "objectID": "slides/lec-10.html#using-mean-centered-variables-in-the-model",
    "href": "slides/lec-10.html#using-mean-centered-variables-in-the-model",
    "title": "Types of predictors",
    "section": "Using mean-centered variables in the model",
    "text": "Using mean-centered variables in the model\n\nHow do you expect the model to change if we use the debt_inc_cent and annual_income_cent in the model?\n\n\n\n\n# A tibble: 5 √ó 7\n  term                  estimate std.error statistic  p.value conf.low conf.high\n  <chr>                    <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)             9.44      0.977      9.66  1.50e-12   7.48    11.4    \n2 debt_inc_cent           0.671     0.676      0.993 3.26e- 1  -0.690    2.03   \n3 verified_incomeSourc‚Ä¶   2.21      1.40       1.58  1.21e- 1  -0.606    5.03   \n4 verified_incomeVerif‚Ä¶   6.88      1.80       3.82  4.06e- 4   3.25    10.5    \n5 annual_income_th_cent  -0.0205    0.0114    -1.80  7.79e- 2  -0.0434   0.00238"
  },
  {
    "objectID": "slides/lec-10.html#original-vs.-mean-centered-model",
    "href": "slides/lec-10.html#original-vs.-mean-centered-model",
    "title": "Types of predictors",
    "section": "Original vs.¬†mean-centered model",
    "text": "Original vs.¬†mean-centered model\n\n\n\n\n\n \n  \n    term \n    estimate \n  \n \n\n  \n    (Intercept) \n    10.726 \n  \n  \n    debt_to_income \n    0.671 \n  \n  \n    verified_incomeSource Verified \n    2.211 \n  \n  \n    verified_incomeVerified \n    6.880 \n  \n  \n    annual_income_th \n    -0.021 \n  \n\n\n\n\n\n\n\n\n\n \n  \n    term \n    estimate \n  \n \n\n  \n    (Intercept) \n    9.444 \n  \n  \n    debt_inc_cent \n    0.671 \n  \n  \n    verified_incomeSource Verified \n    2.211 \n  \n  \n    verified_incomeVerified \n    6.880 \n  \n  \n    annual_income_th_cent \n    -0.021"
  },
  {
    "objectID": "slides/lec-10.html#dummy-variables",
    "href": "slides/lec-10.html#dummy-variables",
    "title": "Types of predictors",
    "section": "Dummy variables",
    "text": "Dummy variables\n\nSuppose there is a categorical variable with \\(K\\) categories (levels)\nWe can treat them as continuous variables, Or\nWe can make \\(K-1\\) indicator variables (by default)\nAn indicator variable takes values 1 or 0\n\n1 if the observation belongs to that category\n0 if the observation does not belong to that category\nall 0s if the observation belong to the benchmark category"
  },
  {
    "objectID": "slides/lec-10.html#indicator-variables-1",
    "href": "slides/lec-10.html#indicator-variables-1",
    "title": "Types of predictors",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nWe can also make \\(K\\) indicator variables - one indicator for each category\nAn indicator variable takes values 1 or 0\n\n1 if the observation belongs to that category\n0 if the observation does not belong to that category\n\nNo intercept in this case"
  },
  {
    "objectID": "slides/lec-10.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "href": "slides/lec-10.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "title": "Types of predictors",
    "section": "Data manipulation 3: Create indicator variables for verified_income",
    "text": "Data manipulation 3: Create indicator variables for verified_income\nSince\n\nloan50 <- loan50 %>%\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )\n\n\n\n\n# A tibble: 3 √ó 4\n  verified_income not_verified source_verified verified\n  <fct>                  <dbl>           <dbl>    <dbl>\n1 Not Verified               1               0        0\n2 Verified                   0               0        1\n3 Source Verified            0               1        0"
  },
  {
    "objectID": "slides/lec-10.html#indicators-in-the-model",
    "href": "slides/lec-10.html#indicators-in-the-model",
    "title": "Types of predictors",
    "section": "Indicators in the model",
    "text": "Indicators in the model\n\nWe will use \\(K-1\\) of the indicator variables in the model.\nThe baseline is the category that doesn‚Äôt have a term in the model.\nThe coefficients of the indicator variables in the model are interpreted as the expected change in the response compared to the baseline, holding all other variables constant.\nThis approach is also called dummy coding.\n\n\n\n\n# A tibble: 3 √ó 3\n  verified_income source_verified verified\n  <fct>                     <dbl>    <dbl>\n1 Not Verified                  0        0\n2 Verified                      0        1\n3 Source Verified               1        0"
  },
  {
    "objectID": "slides/lec-10.html#interpreting-verified_income",
    "href": "slides/lec-10.html#interpreting-verified_income",
    "title": "Types of predictors",
    "section": "Interpreting verified_income",
    "text": "Interpreting verified_income\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n  \n \n\n  \n    (Intercept) \n    9.444 \n    0.977 \n    9.663 \n    0.000 \n    7.476 \n    11.413 \n  \n  \n    debt_inc_cent \n    0.671 \n    0.676 \n    0.993 \n    0.326 \n    -0.690 \n    2.033 \n  \n  \n    verified_incomeSource Verified \n    2.211 \n    1.399 \n    1.581 \n    0.121 \n    -0.606 \n    5.028 \n  \n  \n    verified_incomeVerified \n    6.880 \n    1.801 \n    3.820 \n    0.000 \n    3.253 \n    10.508 \n  \n  \n    annual_income_th_cent \n    -0.021 \n    0.011 \n    -1.804 \n    0.078 \n    -0.043 \n    0.002 \n  \n\n\n\n\n\n\n\nThe baseline category is Not verified.\nPeople with source verified income are expected to take a loan with an interest rate that is 2.211% higher, on average, than the rate on loans to those whose income is not verified, holding all else constant.\nPeople with verified income are expected to take a loan with an interest rate that is 6.880% higher, on average, than the rate on loans to those whose income is not verified, holding all else constant."
  },
  {
    "objectID": "slides/lec-10.html#interaction-terms-1",
    "href": "slides/lec-10.html#interaction-terms-1",
    "title": "Types of predictors",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "slides/lec-10.html#interest-rate-vs.-annual-income",
    "href": "slides/lec-10.html#interest-rate-vs.-annual-income",
    "title": "Types of predictors",
    "section": "Interest rate vs.¬†annual income",
    "text": "Interest rate vs.¬†annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/lec-10.html#interaction-term-in-model",
    "href": "slides/lec-10.html#interaction-term-in-model",
    "title": "Types of predictors",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\nint_cent_int_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(interest_rate ~ debt_inc_cent  +  debt_inc_cent + \n        annual_income_th_cent + verified_income * annual_income_th_cent,\n      data = loan50)\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    9.484 \n    0.989 \n    9.586 \n    0.000 \n  \n  \n    debt_inc_cent \n    0.691 \n    0.685 \n    1.009 \n    0.319 \n  \n  \n    annual_income_th_cent \n    -0.007 \n    0.020 \n    -0.341 \n    0.735 \n  \n  \n    verified_incomeSource Verified \n    2.157 \n    1.418 \n    1.522 \n    0.135 \n  \n  \n    verified_incomeVerified \n    7.181 \n    1.870 \n    3.840 \n    0.000 \n  \n  \n    annual_income_th_cent:verified_incomeSource Verified \n    -0.016 \n    0.026 \n    -0.643 \n    0.523 \n  \n  \n    annual_income_th_cent:verified_incomeVerified \n    -0.032 \n    0.033 \n    -0.979 \n    0.333"
  },
  {
    "objectID": "slides/lec-10.html#interpreting-interaction-terms",
    "href": "slides/lec-10.html#interpreting-interaction-terms",
    "title": "Types of predictors",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the interest rate differs by -0.016 when the income is source verified compared to when it is not verified, holding all else constant.\nInterpreting annual_income for source verified: If the income is source verified, we expect the interest rate to decrease by 0.023% (-0.007 + -0.016) for each additional thousand dollars in annual income, holding all else constant."
  },
  {
    "objectID": "slides/lec-10.html#data-manipulation-4-create-interaction-variables",
    "href": "slides/lec-10.html#data-manipulation-4-create-interaction-variables",
    "title": "Types of predictors",
    "section": "Data manipulation 4: Create interaction variables",
    "text": "Data manipulation 4: Create interaction variables\nDefining the interaction variable in the model formula as verified_income * annual_income_th_cent is an implicit data manipulation step as well\n\n\nRows: 50\nColumns: 9\n$ `(Intercept)`                                          <dbl> 1, 1, 1, 1, 1, ‚Ä¶\n$ debt_inc_cent                                          <dbl> -0.16511719, 0.‚Ä¶\n$ annual_income_th_cent                                  <dbl> -27.17, -26.17,‚Ä¶\n$ `verified_incomeNot Verified`                          <dbl> 1, 1, 0, 1, 1, ‚Ä¶\n$ `verified_incomeSource Verified`                       <dbl> 0, 0, 0, 0, 0, ‚Ä¶\n$ verified_incomeVerified                                <dbl> 0, 0, 1, 0, 0, ‚Ä¶\n$ `annual_income_th_cent:verified_incomeNot Verified`    <dbl> -27.17, -26.17,‚Ä¶\n$ `annual_income_th_cent:verified_incomeSource Verified` <dbl> 0.00, 0.00, 0.0‚Ä¶\n$ `annual_income_th_cent:verified_incomeVerified`        <dbl> 0.00, 0.00, -11‚Ä¶"
  },
  {
    "objectID": "slides/lec-10.html#data-manipulation-5-transformation-on-variables",
    "href": "slides/lec-10.html#data-manipulation-5-transformation-on-variables",
    "title": "Types of predictors",
    "section": "Data manipulation 5: Transformation on variables",
    "text": "Data manipulation 5: Transformation on variables\n\nLinearity is with respect of \\(\\beta\\): \\(y = \\beta_0+\\beta_1 x^2\\) is also a linear regression\nFor right-skewed long tail distributed variable (financial data), log transformation : decrease the variability of data and make data conform more closely to the normal distribution"
  },
  {
    "objectID": "slides/lec-10.html#data-manipulation-5-log-transformation",
    "href": "slides/lec-10.html#data-manipulation-5-log-transformation",
    "title": "Types of predictors",
    "section": "Data manipulation 5: log transformation",
    "text": "Data manipulation 5: log transformation"
  },
  {
    "objectID": "slides/lec-10.html#interpretation-on-log-scale",
    "href": "slides/lec-10.html#interpretation-on-log-scale",
    "title": "Types of predictors",
    "section": "Interpretation on log-scale",
    "text": "Interpretation on log-scale\nAs annual income increase by 10% (\\(\\log(x+0.1x)=\\log(1.1\\times x)=log(1.1)+\\log x\\)), the interest rate is expected to increase by \\(\\beta \\times \\log(1.1)\\) on average, hold ‚Ä¶ constant.\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    35.144 \n    14.200 \n    2.475 \n    0.017 \n  \n  \n    debt_to_income \n    0.725 \n    0.671 \n    1.081 \n    0.286 \n  \n  \n    verified_incomeSource Verified \n    2.140 \n    1.397 \n    1.532 \n    0.133 \n  \n  \n    verified_incomeVerified \n    7.032 \n    1.809 \n    3.888 \n    0.000 \n  \n  \n    annual_income_log \n    -2.338 \n    1.260 \n    -1.855 \n    0.070"
  },
  {
    "objectID": "slides/lec-10.html#recap",
    "href": "slides/lec-10.html#recap",
    "title": "Types of predictors",
    "section": "Recap",
    "text": "Recap\n\nMean-centering quantitative predictors\nUsing indicator variables for categorical predictors\nUsing interaction terms"
  },
  {
    "objectID": "slides/lec-10.html#looking-backward",
    "href": "slides/lec-10.html#looking-backward",
    "title": "Types of predictors",
    "section": "Looking backward",
    "text": "Looking backward\nData manipulation, with dplyr (from tidyverse):\n\nloan50 %>%\n  select(interest_rate, annual_income, debt_to_income, verified_income) %>%\n  mutate(\n    # 1. rescale income\n    annual_income_th = annual_income / 1000,\n    # 2. mean-center quantitative predictors\n    debt_inc_cent = debt_to_income - mean(debt_to_income),\n    annual_income_th_cent = annual_income_th - mean(annual_income_th),\n    # 3. create dummy variables for verified_income\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0),\n    # 4. create interaction variables\n    `annual_income_th_cent:verified_incomeSource Verified` = annual_income_th_cent * source_verified,\n    `annual_income_th_cent:verified_incomeVerified` = annual_income_th_cent * verified\n  )"
  },
  {
    "objectID": "slides/lec-10.html#looking-forward",
    "href": "slides/lec-10.html#looking-forward",
    "title": "Types of predictors",
    "section": "Looking forward",
    "text": "Looking forward\nFeature engineering, with recipes (from tidymodels):\n\nloan_rec <- recipe( ~ ., data = loan50) %>%\n  # 1. rescale income\n  step_mutate(annual_income_th = annual_income / 1000) %>%\n  # 2. mean-center quantitative predictors\n  step_center(all_numeric_predictors()) %>%\n  # 3. create dummy variables for verified_income\n  step_dummy(verified_income) %>%\n  # 4. create interaction variables\n  step_interact(terms = ~ annual_income_th:verified_income)"
  },
  {
    "objectID": "slides/lec-10.html#recipe",
    "href": "slides/lec-10.html#recipe",
    "title": "Types of predictors",
    "section": "Recipe",
    "text": "Recipe\n\nloan_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n predictor         25\n\nOperations:\n\nVariable mutation for annual_income / 1000\nCentering for all_numeric_predictors()\nDummy variables from verified_income\nInteractions with annual_income_th:verified_income\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-11.html#announcements",
    "href": "slides/lec-11.html#announcements",
    "title": "Model comparison",
    "section": "Announcements",
    "text": "Announcements\n\n\nHW 2 posted today, due Sunday\nLab 3 posted today, due Friday"
  },
  {
    "objectID": "slides/lec-11.html#review-on-exam-1-part-1",
    "href": "slides/lec-11.html#review-on-exam-1-part-1",
    "title": "Model comparison",
    "section": "Review on Exam 1 Part 1",
    "text": "Review on Exam 1 Part 1\nExam 1 Part 1"
  },
  {
    "objectID": "slides/lec-11.html#interpretation-on-coefficients",
    "href": "slides/lec-11.html#interpretation-on-coefficients",
    "title": "Model comparison",
    "section": "Interpretation on coefficients",
    "text": "Interpretation on coefficients\nNotes"
  },
  {
    "objectID": "slides/lec-11.html#topics",
    "href": "slides/lec-11.html#topics",
    "title": "Model comparison",
    "section": "Topics",
    "text": "Topics\n\n\nANOVA for Multiple Linear Regression and sum of squares\nComparing models with \\(R^2\\) vs.¬†\\(R^2_{adj}\\)\nComparing models with AIC and BIC\nOccam‚Äôs razor and parsimony\nOverfitting and spending our data"
  },
  {
    "objectID": "slides/lec-11.html#computational-setup",
    "href": "slides/lec-11.html#computational-setup",
    "title": "Model comparison",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-11.html#data-restaurant-tips",
    "href": "slides/lec-11.html#data-restaurant-tips",
    "title": "Model comparison",
    "section": "Data: Restaurant tips",
    "text": "Data: Restaurant tips\nWhich variables help us predict the amount customers tip at a restaurant?\n\n\n\n\n\n# A tibble: 169 √ó 4\n     Tip Party Meal   Age   \n   <dbl> <dbl> <chr>  <chr> \n 1  2.99     1 Dinner Yadult\n 2  2        1 Dinner Yadult\n 3  5        1 Dinner SenCit\n 4  4        3 Dinner Middle\n 5 10.3      2 Dinner SenCit\n 6  4.85     2 Dinner Middle\n 7  5        4 Dinner Yadult\n 8  4        3 Dinner Middle\n 9  5        2 Dinner Middle\n10  1.58     1 Dinner SenCit\n# ‚Ä¶ with 159 more rows"
  },
  {
    "objectID": "slides/lec-11.html#variables",
    "href": "slides/lec-11.html#variables",
    "title": "Model comparison",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nParty: Number of people in the party\nMeal: Time of day (Lunch, Dinner, Late Night)\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)\n\n\nOutcome: Tip: Amount of tip"
  },
  {
    "objectID": "slides/lec-11.html#outcome-tip",
    "href": "slides/lec-11.html#outcome-tip",
    "title": "Model comparison",
    "section": "Outcome: Tip",
    "text": "Outcome: Tip"
  },
  {
    "objectID": "slides/lec-11.html#predictors",
    "href": "slides/lec-11.html#predictors",
    "title": "Model comparison",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/lec-11.html#relevel-categorical-predictors",
    "href": "slides/lec-11.html#relevel-categorical-predictors",
    "title": "Model comparison",
    "section": "Relevel categorical predictors",
    "text": "Relevel categorical predictors\n\ntips <- tips %>%\n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )"
  },
  {
    "objectID": "slides/lec-11.html#predictors-again",
    "href": "slides/lec-11.html#predictors-again",
    "title": "Model comparison",
    "section": "Predictors, again",
    "text": "Predictors, again"
  },
  {
    "objectID": "slides/lec-11.html#outcome-vs.-predictors",
    "href": "slides/lec-11.html#outcome-vs.-predictors",
    "title": "Model comparison",
    "section": "Outcome vs.¬†predictors",
    "text": "Outcome vs.¬†predictors"
  },
  {
    "objectID": "slides/lec-11.html#fit-and-summarize-model",
    "href": "slides/lec-11.html#fit-and-summarize-model",
    "title": "Model comparison",
    "section": "Fit and summarize model",
    "text": "Fit and summarize model\n\ntip_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + Age, data = tips)\n\ntidy(tip_fit, conf.int = TRUE) %>%\n  kable(digits = 3)\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n  \n \n\n  \n    (Intercept) \n    -0.170 \n    0.366 \n    -0.465 \n    0.643 \n    -0.893 \n    0.553 \n  \n  \n    Party \n    1.837 \n    0.124 \n    14.758 \n    0.000 \n    1.591 \n    2.083 \n  \n  \n    AgeMiddle \n    1.009 \n    0.408 \n    2.475 \n    0.014 \n    0.204 \n    1.813 \n  \n  \n    AgeSenCit \n    1.388 \n    0.485 \n    2.862 \n    0.005 \n    0.430 \n    2.345 \n  \n\n\n\n\n\n\n\n\nIs this the best model to explain variation in tips?"
  },
  {
    "objectID": "slides/lec-11.html#another-model-summary",
    "href": "slides/lec-11.html#another-model-summary",
    "title": "Model comparison",
    "section": "Another model summary",
    "text": "Another model summary\n\n\n\n \n  \n    term \n    df \n    sumsq \n    meansq \n    statistic \n    p.value \n  \n \n\n  \n    Party \n    1 \n    1188.64 \n    1188.64 \n    285.71 \n    0.00 \n  \n  \n    Age \n    2 \n    38.03 \n    19.01 \n    4.57 \n    0.01 \n  \n  \n    Residuals \n    165 \n    686.44 \n    4.16 \n    NA \n    NA"
  },
  {
    "objectID": "slides/lec-11.html#analysis-of-variance-anova-1",
    "href": "slides/lec-11.html#analysis-of-variance-anova-1",
    "title": "Model comparison",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)\n\nMain Idea: Decompose the total variation on the outcome into:\n\nthe variation that can be explained by the each of the variables in the model\nthe variation that can‚Äôt be explained by the model (left in the residuals)\n\nIf the variation that can be explained by the variables in the model is greater than the variation in the residuals, this signals that the model might be ‚Äúvaluable‚Äù (at least one of the \\(\\beta\\)s not equal to 0)"
  },
  {
    "objectID": "slides/lec-11.html#anova-output",
    "href": "slides/lec-11.html#anova-output",
    "title": "Model comparison",
    "section": "ANOVA output",
    "text": "ANOVA output\n\nanova(tip_fit$fit) %>%\n  tidy() %>%\n  kable(digits = 2)\n\n\n \n  \n    term \n    df \n    sumsq \n    meansq \n    statistic \n    p.value \n  \n \n\n  \n    Party \n    1 \n    1188.64 \n    1188.64 \n    285.71 \n    0.00 \n  \n  \n    Age \n    2 \n    38.03 \n    19.01 \n    4.57 \n    0.01 \n  \n  \n    Residuals \n    165 \n    686.44 \n    4.16 \n    NA \n    NA"
  },
  {
    "objectID": "slides/lec-11.html#anova-output-with-totals",
    "href": "slides/lec-11.html#anova-output-with-totals",
    "title": "Model comparison",
    "section": "ANOVA output, with totals",
    "text": "ANOVA output, with totals\n\n\n\n \n  \n    term \n    df \n    sumsq \n    meansq \n    statistic \n    p.value \n  \n \n\n  \n    Party \n    1 \n    1188.64 \n    1188.64 \n    285.71 \n    0 \n  \n  \n    Age \n    2 \n    38.03 \n    19.01 \n    4.57 \n    0.01 \n  \n  \n    Residuals \n    165 \n    686.44 \n    4.16 \n     \n     \n  \n  \n    Total \n    168 \n    1913.11"
  },
  {
    "objectID": "slides/lec-11.html#sum-of-squares",
    "href": "slides/lec-11.html#sum-of-squares",
    "title": "Model comparison",
    "section": "Sum of squares",
    "text": "Sum of squares\n\n\n\n\n\n \n  \n    term \n    df \n    sumsq \n  \n \n\n  \n    Party \n    1 \n    1188.64 \n  \n  \n    Age \n    2 \n    38.03 \n  \n  \n    Residuals \n    165 \n    686.44 \n  \n  \n    Total \n    168 \n    1913.11 \n  \n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome, \\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\)\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals, \\(\\sum_{i = 1}^n (y_i - \\hat{y})^2\\)\n\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained by the model"
  },
  {
    "objectID": "slides/lec-11.html#r-squared-r2",
    "href": "slides/lec-11.html#r-squared-r2",
    "title": "Model comparison",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\nRecall: \\(R^2\\) is the proportion of the variation in the response variable explained by the regression model.\n\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}} = 1 - \\frac{686.44}{1913.11} = 0.641\n\\]\n\n\n\nglance(tip_fit)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.641         0.635  2.04      98.3 1.56e-36     3  -358.  726.  742.\n# ‚Ä¶ with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>"
  },
  {
    "objectID": "slides/lec-11.html#r-squared-r2-1",
    "href": "slides/lec-11.html#r-squared-r2-1",
    "title": "Model comparison",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\n\n\\(R^2\\) will always increase as we add more variables to the model + If we add enough variables, we can always achieve \\(R^2=100\\%\\)\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choose the model with the most predictor variables"
  },
  {
    "objectID": "slides/lec-11.html#adjusted-r2",
    "href": "slides/lec-11.html#adjusted-r2",
    "title": "Model comparison",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\): measure that includes a penalty for unnecessary predictor variables\nSimilar to \\(R^2\\), it is a measure of the amount of variation in the response that is explained by the regression model\nDiffers from \\(R^2\\) by using the mean squares rather than sums of squares and therefore adjusting for the number of predictor variables"
  },
  {
    "objectID": "slides/lec-11.html#r2-and-adjusted-r2",
    "href": "slides/lec-11.html#r2-and-adjusted-r2",
    "title": "Model comparison",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}\\]\n\n\\[R^2_{adj} = 1 - \\frac{SS_{Error}/(n-p-1)}{SS_{Total}/(n-1)}\\]"
  },
  {
    "objectID": "slides/lec-11.html#using-r2-and-adjusted-r2",
    "href": "slides/lec-11.html#using-r2-and-adjusted-r2",
    "title": "Model comparison",
    "section": "Using \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "Using \\(R^2\\) and Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\nUse \\(R^2\\) when describing the relationship between the response and predictor variables"
  },
  {
    "objectID": "slides/lec-11.html#comparing-models-with-r2_adj",
    "href": "slides/lec-11.html#comparing-models-with-r2_adj",
    "title": "Model comparison",
    "section": "Comparing models with \\(R^2_{adj}\\)",
    "text": "Comparing models with \\(R^2_{adj}\\)\n\n\n\ntip_fit_1 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n      data = tips)\n\nglance(tip_fit_1) %>% \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 √ó 2\n  r.squared adj.r.squared\n      <dbl>         <dbl>\n1     0.674         0.664\n\n\n\n\ntip_fit_2 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) %>% \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 √ó 2\n  r.squared adj.r.squared\n      <dbl>         <dbl>\n1     0.683         0.662"
  },
  {
    "objectID": "slides/lec-11.html#aic-bic",
    "href": "slides/lec-11.html#aic-bic",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\nEstimators of prediction error and relative quality of models:\n\nAkaike‚Äôs Information Criterion (AIC): \\[AIC = n\\log(SS_\\text{Error}) - n \\log(n) + 2(p+1)\\] \n\n\nSchwarz‚Äôs Bayesian Information Criterion (BIC): \\[BIC = n\\log(SS_\\text{Error}) - n\\log(n) + log(n)\\times(p+1)\\]"
  },
  {
    "objectID": "slides/lec-11.html#aic-bic-1",
    "href": "slides/lec-11.html#aic-bic-1",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = \\color{blue}{n\\log(SS_\\text{Error})} - n \\log(n) + 2(p+1) \\\\\n& BIC = \\color{blue}{n\\log(SS_\\text{Error})} - n\\log(n) + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\n\nFirst Term: Decreases as p increases"
  },
  {
    "objectID": "slides/lec-11.html#aic-bic-2",
    "href": "slides/lec-11.html#aic-bic-2",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = n\\log(SS_\\text{Error}) - \\color{blue}{n \\log(n)} + 2(p+1) \\\\\n& BIC = n\\log(SS_\\text{Error}) - \\color{blue}{n\\log(n)} + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\nSecond Term: Fixed for a given sample size n"
  },
  {
    "objectID": "slides/lec-11.html#aic-bic-3",
    "href": "slides/lec-11.html#aic-bic-3",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{2(p+1)} \\\\\n& BIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nThird Term: Increases as p increases"
  },
  {
    "objectID": "slides/lec-11.html#using-aic-bic",
    "href": "slides/lec-11.html#using-aic-bic",
    "title": "Model comparison",
    "section": "Using AIC & BIC",
    "text": "Using AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_{Error}) - n \\log(n) + \\color{red}{2(p+1)} \\\\\n& BIC = n\\log(SS_{Error}) - n\\log(n) + \\color{red}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nChoose model with the smaller value of AIC or BIC\nIf \\(n \\geq 8\\), the penalty for BIC is larger than that of AIC, so BIC tends to favor more parsimonious models (i.e.¬†models with fewer terms)"
  },
  {
    "objectID": "slides/lec-11.html#comparing-models-with-aic-and-bic",
    "href": "slides/lec-11.html#comparing-models-with-aic-and-bic",
    "title": "Model comparison",
    "section": "Comparing models with AIC and BIC",
    "text": "Comparing models with AIC and BIC\n\n\n\ntip_fit_1 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n      data = tips)\n\nglance(tip_fit_1) %>% \n  select(AIC, BIC)\n\n# A tibble: 1 √ó 2\n    AIC   BIC\n  <dbl> <dbl>\n1  714.  736.\n\n\n\n\ntip_fit_2 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) %>% \n  select(AIC, BIC)\n\n# A tibble: 1 √ó 2\n    AIC   BIC\n  <dbl> <dbl>\n1  720.  757."
  },
  {
    "objectID": "slides/lec-11.html#commonalities-between-criteria",
    "href": "slides/lec-11.html#commonalities-between-criteria",
    "title": "Model comparison",
    "section": "Commonalities between criteria",
    "text": "Commonalities between criteria\n\n\\(R^2_{adj}\\), AIC, and BIC all apply a penalty for more predictors\nThe penalty for added model complexity attempts to strike a balance between underfitting (too few predictors in the model) and overfitting (too many predictors in the model)\nGoal: Parsimony"
  },
  {
    "objectID": "slides/lec-11.html#parsimony-and-occams-razor",
    "href": "slides/lec-11.html#parsimony-and-occams-razor",
    "title": "Model comparison",
    "section": "Parsimony and Occam‚Äôs razor",
    "text": "Parsimony and Occam‚Äôs razor\n\nThe principle of parsimony is attributed to William of Occam (early 14th-century English nominalist philosopher), who insisted that, given a set of equally good explanations for a given phenomenon, the correct explanation is the simplest explanation1\nCalled Occam‚Äôs razor because he ‚Äúshaved‚Äù his explanations down to the bare minimum\nParsimony in modeling:\n\n\nmodels should have as few parameters as possible\nlinear models should be preferred to non-linear models\nexperiments relying on few assumptions should be preferred to those relying on many\nmodels should be pared down until they are minimal adequate\nsimple explanations should be preferred to complex explanations\n\n\n\nSource: The R Book by Michael J. Crawley."
  },
  {
    "objectID": "slides/lec-11.html#in-pursuit-of-occams-razor",
    "href": "slides/lec-11.html#in-pursuit-of-occams-razor",
    "title": "Model comparison",
    "section": "In pursuit of Occam‚Äôs razor",
    "text": "In pursuit of Occam‚Äôs razor\n\nOccam‚Äôs razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected\nModel selection follows this principle\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model\nIn other words, we prefer the simplest best model, i.e.¬†parsimonious model"
  },
  {
    "objectID": "slides/lec-11.html#alternate-views",
    "href": "slides/lec-11.html#alternate-views",
    "title": "Model comparison",
    "section": "Alternate views",
    "text": "Alternate views\n\nSometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n\nRadford Neal - Bayesian Learning for Neural Networks1\n\nSuggested blog post: Occam by Andrew Gelman"
  },
  {
    "objectID": "slides/lec-11.html#other-concerns-with-our-approach",
    "href": "slides/lec-11.html#other-concerns-with-our-approach",
    "title": "Model comparison",
    "section": "Other concerns with our approach",
    "text": "Other concerns with our approach\n\nAll criteria we considered for model comparison require making predictions for our data and then uses the prediction error (\\(SS_{Error}\\)) somewhere in the formula\nBut we‚Äôre making prediction for the data we used to build the model (estimate the coefficients), which can lead to overfitting\nInstead we should\n\nsplit our data into testing and training sets\n‚Äútrain‚Äù the model on the training data and pick a few models we‚Äôre genuinely considering as potentially good models\ntest those models on the testing set"
  },
  {
    "objectID": "slides/lec-11.html#recap",
    "href": "slides/lec-11.html#recap",
    "title": "Model comparison",
    "section": "Recap",
    "text": "Recap\n\nANOVA for Multiple Linear Regression and sum of squares\nComparing models with \\(R^2\\) vs.¬†\\(R^2_{adj}\\)\nComparing models with AIC and BIC\nOccam‚Äôs razor and parsimony\nOverfitting and spending our data\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-13.html#announcements",
    "href": "slides/lec-13.html#announcements",
    "title": "Feature engineering",
    "section": "Announcements",
    "text": "Announcements\n\n\nCheck your grades for exam-1\nGo through conceptual questions in homework 2"
  },
  {
    "objectID": "slides/lec-13.html#topics",
    "href": "slides/lec-13.html#topics",
    "title": "Feature engineering",
    "section": "Topics",
    "text": "Topics\n\nrecipes:\n\nSplitting training set and testing set\nSpecify the recipes (predictors, outcome)\nsteps within recipes (feature engineering)\nWorkflows to bring together models and recipes\nCook: Fit model and prediction\nTaste it : RMSE and \\(R^2\\) for model evaluation\nImprove it: Cross validation"
  },
  {
    "objectID": "slides/lec-13.html#computational-setup",
    "href": "slides/lec-13.html#computational-setup",
    "title": "Feature engineering",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-13.html#the-office",
    "href": "slides/lec-13.html#the-office",
    "title": "Feature engineering",
    "section": "The Office",
    "text": "The Office"
  },
  {
    "objectID": "slides/lec-13.html#data",
    "href": "slides/lec-13.html#data",
    "title": "Feature engineering",
    "section": "Data",
    "text": "Data\nThe data come from data.world, by way of TidyTuesday\n\noffice_ratings <- read_csv(here::here(\"slides\", \"data/office_ratings.csv\"))\noffice_ratings\n\n# A tibble: 188 √ó 6\n   season episode title             imdb_rating total_votes air_date  \n    <dbl>   <dbl> <chr>                   <dbl>       <dbl> <date>    \n 1      1       1 Pilot                     7.6        3706 2005-03-24\n 2      1       2 Diversity Day             8.3        3566 2005-03-29\n 3      1       3 Health Care               7.9        2983 2005-04-05\n 4      1       4 The Alliance              8.1        2886 2005-04-12\n 5      1       5 Basketball                8.4        3179 2005-04-19\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26\n 7      2       1 The Dundies               8.7        3213 2005-09-20\n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 9      2       3 Office Olympics           8.4        2742 2005-10-04\n10      2       4 The Fire                  8.4        2713 2005-10-11\n# ‚Ä¶ with 178 more rows"
  },
  {
    "objectID": "slides/lec-13.html#imdb-ratings",
    "href": "slides/lec-13.html#imdb-ratings",
    "title": "Feature engineering",
    "section": "IMDB ratings",
    "text": "IMDB ratings"
  },
  {
    "objectID": "slides/lec-13.html#imdb-ratings-vs.-number-of-votes",
    "href": "slides/lec-13.html#imdb-ratings-vs.-number-of-votes",
    "title": "Feature engineering",
    "section": "IMDB ratings vs.¬†number of votes",
    "text": "IMDB ratings vs.¬†number of votes"
  },
  {
    "objectID": "slides/lec-13.html#outliers",
    "href": "slides/lec-13.html#outliers",
    "title": "Feature engineering",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/lec-13.html#rating-vs.-air-date",
    "href": "slides/lec-13.html#rating-vs.-air-date",
    "title": "Feature engineering",
    "section": "Rating vs.¬†air date",
    "text": "Rating vs.¬†air date"
  },
  {
    "objectID": "slides/lec-13.html#imdb-ratings-vs.-seasons",
    "href": "slides/lec-13.html#imdb-ratings-vs.-seasons",
    "title": "Feature engineering",
    "section": "IMDB ratings vs.¬†seasons",
    "text": "IMDB ratings vs.¬†seasons"
  },
  {
    "objectID": "slides/lec-13.html#train-test",
    "href": "slides/lec-13.html#train-test",
    "title": "Feature engineering",
    "section": "Train / test",
    "text": "Train / test\nStep 1: Create an initial split:\n\nset.seed(123)\noffice_split <- initial_split(office_ratings) # prop = 3/4 by default\n\n\n\nStep 2: Save training data\n\noffice_train <- training(office_split)\ndim(office_train)\n\n[1] 141   6\n\n\n\n\n\nStep 3: Save testing data\n\noffice_test  <- testing(office_split)\ndim(office_test)\n\n[1] 47  6"
  },
  {
    "objectID": "slides/lec-13.html#training-data",
    "href": "slides/lec-13.html#training-data",
    "title": "Feature engineering",
    "section": "Training data",
    "text": "Training data\n\noffice_train\n\n# A tibble: 141 √ó 6\n   season episode title               imdb_rating total_votes air_date  \n    <dbl>   <dbl> <chr>                     <dbl>       <dbl> <date>    \n 1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2      9      14 Vandalism                   7.6        1402 2013-01-31\n 3      2       8 Performance Review          8.2        2416 2005-11-15\n 4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5      3      22 Beach Games                 9.1        2783 2007-05-10\n 6      7       1 Nepotism                    8.4        1897 2010-09-23\n 7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9      9      18 Promos                      8          1445 2013-04-04\n10      8      12 Pool Party                  8          1612 2012-01-19\n# ‚Ä¶ with 131 more rows"
  },
  {
    "objectID": "slides/lec-13.html#feature-engineering",
    "href": "slides/lec-13.html#feature-engineering",
    "title": "Feature engineering",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nWe prefer simple models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)\nVariable selection (parsimony) + Variable representation (feature engineering)"
  },
  {
    "objectID": "slides/lec-13.html#feature-engineering-date",
    "href": "slides/lec-13.html#feature-engineering-date",
    "title": "Feature engineering",
    "section": "Feature engineering : date",
    "text": "Feature engineering : date\nPeriodic: month or weekday"
  },
  {
    "objectID": "slides/lec-13.html#feature-engineering-with-dplyr",
    "href": "slides/lec-13.html#feature-engineering-with-dplyr",
    "title": "Feature engineering",
    "section": "Feature engineering with dplyr",
    "text": "Feature engineering with dplyr\n\n\n\n\noffice_train %>%\n  mutate(\n    season = as_factor(season),\n    month = lubridate::month(air_date),\n    wday = lubridate::wday(air_date)\n  )\n\n# A tibble: 141 √ó 8\n  season episode title            imdb_rating total_votes air_date   month  wday\n  <fct>    <dbl> <chr>                  <dbl>       <dbl> <date>     <dbl> <dbl>\n1 8           18 Last Day in Flo‚Ä¶         7.8        1429 2012-03-08     3     5\n2 9           14 Vandalism                7.6        1402 2013-01-31     1     5\n3 2            8 Performance Rev‚Ä¶         8.2        2416 2005-11-15    11     3\n4 9            5 Here Comes Treb‚Ä¶         7.1        1515 2012-10-25    10     5\n5 3           22 Beach Games              9.1        2783 2007-05-10     5     5\n6 7            1 Nepotism                 8.4        1897 2010-09-23     9     5\n# ‚Ä¶ with 135 more rows\n\n\n\n\nCan you identify any potential problems with this approach?\n\nHolidays"
  },
  {
    "objectID": "slides/lec-13.html#modeling-workflow",
    "href": "slides/lec-13.html#modeling-workflow",
    "title": "Feature engineering",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/lec-13.html#initiate-a-recipe",
    "href": "slides/lec-13.html#initiate-a-recipe",
    "title": "Feature engineering",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\noffice_rec <- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloguing names and types of variables\n  )\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          5"
  },
  {
    "objectID": "slides/lec-13.html#step-1-alter-roles",
    "href": "slides/lec-13.html#step-1-alter-roles",
    "title": "Feature engineering",
    "section": "Step 1: Alter roles",
    "text": "Step 1: Alter roles\ntitle isn‚Äôt a predictor, but we might want to keep it around as an ID\n\noffice_rec <- office_rec %>%\n  update_role(title, new_role = \"ID\")\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4"
  },
  {
    "objectID": "slides/lec-13.html#step-2-add-features",
    "href": "slides/lec-13.html#step-2-add-features",
    "title": "Feature engineering",
    "section": "Step 2: Add features",
    "text": "Step 2: Add features\nNew features for day of week and month. step_date creates a specification of a recipe step that will convert date data into one or more factor or numeric variables.\n\noffice_rec <- office_rec %>%\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date"
  },
  {
    "objectID": "slides/lec-13.html#step-3-add-more-features",
    "href": "slides/lec-13.html#step-3-add-more-features",
    "title": "Feature engineering",
    "section": "Step 3: Add more features",
    "text": "Step 3: Add more features\nIdentify holidays in air_date, then remove air_date\n\noffice_rec <- office_rec %>%\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date"
  },
  {
    "objectID": "slides/lec-13.html#step-4-convert-numbers-to-factors",
    "href": "slides/lec-13.html#step-4-convert-numbers-to-factors",
    "title": "Feature engineering",
    "section": "Step 4: Convert numbers to factors",
    "text": "Step 4: Convert numbers to factors\nConvert season to factor\n\noffice_rec <- office_rec %>%\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season"
  },
  {
    "objectID": "slides/lec-13.html#step-5-make-dummy-variables",
    "href": "slides/lec-13.html#step-5-make-dummy-variables",
    "title": "Feature engineering",
    "section": "Step 5: Make dummy variables",
    "text": "Step 5: Make dummy variables\nConvert all nominal (categorical) predictors to factors\n\noffice_rec <- office_rec %>%\n  step_dummy(all_nominal_predictors())\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()"
  },
  {
    "objectID": "slides/lec-13.html#step-6-remove-zero-variance-pred.s",
    "href": "slides/lec-13.html#step-6-remove-zero-variance-pred.s",
    "title": "Feature engineering",
    "section": "Step 6: Remove zero variance pred.s",
    "text": "Step 6: Remove zero variance pred.s\nRemove all predictors that contain only a single value\n\noffice_rec <- office_rec %>%\n  step_zv(all_predictors())\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/lec-13.html#steps-that-may-be-useful",
    "href": "slides/lec-13.html#steps-that-may-be-useful",
    "title": "Feature engineering",
    "section": "Steps that may be useful",
    "text": "Steps that may be useful\n\nImpute data if there is missing data in predictors:\n\nstep_impute_knn(all_predictors())\n\nIf intercept is not meaningful and want to center predictors:\n\nstep_center(all_numeric_predictors())\n\nCheck other possible steps: recipes"
  },
  {
    "objectID": "slides/lec-13.html#putting-it-altogether",
    "href": "slides/lec-13.html#putting-it-altogether",
    "title": "Feature engineering",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\noffice_rec <- recipe(imdb_rating ~ ., data = office_train) %>%\n  # make title's role ID\n  update_role(title, new_role = \"ID\") %>%\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) %>%\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) %>%\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) %>%\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) %>%\n  # remove zero variance predictors\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/lec-13.html#putting-it-altogether-1",
    "href": "slides/lec-13.html#putting-it-altogether-1",
    "title": "Feature engineering",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/lec-13.html#specify-model",
    "href": "slides/lec-13.html#specify-model",
    "title": "Feature engineering",
    "section": "Specify model",
    "text": "Specify model\n\noffice_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\noffice_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-13.html#build-workflow",
    "href": "slides/lec-13.html#build-workflow",
    "title": "Feature engineering",
    "section": "Build workflow",
    "text": "Build workflow\nWorkflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\noffice_wflow <- workflow() %>%\n  add_model(office_spec) %>%\n  add_recipe(office_rec)"
  },
  {
    "objectID": "slides/lec-13.html#view-workflow",
    "href": "slides/lec-13.html#view-workflow",
    "title": "Feature engineering",
    "section": "View workflow",
    "text": "View workflow\n\noffice_wflow\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: linear_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n5 Recipe Steps\n\n‚Ä¢ step_date()\n‚Ä¢ step_holiday()\n‚Ä¢ step_num2factor()\n‚Ä¢ step_dummy()\n‚Ä¢ step_zv()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-13.html#fit-model-to-training-data",
    "href": "slides/lec-13.html#fit-model-to-training-data",
    "title": "Feature engineering",
    "section": "Fit model to training data",
    "text": "Fit model to training data\nCold Call !\n\noffice_fit <- office_wflow %>%\n  fit(data = office_train)\n\ntidy(office_fit)\n\n# A tibble: 21 √ó 5\n   term         estimate std.error statistic  p.value\n   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)  6.40     0.510        12.5   1.51e-23\n 2 episode     -0.00393  0.0171       -0.230 8.18e- 1\n 3 total_votes  0.000375 0.0000414     9.07  2.75e-15\n 4 season_X2    0.811    0.327         2.48  1.44e- 2\n 5 season_X3    1.04     0.343         3.04  2.91e- 3\n 6 season_X4    1.09     0.295         3.70  3.32e- 4\n 7 season_X5    1.08     0.348         3.11  2.34e- 3\n 8 season_X6    1.00     0.367         2.74  7.18e- 3\n 9 season_X7    1.02     0.352         2.89  4.52e- 3\n10 season_X8    0.497    0.348         1.43  1.55e- 1\n# ‚Ä¶ with 11 more rows\n\n\n\n\nSo many predictors!"
  },
  {
    "objectID": "slides/lec-13.html#model-fit-summary",
    "href": "slides/lec-13.html#model-fit-summary",
    "title": "Feature engineering",
    "section": "Model fit summary",
    "text": "Model fit summary\nPrint all the predictors w/o folding any\n\ntidy(office_fit) %>% print(n = 21)\n\n# A tibble: 21 √ó 5\n   term                estimate std.error statistic  p.value\n   <chr>                  <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)         6.40     0.510        12.5   1.51e-23\n 2 episode            -0.00393  0.0171       -0.230 8.18e- 1\n 3 total_votes         0.000375 0.0000414     9.07  2.75e-15\n 4 season_X2           0.811    0.327         2.48  1.44e- 2\n 5 season_X3           1.04     0.343         3.04  2.91e- 3\n 6 season_X4           1.09     0.295         3.70  3.32e- 4\n 7 season_X5           1.08     0.348         3.11  2.34e- 3\n 8 season_X6           1.00     0.367         2.74  7.18e- 3\n 9 season_X7           1.02     0.352         2.89  4.52e- 3\n10 season_X8           0.497    0.348         1.43  1.55e- 1\n11 season_X9           0.621    0.345         1.80  7.41e- 2\n12 air_date_dow_Tue    0.382    0.422         0.904 3.68e- 1\n13 air_date_dow_Thu    0.284    0.389         0.731 4.66e- 1\n14 air_date_month_Feb -0.0597   0.132        -0.452 6.52e- 1\n15 air_date_month_Mar -0.0752   0.156        -0.481 6.31e- 1\n16 air_date_month_Apr  0.0954   0.177         0.539 5.91e- 1\n17 air_date_month_May  0.156    0.213         0.734 4.64e- 1\n18 air_date_month_Sep -0.0776   0.223        -0.348 7.28e- 1\n19 air_date_month_Oct -0.176    0.174        -1.01  3.13e- 1\n20 air_date_month_Nov -0.156    0.149        -1.05  2.98e- 1\n21 air_date_month_Dec  0.170    0.149         1.14  2.55e- 1"
  },
  {
    "objectID": "slides/lec-13.html#make-predictions-for-training-data",
    "href": "slides/lec-13.html#make-predictions-for-training-data",
    "title": "Feature engineering",
    "section": "Make predictions for training data",
    "text": "Make predictions for training data\n\noffice_train_pred <- predict(office_fit, office_train) %>%\n  bind_cols(office_train %>% select(imdb_rating, title))\n\noffice_train_pred\n\n# A tibble: 141 √ó 3\n   .pred imdb_rating title              \n   <dbl>       <dbl> <chr>              \n 1  7.57         7.8 Last Day in Florida\n 2  7.77         7.6 Vandalism          \n 3  8.31         8.2 Performance Review \n 4  7.67         7.1 Here Comes Treble  \n 5  8.84         9.1 Beach Games        \n 6  8.33         8.4 Nepotism           \n 7  8.46         8.3 Phyllis' Wedding   \n 8  8.14         8.9 Livin' the Dream   \n 9  7.87         8   Promos             \n10  7.74         8   Pool Party         \n# ‚Ä¶ with 131 more rows"
  },
  {
    "objectID": "slides/lec-13.html#r-squared",
    "href": "slides/lec-13.html#r-squared",
    "title": "Feature engineering",
    "section": "R-squared",
    "text": "R-squared\nPercentage of variability in the IMDB ratings explained by the model.\n\n\nrsq(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.670\n\n\n\n\n\nAre models with high or low \\(R^2\\) more preferable?"
  },
  {
    "objectID": "slides/lec-13.html#rmse",
    "href": "slides/lec-13.html#rmse",
    "title": "Feature engineering",
    "section": "RMSE",
    "text": "RMSE\nAn alternative model performance statistic: root mean square error.\n\\[ RMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}} \\]\n\n\nrmse(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.302\n\n\n\n\n\nAre models with high or low RMSE are more preferable?"
  },
  {
    "objectID": "slides/lec-13.html#interpreting-rmse",
    "href": "slides/lec-13.html#interpreting-rmse",
    "title": "Feature engineering",
    "section": "Interpreting RMSE",
    "text": "Interpreting RMSE\n\nIs this RMSE considered low or high?\n\n\nrmse(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.302\n\n\n\n\nDepends‚Ä¶\n\noffice_train %>%\n  summarise(min = min(imdb_rating), max = max(imdb_rating))\n\n# A tibble: 1 √ó 2\n    min   max\n  <dbl> <dbl>\n1   6.7   9.7"
  },
  {
    "objectID": "slides/lec-13.html#but-really",
    "href": "slides/lec-13.html#but-really",
    "title": "Feature engineering",
    "section": "But, really‚Ä¶",
    "text": "But, really‚Ä¶\nwho cares about predictions on training data?"
  },
  {
    "objectID": "slides/lec-13.html#make-predictions-for-testing-data",
    "href": "slides/lec-13.html#make-predictions-for-testing-data",
    "title": "Feature engineering",
    "section": "Make predictions for testing data",
    "text": "Make predictions for testing data\n\noffice_test_pred <- predict(office_fit, office_test) %>%\n  bind_cols(office_test %>% select(imdb_rating, title))\n\noffice_test_pred\n\n# A tibble: 47 √ó 3\n   .pred imdb_rating title              \n   <dbl>       <dbl> <chr>              \n 1  8.03         8.3 Diversity Day      \n 2  7.98         7.9 Health Care        \n 3  8.41         8.4 The Fire           \n 4  8.35         8.2 Halloween          \n 5  8.35         8.4 E-Mail Surveillance\n 6  8.68         9   The Injury         \n 7  8.32         7.9 The Carpet         \n 8  8.93         9.3 Casino Night       \n 9  8.80         8.9 Gay Witch Hunt     \n10  8.37         8.2 Initiation         \n# ‚Ä¶ with 37 more rows"
  },
  {
    "objectID": "slides/lec-13.html#evaluate-performance-for-testing-data",
    "href": "slides/lec-13.html#evaluate-performance-for-testing-data",
    "title": "Feature engineering",
    "section": "Evaluate performance for testing data",
    "text": "Evaluate performance for testing data\nRMSE of model fit to testing data\n\nrmse(office_test_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.411\n\n\nR-sq of model fit to testing data\n\nrsq(office_test_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.468"
  },
  {
    "objectID": "slides/lec-13.html#training-vs.-testing",
    "href": "slides/lec-13.html#training-vs.-testing",
    "title": "Feature engineering",
    "section": "Training vs.¬†testing",
    "text": "Training vs.¬†testing\n\n\n\n\n\n\n\n\n\n\n\n\nmetric\ntrain\ntest\ncomparison\n\n\n\n\nRMSE\n0.302\n0.411\nRMSE lower for training\n\n\nR-squared\n0.67\n0.468\nR-squared higher for training"
  },
  {
    "objectID": "slides/lec-13.html#evaluating-performance-on-training-data",
    "href": "slides/lec-13.html#evaluating-performance-on-training-data",
    "title": "Feature engineering",
    "section": "Evaluating performance on training data",
    "text": "Evaluating performance on training data\n\nThe training set does not have the capacity to be a good arbiter of performance.\nIt is not an independent piece of information; predicting the training set can only reflect what the model already knows.\nSuppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test.\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-12.html#announcements",
    "href": "slides/lec-12.html#announcements",
    "title": "Feature engineering",
    "section": "Announcements",
    "text": "Announcements\n\n\nCheck your grades for exam-1\nAE-5 released today but we may cover the content tomorrow\nOH today"
  },
  {
    "objectID": "slides/lec-12.html#topics",
    "href": "slides/lec-12.html#topics",
    "title": "Feature engineering",
    "section": "Topics",
    "text": "Topics\n\n\nReview: Training and testing splits\nFeature engineering with recipes\nWorkflows to bring together models and recipes\nRMSE and \\(R^2\\) for model evaluation\nCross validation"
  },
  {
    "objectID": "slides/lec-12.html#computational-setup",
    "href": "slides/lec-12.html#computational-setup",
    "title": "Feature engineering",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-12.html#the-office",
    "href": "slides/lec-12.html#the-office",
    "title": "Feature engineering",
    "section": "The Office",
    "text": "The Office"
  },
  {
    "objectID": "slides/lec-12.html#data",
    "href": "slides/lec-12.html#data",
    "title": "Feature engineering",
    "section": "Data",
    "text": "Data\nThe data come from data.world, by way of TidyTuesday\n\noffice_ratings <- read_csv(here::here(\"slides\", \"data/office_ratings.csv\"))\noffice_ratings\n\n# A tibble: 188 √ó 6\n   season episode title             imdb_rating total_votes air_date  \n    <dbl>   <dbl> <chr>                   <dbl>       <dbl> <date>    \n 1      1       1 Pilot                     7.6        3706 2005-03-24\n 2      1       2 Diversity Day             8.3        3566 2005-03-29\n 3      1       3 Health Care               7.9        2983 2005-04-05\n 4      1       4 The Alliance              8.1        2886 2005-04-12\n 5      1       5 Basketball                8.4        3179 2005-04-19\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26\n 7      2       1 The Dundies               8.7        3213 2005-09-20\n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 9      2       3 Office Olympics           8.4        2742 2005-10-04\n10      2       4 The Fire                  8.4        2713 2005-10-11\n# ‚Ä¶ with 178 more rows"
  },
  {
    "objectID": "slides/lec-12.html#imdb-ratings",
    "href": "slides/lec-12.html#imdb-ratings",
    "title": "Feature engineering",
    "section": "IMDB ratings",
    "text": "IMDB ratings"
  },
  {
    "objectID": "slides/lec-12.html#imdb-ratings-vs.-number-of-votes",
    "href": "slides/lec-12.html#imdb-ratings-vs.-number-of-votes",
    "title": "Feature engineering",
    "section": "IMDB ratings vs.¬†number of votes",
    "text": "IMDB ratings vs.¬†number of votes"
  },
  {
    "objectID": "slides/lec-12.html#outliers",
    "href": "slides/lec-12.html#outliers",
    "title": "Feature engineering",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/lec-12.html#rating-vs.-air-date",
    "href": "slides/lec-12.html#rating-vs.-air-date",
    "title": "Feature engineering",
    "section": "Rating vs.¬†air date",
    "text": "Rating vs.¬†air date"
  },
  {
    "objectID": "slides/lec-12.html#imdb-ratings-vs.-seasons",
    "href": "slides/lec-12.html#imdb-ratings-vs.-seasons",
    "title": "Feature engineering",
    "section": "IMDB ratings vs.¬†seasons",
    "text": "IMDB ratings vs.¬†seasons"
  },
  {
    "objectID": "slides/lec-12.html#train-test",
    "href": "slides/lec-12.html#train-test",
    "title": "Feature engineering",
    "section": "Train / test",
    "text": "Train / test\nStep 1: Create an initial split:\n\nset.seed(123)\noffice_split <- initial_split(office_ratings) # prop = 3/4 by default\n\n\n\nStep 2: Save training data\n\noffice_train <- training(office_split)\ndim(office_train)\n\n[1] 141   6\n\n\n\n\n\nStep 3: Save testing data\n\noffice_test  <- testing(office_split)\ndim(office_test)\n\n[1] 47  6"
  },
  {
    "objectID": "slides/lec-12.html#training-data",
    "href": "slides/lec-12.html#training-data",
    "title": "Feature engineering",
    "section": "Training data",
    "text": "Training data\n\noffice_train\n\n# A tibble: 141 √ó 6\n   season episode title               imdb_rating total_votes air_date  \n    <dbl>   <dbl> <chr>                     <dbl>       <dbl> <date>    \n 1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2      9      14 Vandalism                   7.6        1402 2013-01-31\n 3      2       8 Performance Review          8.2        2416 2005-11-15\n 4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5      3      22 Beach Games                 9.1        2783 2007-05-10\n 6      7       1 Nepotism                    8.4        1897 2010-09-23\n 7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9      9      18 Promos                      8          1445 2013-04-04\n10      8      12 Pool Party                  8          1612 2012-01-19\n# ‚Ä¶ with 131 more rows"
  },
  {
    "objectID": "slides/lec-12.html#feature-engineering",
    "href": "slides/lec-12.html#feature-engineering",
    "title": "Feature engineering",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nWe prefer simple models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)\nVariable selection (parsimony) + Variable representation (feature engineering)"
  },
  {
    "objectID": "slides/lec-12.html#feature-engineering-date",
    "href": "slides/lec-12.html#feature-engineering-date",
    "title": "Feature engineering",
    "section": "Feature engineering : date",
    "text": "Feature engineering : date\nPeriodic: month or weekday"
  },
  {
    "objectID": "slides/lec-12.html#feature-engineering-with-dplyr",
    "href": "slides/lec-12.html#feature-engineering-with-dplyr",
    "title": "Feature engineering",
    "section": "Feature engineering with dplyr",
    "text": "Feature engineering with dplyr\n\n\n\n\noffice_train %>%\n  mutate(\n    season = as_factor(season),\n    month = lubridate::month(air_date),\n    wday = lubridate::wday(air_date)\n  )\n\n# A tibble: 141 √ó 8\n  season episode title            imdb_rating total_votes air_date   month  wday\n  <fct>    <dbl> <chr>                  <dbl>       <dbl> <date>     <dbl> <dbl>\n1 8           18 Last Day in Flo‚Ä¶         7.8        1429 2012-03-08     3     5\n2 9           14 Vandalism                7.6        1402 2013-01-31     1     5\n3 2            8 Performance Rev‚Ä¶         8.2        2416 2005-11-15    11     3\n4 9            5 Here Comes Treb‚Ä¶         7.1        1515 2012-10-25    10     5\n5 3           22 Beach Games              9.1        2783 2007-05-10     5     5\n6 7            1 Nepotism                 8.4        1897 2010-09-23     9     5\n# ‚Ä¶ with 135 more rows\n\n\n\n\nCan you identify any potential problems with this approach?\n\nHolidays"
  },
  {
    "objectID": "slides/lec-12.html#modeling-workflow",
    "href": "slides/lec-12.html#modeling-workflow",
    "title": "Feature engineering",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/lec-12.html#initiate-a-recipe",
    "href": "slides/lec-12.html#initiate-a-recipe",
    "title": "Feature engineering",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\noffice_rec <- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloguing names and types of variables\n  )\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          5"
  },
  {
    "objectID": "slides/lec-12.html#step-1-alter-roles",
    "href": "slides/lec-12.html#step-1-alter-roles",
    "title": "Feature engineering",
    "section": "Step 1: Alter roles",
    "text": "Step 1: Alter roles\ntitle isn‚Äôt a predictor, but we might want to keep it around as an ID\n\noffice_rec <- office_rec %>%\n  update_role(title, new_role = \"ID\")\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4"
  },
  {
    "objectID": "slides/lec-12.html#step-2-add-features",
    "href": "slides/lec-12.html#step-2-add-features",
    "title": "Feature engineering",
    "section": "Step 2: Add features",
    "text": "Step 2: Add features\nNew features for day of week and month. step_date creates a specification of a recipe step that will convert date data into one or more factor or numeric variables.\n\noffice_rec <- office_rec %>%\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date"
  },
  {
    "objectID": "slides/lec-12.html#step-3-add-more-features",
    "href": "slides/lec-12.html#step-3-add-more-features",
    "title": "Feature engineering",
    "section": "Step 3: Add more features",
    "text": "Step 3: Add more features\nIdentify holidays in air_date, then remove air_date\n\noffice_rec <- office_rec %>%\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date"
  },
  {
    "objectID": "slides/lec-12.html#step-4-convert-numbers-to-factors",
    "href": "slides/lec-12.html#step-4-convert-numbers-to-factors",
    "title": "Feature engineering",
    "section": "Step 4: Convert numbers to factors",
    "text": "Step 4: Convert numbers to factors\nConvert season to factor\n\noffice_rec <- office_rec %>%\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season"
  },
  {
    "objectID": "slides/lec-12.html#step-5-make-dummy-variables",
    "href": "slides/lec-12.html#step-5-make-dummy-variables",
    "title": "Feature engineering",
    "section": "Step 5: Make dummy variables",
    "text": "Step 5: Make dummy variables\nConvert all nominal (categorical) predictors to factors\n\noffice_rec <- office_rec %>%\n  step_dummy(all_nominal_predictors())\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()"
  },
  {
    "objectID": "slides/lec-12.html#step-6-remove-zero-variance-pred.s",
    "href": "slides/lec-12.html#step-6-remove-zero-variance-pred.s",
    "title": "Feature engineering",
    "section": "Step 6: Remove zero variance pred.s",
    "text": "Step 6: Remove zero variance pred.s\nRemove all predictors that contain only a single value\n\noffice_rec <- office_rec %>%\n  step_zv(all_predictors())\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/lec-12.html#putting-it-altogether",
    "href": "slides/lec-12.html#putting-it-altogether",
    "title": "Feature engineering",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\noffice_rec <- recipe(imdb_rating ~ ., data = office_train) %>%\n  # make title's role ID\n  update_role(title, new_role = \"ID\") %>%\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) %>%\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) %>%\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) %>%\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) %>%\n  # remove zero variance predictors\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/lec-12.html#putting-it-altogether-1",
    "href": "slides/lec-12.html#putting-it-altogether-1",
    "title": "Feature engineering",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/lec-12.html#recap",
    "href": "slides/lec-12.html#recap",
    "title": "Feature engineering",
    "section": "Recap",
    "text": "Recap\n\n\nReview: Training and testing splits\nFeature engineering with recipes: update role - step -\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-13.html#hw-2-part-1",
    "href": "slides/lec-13.html#hw-2-part-1",
    "title": "Feature engineering",
    "section": "HW 2 Part 1",
    "text": "HW 2 Part 1"
  },
  {
    "objectID": "slides/lec-13.html#categorical-predictors",
    "href": "slides/lec-13.html#categorical-predictors",
    "title": "Feature engineering",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\nSet it to be factor as.factor\nFor variable only contains two levels, taking values 0 or 1. No actions needed. Can treat as continuous variable in R, since 1 unit increase equals to difference brought by level 1 compared to baseline level\nOkay to take correlation to depict association"
  },
  {
    "objectID": "slides/lec-13.html#high-correlation",
    "href": "slides/lec-13.html#high-correlation",
    "title": "Feature engineering",
    "section": "High correlation",
    "text": "High correlation\n\nHigh correlation between predictors: bad\nmulticollinearity: large variance -> unreasonable estimator\n\n\nset.seed(1)\nx=rnorm(100,1,1)\nepsilon=rnorm(100,0,0.01)\ny=2*x+epsilon\ndat=tibble(x=x,y=y)\nx1=0.8*x+rnorm(100,0.1,0.01)\ndat=dat%>%mutate(x1=x1)\nlm1=lm(y~x,data = dat)\nlm2=lm(y~x+x1,data=dat)\nlm3=lm(y~x1+x,data=dat)"
  },
  {
    "objectID": "slides/lec-13.html#multicollinearity",
    "href": "slides/lec-13.html#multicollinearity",
    "title": "Feature engineering",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nNo change on point estimator. (rotation)\nIncrease se\n\n\nsummary(lm1)\n\n\nCall:\nlm(formula = y ~ x, data = dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.018768 -0.006138 -0.001395  0.005394  0.023462 \n\nCoefficients:\n              Estimate Std. Error  t value Pr(>|t|)    \n(Intercept) -0.0003663  0.0015342   -0.239    0.812    \nx            1.9999894  0.0010773 1856.534   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.009628 on 98 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 3.447e+06 on 1 and 98 DF,  p-value: < 2.2e-16\n\nsummary(lm2)\n\n\nCall:\nlm(formula = y ~ x + x1, data = dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.018455 -0.006465 -0.001507  0.005328  0.023326 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.004224   0.009525   0.443    0.658    \nx            2.036700   0.075177  27.092   <2e-16 ***\nx1          -0.045876   0.093936  -0.488    0.626    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.009665 on 97 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 1.71e+06 on 2 and 97 DF,  p-value: < 2.2e-16\n\nsummary(lm3)\n\n\nCall:\nlm(formula = y ~ x1 + x, data = dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.018455 -0.006465 -0.001507  0.005328  0.023326 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.004224   0.009525   0.443    0.658    \nx1          -0.045876   0.093936  -0.488    0.626    \nx            2.036700   0.075177  27.092   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.009665 on 97 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 1.71e+06 on 2 and 97 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "slides/lec-13.html#interpretation-on-coefficient",
    "href": "slides/lec-13.html#interpretation-on-coefficient",
    "title": "Feature engineering",
    "section": "Interpretation on coefficient",
    "text": "Interpretation on coefficient\n\nwe expect = model predicts = \\(\\hat{y}\\)\non average = expectation of y\nhold other constant\n\nCold call since today ! Be prepared !"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\n\nDue dates:\n\nAE 6: Wed, June 1, 11:59pm ET\nLab 4 + AE 7: Fri, June 3, 11:59pm ET\nAE 8: Sun, June 5, 11:59pm ET\nExam 2: Mon, June 6, 11:59pm ET\n\nReleased:\n\nAE 6 + 7 + 8\nLab 4\nExam 2"
  },
  {
    "objectID": "weeks/week-4.html#prepare",
    "href": "weeks/week-4.html#prepare",
    "title": "Week 4",
    "section": "Prepare",
    "text": "Prepare\nüìñ Read Tidy Modeling in R Chp 10: Resampling for evaluating performance"
  },
  {
    "objectID": "weeks/week-4.html#participate",
    "href": "weeks/week-4.html#participate",
    "title": "Week 4",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Lecture 14 - MLR: Cross validation\nüñ•Ô∏è Lecture 16 - MLR: Inference\nüñ•Ô∏è Lecture 17 - MLR: Inference conditions + multicollinearity\nüñ•Ô∏è Lecture 15 - Exam 2 review"
  },
  {
    "objectID": "weeks/week-4.html#practice",
    "href": "weeks/week-4.html#practice",
    "title": "Week 4",
    "section": "Practice",
    "text": "Practice\nüìã Application Exercise 6 - The office - CV\nüìã Application Exercise 8 - Rail Trail\nüìã Application Exercise 7 - Exam 2 Review"
  },
  {
    "objectID": "weeks/week-4.html#perform",
    "href": "weeks/week-4.html#perform",
    "title": "Week 4",
    "section": "Perform",
    "text": "Perform\n‚å®Ô∏è Lab 4 - The Office, another look\n‚úÖ Exam 2\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "slides/lec-14.html#annoucement",
    "href": "slides/lec-14.html#annoucement",
    "title": "Cross validation",
    "section": "Annoucement",
    "text": "Annoucement\n\nClass observation on June 1 by Ed and June 8 by Ben\nCertificate in College Teaching (CCT program): Observation Requirement\nFeedbacks on final project topics ## Topics\n\n\n\nCross validation for model evaluation\nCross validation for model comparison"
  },
  {
    "objectID": "slides/lec-14.html#topics",
    "href": "slides/lec-14.html#topics",
    "title": "Cross validation",
    "section": "Topics",
    "text": "Topics\n\n\nCross validation for model evaluation\nCross validation for model comparison"
  },
  {
    "objectID": "slides/lec-14.html#computational-setup",
    "href": "slides/lec-14.html#computational-setup",
    "title": "Cross validation",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(schrute)"
  },
  {
    "objectID": "slides/lec-14.html#data-goal",
    "href": "slides/lec-14.html#data-goal",
    "title": "Cross validation",
    "section": "Data & goal",
    "text": "Data & goal\n\n\nData: The data come from the shrute package, and has been transformed using instructions from Lab 4(released on Wednesday)\nGoal: Predict imdb_rating from other variables in the dataset\n\n\n\noffice_episodes <- read_csv(here::here(\"slides\", \"data/office_episodes.csv\"))\noffice_episodes\n\n# A tibble: 186 √ó 14\n   season episode episode_name      imdb_rating total_votes air_date   lines_jim\n    <dbl>   <dbl> <chr>                   <dbl>       <dbl> <date>         <dbl>\n 1      1       1 Pilot                     7.6        3706 2005-03-24    0.157 \n 2      1       2 Diversity Day             8.3        3566 2005-03-29    0.123 \n 3      1       3 Health Care               7.9        2983 2005-04-05    0.172 \n 4      1       4 The Alliance              8.1        2886 2005-04-12    0.202 \n 5      1       5 Basketball                8.4        3179 2005-04-19    0.0913\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26    0.159 \n 7      2       1 The Dundies               8.7        3213 2005-09-20    0.125 \n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27    0.0565\n 9      2       3 Office Olympics           8.4        2742 2005-10-04    0.196 \n10      2       4 The Fire                  8.4        2713 2005-10-11    0.160 \n# ‚Ä¶ with 176 more rows, and 7 more variables: lines_pam <dbl>,\n#   lines_michael <dbl>, lines_dwight <dbl>, halloween <chr>, valentine <chr>,\n#   christmas <chr>, michael <chr>"
  },
  {
    "objectID": "slides/lec-14.html#split-data-into-training-and-testing",
    "href": "slides/lec-14.html#split-data-into-training-and-testing",
    "title": "Cross validation",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\n\nset.seed(123)\noffice_split <- initial_split(office_episodes)\noffice_train <- training(office_split)\noffice_test <- testing(office_split)"
  },
  {
    "objectID": "slides/lec-14.html#specify-model",
    "href": "slides/lec-14.html#specify-model",
    "title": "Cross validation",
    "section": "Specify model",
    "text": "Specify model\n\noffice_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\noffice_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-14.html#one-possible-recipe",
    "href": "slides/lec-14.html#one-possible-recipe",
    "title": "Cross validation",
    "section": "One possible recipe",
    "text": "One possible recipe\n\nCreate a recipe that uses the new variables we generated\nDenotes episode_name as an ID variable and doesn‚Äôt use air_date as a predictor\nCreate dummy variables for all nominal predictors\nRemove all zero variance predictors"
  },
  {
    "objectID": "slides/lec-14.html#create-recipe",
    "href": "slides/lec-14.html#create-recipe",
    "title": "Cross validation",
    "section": "Create recipe",
    "text": "Create recipe\n\noffice_rec1 <- recipe(imdb_rating ~ ., data = office_train) %>%\n  update_role(episode_name, new_role = \"id\") %>%\n  step_rm(air_date) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\noffice_rec1\n\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor         12\n\nOperations:\n\nVariables removed air_date\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/lec-14.html#create-workflow",
    "href": "slides/lec-14.html#create-workflow",
    "title": "Cross validation",
    "section": "Create workflow",
    "text": "Create workflow\n\noffice_wflow1 <- workflow() %>%\n  add_model(office_spec) %>%\n  add_recipe(office_rec1)\n\noffice_wflow1\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: linear_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n3 Recipe Steps\n\n‚Ä¢ step_rm()\n‚Ä¢ step_dummy()\n‚Ä¢ step_zv()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-14.html#build-model",
    "href": "slides/lec-14.html#build-model",
    "title": "Cross validation",
    "section": "Build model",
    "text": "Build model\n\n\nFit model to training data\nMake predictions on testing data\nEvaluate model\n\n\n\nData Splitting is random ! May vary from time to time.\nTry multiple times and take average to make the conclusion robust."
  },
  {
    "objectID": "slides/lec-14.html#spending-our-data",
    "href": "slides/lec-14.html#spending-our-data",
    "title": "Cross validation",
    "section": "Spending our data",
    "text": "Spending our data\n\nIdea of data spending: test set was recommended for performance evaluation.\nTraining data (Model fitting) + Test data (Model prediction)\nbefore using the test set: assure effectiveness of the model\nHow to decide on which final model to take to the test set\nTreat training set as your dataset do data splitting. Repeat and take average."
  },
  {
    "objectID": "slides/lec-14.html#resampling-for-model-assessment",
    "href": "slides/lec-14.html#resampling-for-model-assessment",
    "title": "Cross validation",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\nResampling is only conducted on the training set. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples:\n\nThe model is fit with the analysis set (training).\nThe model is evaluated with the assessment set(test)."
  },
  {
    "objectID": "slides/lec-14.html#resampling-for-model-assessment-1",
    "href": "slides/lec-14.html#resampling-for-model-assessment-1",
    "title": "Cross validation",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\n\n\nSource: Kuhn and Silge. Tidy modeling with R."
  },
  {
    "objectID": "slides/lec-14.html#analysis-and-assessment-sets",
    "href": "slides/lec-14.html#analysis-and-assessment-sets",
    "title": "Cross validation",
    "section": "Analysis and assessment sets",
    "text": "Analysis and assessment sets\n\nAnalysis set is analogous to training set.\nAssessment set is analogous to test set.\nThe terms analysis and assessment avoids confusion with initial split of the data.\nThese data sets are mutually exclusive."
  },
  {
    "objectID": "slides/lec-14.html#cross-validation-1",
    "href": "slides/lec-14.html#cross-validation-1",
    "title": "Cross validation",
    "section": "Cross validation",
    "text": "Cross validation\nMore specifically, v-fold cross validation ‚Äì commonly used resampling technique:\n\nRandomly split your training data into v partitions\nUse 1 partition for assessment, and the remaining v-1 partitions for analysis\nRepeat v times, updating which partition is used for assessment each time\n\n\nLet‚Äôs give an example where v = 3‚Ä¶"
  },
  {
    "objectID": "slides/lec-14.html#cross-validation-step-1",
    "href": "slides/lec-14.html#cross-validation-step-1",
    "title": "Cross validation",
    "section": "Cross validation, step 1",
    "text": "Cross validation, step 1\nRandomly split your training data into 3 partitions:"
  },
  {
    "objectID": "slides/lec-14.html#split-data",
    "href": "slides/lec-14.html#split-data",
    "title": "Cross validation",
    "section": "Split data",
    "text": "Split data\n\nset.seed(345)\nfolds <- vfold_cv(office_train, v = 3)\nfolds\n\n#  3-fold cross-validation \n# A tibble: 3 √ó 2\n  splits          id   \n  <list>          <chr>\n1 <split [92/47]> Fold1\n2 <split [93/46]> Fold2\n3 <split [93/46]> Fold3"
  },
  {
    "objectID": "slides/lec-14.html#cross-validation-steps-2-and-3",
    "href": "slides/lec-14.html#cross-validation-steps-2-and-3",
    "title": "Cross validation",
    "section": "Cross validation, steps 2 and 3",
    "text": "Cross validation, steps 2 and 3\n\n\nUse 1 partition for assessment, and the remaining v-1 partitions for analysis\nRepeat v times, updating which partition is used for assessment each time"
  },
  {
    "objectID": "slides/lec-14.html#fit-resamples",
    "href": "slides/lec-14.html#fit-resamples",
    "title": "Cross validation",
    "section": "Fit resamples",
    "text": "Fit resamples\n\nset.seed(456)\n\noffice_fit_rs1 <- office_wflow1 %>%\n  fit_resamples(folds)\n\noffice_fit_rs1\n\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 √ó 4\n  splits          id    .metrics         .notes          \n  <list>          <chr> <list>           <list>          \n1 <split [92/47]> Fold1 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n2 <split [93/46]> Fold2 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n3 <split [93/46]> Fold3 <tibble [2 √ó 4]> <tibble [0 √ó 3]>"
  },
  {
    "objectID": "slides/lec-14.html#cross-validation-now-what",
    "href": "slides/lec-14.html#cross-validation-now-what",
    "title": "Cross validation",
    "section": "Cross validation, now what?",
    "text": "Cross validation, now what?\n\nWe‚Äôve fit a bunch of models\nNow it‚Äôs time to use them to collect metrics (e.g., R-squared, RMSE) on each model and use them to evaluate model fit and how it varies across folds"
  },
  {
    "objectID": "slides/lec-14.html#collect-cv-metrics",
    "href": "slides/lec-14.html#collect-cv-metrics",
    "title": "Cross validation",
    "section": "Collect CV metrics",
    "text": "Collect CV metrics\n\ncollect_metrics(office_fit_rs1)\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.351     3  0.0111 Preprocessor1_Model1\n2 rsq     standard   0.546     3  0.0378 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/lec-14.html#deeper-look-into-cv-metrics",
    "href": "slides/lec-14.html#deeper-look-into-cv-metrics",
    "title": "Cross validation",
    "section": "Deeper look into CV metrics",
    "text": "Deeper look into CV metrics\n\ncv_metrics1 <- collect_metrics(office_fit_rs1, summarize = FALSE) \n\ncv_metrics1\n\n# A tibble: 6 √ó 5\n  id    .metric .estimator .estimate .config             \n  <chr> <chr>   <chr>          <dbl> <chr>               \n1 Fold1 rmse    standard       0.356 Preprocessor1_Model1\n2 Fold1 rsq     standard       0.520 Preprocessor1_Model1\n3 Fold2 rmse    standard       0.367 Preprocessor1_Model1\n4 Fold2 rsq     standard       0.498 Preprocessor1_Model1\n5 Fold3 rmse    standard       0.330 Preprocessor1_Model1\n6 Fold3 rsq     standard       0.621 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/lec-14.html#better-tabulation-of-cv-metrics",
    "href": "slides/lec-14.html#better-tabulation-of-cv-metrics",
    "title": "Cross validation",
    "section": "Better tabulation of CV metrics",
    "text": "Better tabulation of CV metrics\n\ncv_metrics1 %>%\n  mutate(.estimate = round(.estimate, 3)) %>%\n  pivot_wider(id_cols = id, names_from = .metric, values_from = .estimate) %>%\n  kable(col.names = c(\"Fold\", \"RMSE\", \"R-squared\"))\n\n\n\n\nFold\nRMSE\nR-squared\n\n\n\n\nFold1\n0.356\n0.520\n\n\nFold2\n0.367\n0.498\n\n\nFold3\n0.330\n0.621"
  },
  {
    "objectID": "slides/lec-14.html#how-does-rmse-compare-to-y",
    "href": "slides/lec-14.html#how-does-rmse-compare-to-y",
    "title": "Cross validation",
    "section": "How does RMSE compare to y?",
    "text": "How does RMSE compare to y?\nCross validation RMSE stats:\n\ncv_metrics1 %>%\n  filter(.metric == \"rmse\") %>%\n  summarise(\n    min = min(.estimate),\n    max = max(.estimate),\n    mean = mean(.estimate),\n    sd = sd(.estimate)\n  )\n\n# A tibble: 1 √ó 4\n    min   max  mean     sd\n  <dbl> <dbl> <dbl>  <dbl>\n1 0.330 0.367 0.351 0.0192\n\n\nTraining data IMDB score stats:\n\noffice_episodes %>%\n  summarise(\n    min = min(imdb_rating),\n    max = max(imdb_rating),\n    mean = mean(imdb_rating),\n    sd = sd(imdb_rating)\n  )\n\n# A tibble: 1 √ó 4\n    min   max  mean    sd\n  <dbl> <dbl> <dbl> <dbl>\n1   6.7   9.7  8.25 0.535"
  },
  {
    "objectID": "slides/lec-14.html#cross-validation-jargon",
    "href": "slides/lec-14.html#cross-validation-jargon",
    "title": "Cross validation",
    "section": "Cross validation jargon",
    "text": "Cross validation jargon\n\nReferred to as v-fold or k-fold cross validation\nAlso commonly abbreviated as CV"
  },
  {
    "objectID": "slides/lec-14.html#cross-validation-for-reals",
    "href": "slides/lec-14.html#cross-validation-for-reals",
    "title": "Cross validation",
    "section": "Cross validation, for reals",
    "text": "Cross validation, for reals\n\nTo illustrate how CV works, we used v = 3:\n\n\nAnalysis sets are 2/3 of the training set\nEach assessment set is a distinct 1/3\nThe final resampling estimate of performance averages each of the 3 replicates\n\n\nThis was useful for illustrative purposes, but v = 3 is a poor choice in practice\nValues of v are most often 5 or 10; we generally prefer 10-fold cross-validation as a default"
  },
  {
    "objectID": "slides/lec-14.html#recap",
    "href": "slides/lec-14.html#recap",
    "title": "Cross validation",
    "section": "Recap",
    "text": "Recap\n\n\nCross validation for model evaluation\nCross validation for model comparison\n\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-16.html#topics",
    "href": "slides/lec-16.html#topics",
    "title": "MLR: Inference",
    "section": "Topics",
    "text": "Topics\n\nConduct a hypothesis test for \\(\\beta_j\\)\nCalculate a confidence interval for \\(\\beta_j\\)\nInference pitfalls"
  },
  {
    "objectID": "slides/lec-16.html#computational-setup",
    "href": "slides/lec-16.html#computational-setup",
    "title": "MLR: Inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)      # for tables\nlibrary(patchwork)  # for laying out plots\nlibrary(rms)        # for vif\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-16.html#data-rail_trail",
    "href": "slides/lec-16.html#data-rail_trail",
    "title": "MLR: Inference",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\nrail_trail <- read_csv(here::here(\"slides\", \"data/rail_trail.csv\"))\nrail_trail\n\n# A tibble: 90 √ó 7\n   volume hightemp avgtemp season cloudcover precip day_type\n    <dbl>    <dbl>   <dbl> <chr>       <dbl>  <dbl> <chr>   \n 1    501       83    66.5 Summer       7.60 0      Weekday \n 2    419       73    61   Summer       6.30 0.290  Weekday \n 3    397       74    63   Spring       7.5  0.320  Weekday \n 4    385       95    78   Summer       2.60 0      Weekend \n 5    200       44    48   Spring      10    0.140  Weekday \n 6    375       69    61.5 Spring       6.60 0.0200 Weekday \n 7    417       66    52.5 Spring       2.40 0      Weekday \n 8    629       66    52   Spring       0    0      Weekend \n 9    533       80    67.5 Summer       3.80 0      Weekend \n10    547       79    62   Summer       4.10 0      Weekday \n# ‚Ä¶ with 80 more rows\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/lec-16.html#variables",
    "href": "slides/lec-16.html#variables",
    "title": "MLR: Inference",
    "section": "Variables",
    "text": "Variables\nOutcome:\nvolume estimated number of trail users that day (number of breaks recorded)\n\nPredictors\n\n\nhightemp daily high temperature (in degrees Fahrenheit)\navgtemp average of daily low and daily high temperature (in degrees Fahrenheit)\nseason one of ‚ÄúFall‚Äù, ‚ÄúSpring‚Äù, or ‚ÄúSummer‚Äù\ncloudcover measure of cloud cover (in oktas)\nprecip measure of precipitation (in inches)\nday_type one of ‚Äúweekday‚Äù or ‚Äúweekend‚Äù"
  },
  {
    "objectID": "slides/lec-16.html#review-simple-linear-regression-slr",
    "href": "slides/lec-16.html#review-simple-linear-regression-slr",
    "title": "MLR: Inference",
    "section": "Review: Simple linear regression (SLR)",
    "text": "Review: Simple linear regression (SLR)\n\nggplot(rail_trail, aes(x = hightemp, y = volume)) + \n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"High temp (F)\", y = \"Number of riders\")"
  },
  {
    "objectID": "slides/lec-16.html#slr-model-summary",
    "href": "slides/lec-16.html#slr-model-summary",
    "title": "MLR: Inference",
    "section": "SLR model summary",
    "text": "SLR model summary\n\nrt_slr_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(volume ~ hightemp, data = rail_trail)\n\ntidy(rt_slr_fit)\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)   -17.1     59.4      -0.288 0.774        \n2 hightemp        5.70     0.848     6.72  0.00000000171"
  },
  {
    "objectID": "slides/lec-16.html#slr-hypothesis-test",
    "href": "slides/lec-16.html#slr-hypothesis-test",
    "title": "MLR: Inference",
    "section": "SLR hypothesis test",
    "text": "SLR hypothesis test\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)   -17.1     59.4      -0.288 0.774        \n2 hightemp        5.70     0.848     6.72  0.00000000171\n\n\n\nSet hypotheses: \\(H_0: \\beta_1 = 0\\) and \\(H_A: \\beta_1 \\ne 0\\)\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t = 6.72\\) with a degrees of freedom of 88, and a p-value < 0.0001.\n\n\n\n\nState the conclusion: With a small p-value, we reject \\(H_0\\). The data provide strong evidence that high temperature is a helpful predictor for the number of daily riders."
  },
  {
    "objectID": "slides/lec-16.html#multiple-linear-regression",
    "href": "slides/lec-16.html#multiple-linear-regression",
    "title": "MLR: Inference",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nrt_mlr_main_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit)\n\n# A tibble: 4 √ó 5\n  term         estimate std.error statistic       p.value\n  <chr>           <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)   -125.       71.7     -1.75  0.0841       \n2 hightemp         7.54      1.17     6.43  0.00000000692\n3 seasonSpring     5.13     34.3      0.150 0.881        \n4 seasonSummer   -76.8      47.7     -1.61  0.111"
  },
  {
    "objectID": "slides/lec-16.html#mlr-hypothesis-test-hightemp",
    "href": "slides/lec-16.html#mlr-hypothesis-test-hightemp",
    "title": "MLR: Inference",
    "section": "MLR hypothesis test: hightemp",
    "text": "MLR hypothesis test: hightemp\n\nSet hypotheses: \\(H_0: \\beta_{hightemp} = 0\\) and \\(H_A: \\beta_{hightemp} \\ne 0\\), given season is in the model\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t = 6.43\\) with a degrees of freedom of 86, and a p-value < 0.0001.\n\n\n\n\nState the conclusion: With such a small p-value, the data provides strong evidence against \\(H_0\\), i.e., the data provide strong evidence that high temperature for the day is a helpful predictor in a model given season is in the model"
  },
  {
    "objectID": "slides/lec-16.html#the-model-for-season-spring",
    "href": "slides/lec-16.html#the-model-for-season-spring",
    "title": "MLR: Inference",
    "section": "The model for season = Spring",
    "text": "The model for season = Spring\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 1 - 76.84 \\times 0 \\\\\n&= -120.10 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-16.html#the-model-for-season-summer",
    "href": "slides/lec-16.html#the-model-for-season-summer",
    "title": "MLR: Inference",
    "section": "The model for season = Summer",
    "text": "The model for season = Summer\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 1 \\\\\n&= -202.07 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-16.html#the-model-for-season-fall",
    "href": "slides/lec-16.html#the-model-for-season-fall",
    "title": "MLR: Inference",
    "section": "The model for season = Fall",
    "text": "The model for season = Fall\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 0 \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-16.html#the-models",
    "href": "slides/lec-16.html#the-models",
    "title": "MLR: Inference",
    "section": "The models",
    "text": "The models\nSame slope, different intercepts\n\nseason = Spring: \\(-120.10 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Summer: \\(-202.07 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Fall: \\(-125.23 + 7.54 \\times \\texttt{hightemp}\\)"
  },
  {
    "objectID": "slides/lec-16.html#application-exercise",
    "href": "slides/lec-16.html#application-exercise",
    "title": "MLR: Inference",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã github.com/sta210-s22/ae-8-rail-trail\n\n\n\n\nEx 1. Recreate the following visualization in R based on the results of the model."
  },
  {
    "objectID": "slides/lec-16.html#application-exercise-1",
    "href": "slides/lec-16.html#application-exercise-1",
    "title": "MLR: Inference",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã ae-8-rail-trail\n\n\nEx 2. Add an interaction effect between hightemp and season and comment on the significance of the interaction predictors. Time permitting, visualize the interaction model as well."
  },
  {
    "objectID": "slides/lec-16.html#confidence-interval-for-beta_j-1",
    "href": "slides/lec-16.html#confidence-interval-for-beta_j-1",
    "title": "MLR: Inference",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\nThe \\(C%\\) confidence interval for \\(\\beta_j\\) \\[\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)\\] where \\(t^*\\) follows a \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom.\nIn context, we are \\(C%\\) confident that for every one unit increase in \\(x_j\\), we expect \\(y\\) to change by LB to UB units, on average, holding all else constant.\nexpect + on average + hold all else constant\np: # of predictors in the model w/ dummy variables"
  },
  {
    "objectID": "slides/lec-16.html#confidence-interval-for-beta_j-2",
    "href": "slides/lec-16.html#confidence-interval-for-beta_j-2",
    "title": "MLR: Inference",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\ntidy(rt_mlr_main_fit, conf.int = TRUE)\n\n# A tibble: 4 √ó 7\n  term         estimate std.error statistic       p.value conf.low conf.high\n  <chr>           <dbl>     <dbl>     <dbl>         <dbl>    <dbl>     <dbl>\n1 (Intercept)   -125.       71.7     -1.75  0.0841         -268.       17.2 \n2 hightemp         7.54      1.17     6.43  0.00000000692     5.21      9.87\n3 seasonSpring     5.13     34.3      0.150 0.881           -63.1      73.4 \n4 seasonSummer   -76.8      47.7     -1.61  0.111          -172.       18.0"
  },
  {
    "objectID": "slides/lec-16.html#ci-for-hightemp",
    "href": "slides/lec-16.html#ci-for-hightemp",
    "title": "MLR: Inference",
    "section": "CI for hightemp",
    "text": "CI for hightemp\n\n\n# A tibble: 4 √ó 7\n  term         estimate std.error statistic       p.value conf.low conf.high\n  <chr>           <dbl>     <dbl>     <dbl>         <dbl>    <dbl>     <dbl>\n1 (Intercept)   -125.       71.7     -1.75  0.0841         -268.       17.2 \n2 hightemp         7.54      1.17     6.43  0.00000000692     5.21      9.87\n3 seasonSpring     5.13     34.3      0.150 0.881           -63.1      73.4 \n4 seasonSummer   -76.8      47.7     -1.61  0.111          -172.       18.0 \n\n\n\nWe are 95% confident that for every degrees Fahrenheit the day is warmer, we expect the number of riders to increase by 5.21 to 9.87, on average, holding season constant."
  },
  {
    "objectID": "slides/lec-16.html#ci-for-seasonspring",
    "href": "slides/lec-16.html#ci-for-seasonspring",
    "title": "MLR: Inference",
    "section": "CI for seasonSpring",
    "text": "CI for seasonSpring\n\n\n# A tibble: 4 √ó 7\n  term         estimate std.error statistic       p.value conf.low conf.high\n  <chr>           <dbl>     <dbl>     <dbl>         <dbl>    <dbl>     <dbl>\n1 (Intercept)   -125.       71.7     -1.75  0.0841         -268.       17.2 \n2 hightemp         7.54      1.17     6.43  0.00000000692     5.21      9.87\n3 seasonSpring     5.13     34.3      0.150 0.881           -63.1      73.4 \n4 seasonSummer   -76.8      47.7     -1.61  0.111          -172.       18.0 \n\n\n\nWe are 95% confident that the number of riders on a Spring day is expected to be lower by 63.1 to higher by 73.4 compared to a Fall day, on average, holding high temperature for the day constant."
  },
  {
    "objectID": "slides/lec-16.html#construct-ht-and-ci-based-on-simulation",
    "href": "slides/lec-16.html#construct-ht-and-ci-based-on-simulation",
    "title": "MLR: Inference",
    "section": "Construct HT and CI based on simulation",
    "text": "Construct HT and CI based on simulation\n\nCI: Bootstrap observations, fit model, obtain Bootstrap distribution\nHT: Permute observations, fit model, obtain dist of permuted samples.\nQ: Permute wrt single variable? wrt observations? diff permutations for diff variables?\nPermutation wrt observations! Hold other variables constant!"
  },
  {
    "objectID": "slides/lec-16.html#large-sample-sizes",
    "href": "slides/lec-16.html#large-sample-sizes",
    "title": "MLR: Inference",
    "section": "Large sample sizes",
    "text": "Large sample sizes\n\n\n\n\n\n\nDanger\n\n\nIf the sample size is large enough, the test will likely result in rejecting \\(H_0: \\beta_j = 0\\) even \\(x_j\\) has a very small effect on \\(y\\). (t-statistics increases)\n\n\nConsider the practical significance of the result not just the statistical significance.\nUse the confidence interval to draw conclusions instead of relying only p-values."
  },
  {
    "objectID": "slides/lec-16.html#small-sample-sizes",
    "href": "slides/lec-16.html#small-sample-sizes",
    "title": "MLR: Inference",
    "section": "Small sample sizes",
    "text": "Small sample sizes\n\n\n\n\n\n\nDanger\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0: \\beta_j=0\\).\n\n\nWhen you fail to reject the null hypothesis, DON‚ÄôT immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association."
  },
  {
    "objectID": "slides/lec-16.html#connections-between-ci-and-ht",
    "href": "slides/lec-16.html#connections-between-ci-and-ht",
    "title": "MLR: Inference",
    "section": "Connections between CI and HT",
    "text": "Connections between CI and HT\n\nInstead of checking p-values, we can use CI to do hypothesis testing.\nIf CI include 0, do not reject; Or else, reject \\(H_0\\)"
  },
  {
    "objectID": "slides/lec-16.html#notes-on-permutation-test",
    "href": "slides/lec-16.html#notes-on-permutation-test",
    "title": "MLR: Inference",
    "section": "Notes on permutation test",
    "text": "Notes on permutation test\n\nPermutation is sensitive to outliers and high-leverage points\nSmall repetitions may not be enough. (Bootstrap will be more representative)\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-17.html#topics",
    "href": "slides/lec-17.html#topics",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Topics",
    "text": "Topics\n\nConditions for inference\nMulticollinearity"
  },
  {
    "objectID": "slides/lec-17.html#computational-setup",
    "href": "slides/lec-17.html#computational-setup",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)      # for tables\nlibrary(patchwork)  # for laying out plots\nlibrary(rms)        # for vif\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-17.html#data-rail_trail",
    "href": "slides/lec-17.html#data-rail_trail",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\nrail_trail <- read_csv(here::here(\"slides\", \"data/rail_trail.csv\"))\nrail_trail\n\n# A tibble: 90 √ó 7\n   volume hightemp avgtemp season cloudcover precip day_type\n    <dbl>    <dbl>   <dbl> <chr>       <dbl>  <dbl> <chr>   \n 1    501       83    66.5 Summer       7.60 0      Weekday \n 2    419       73    61   Summer       6.30 0.290  Weekday \n 3    397       74    63   Spring       7.5  0.320  Weekday \n 4    385       95    78   Summer       2.60 0      Weekend \n 5    200       44    48   Spring      10    0.140  Weekday \n 6    375       69    61.5 Spring       6.60 0.0200 Weekday \n 7    417       66    52.5 Spring       2.40 0      Weekday \n 8    629       66    52   Spring       0    0      Weekend \n 9    533       80    67.5 Summer       3.80 0      Weekend \n10    547       79    62   Summer       4.10 0      Weekday \n# ‚Ä¶ with 80 more rows\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/lec-17.html#variables",
    "href": "slides/lec-17.html#variables",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Variables",
    "text": "Variables\nOutcome:\nvolume estimated number of trail users that day (number of breaks recorded)\nPredictors\n\n\nhightemp daily high temperature (in degrees Fahrenheit)\navgtemp average of daily low and daily high temperature (in degrees Fahrenheit)\nseason one of ‚ÄúFall‚Äù, ‚ÄúSpring‚Äù, or ‚ÄúSummer‚Äù\ncloudcover measure of cloud cover (in oktas)\nprecip measure of precipitation (in inches)\nday_type one of ‚Äúweekday‚Äù or ‚Äúweekend‚Äù"
  },
  {
    "objectID": "slides/lec-17.html#full-model",
    "href": "slides/lec-17.html#full-model",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Full model",
    "text": "Full model\nIncluding all available predictors\nFit:\n\nrt_full_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(volume ~ ., data = rail_trail)\n\n\nSummarize:\n\ntidy(rt_full_fit)\n\n# A tibble: 8 √ó 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\n\n\nAugment:\n\nrt_full_aug <- augment(rt_full_fit$fit)"
  },
  {
    "objectID": "slides/lec-17.html#model-conditions",
    "href": "slides/lec-17.html#model-conditions",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from each other."
  },
  {
    "objectID": "slides/lec-17.html#residuals-vs.-predicted-values",
    "href": "slides/lec-17.html#residuals-vs.-predicted-values",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Residuals vs.¬†predicted values",
    "text": "Residuals vs.¬†predicted values\n\nggplot(data = rt_full_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Predicted values\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/lec-17.html#linearity-residuals-vs.-predicted",
    "href": "slides/lec-17.html#linearity-residuals-vs.-predicted",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Linearity: Residuals vs.¬†predicted",
    "text": "Linearity: Residuals vs.¬†predicted\n\nDoes the linearity condition appear to be met?"
  },
  {
    "objectID": "slides/lec-17.html#linearity-residuals-vs.-predicted-1",
    "href": "slides/lec-17.html#linearity-residuals-vs.-predicted-1",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Linearity: Residuals vs.¬†predicted",
    "text": "Linearity: Residuals vs.¬†predicted\nIf there is some pattern in the plot of residuals vs.¬†predicted values, you can look at individual plots of residuals vs.¬†each predictor to try to identify the issue."
  },
  {
    "objectID": "slides/lec-17.html#linearity-residuals-vs.-each-predictor",
    "href": "slides/lec-17.html#linearity-residuals-vs.-each-predictor",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Linearity: Residuals vs.¬†each predictor",
    "text": "Linearity: Residuals vs.¬†each predictor"
  },
  {
    "objectID": "slides/lec-17.html#checking-linearity",
    "href": "slides/lec-17.html#checking-linearity",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nThe plot of residuals vs.¬†predicted shows a fan shaped pattern\nThe plots of residuals vs.¬†high and low temperature also shows a similar pattern and vs.¬†precipitation does not show a random scatter\nThe linearity condition is not satisfied."
  },
  {
    "objectID": "slides/lec-17.html#checking-constant-variance",
    "href": "slides/lec-17.html#checking-constant-variance",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nDoes the constant variance condition appear to be satisfied?"
  },
  {
    "objectID": "slides/lec-17.html#checking-constant-variance-1",
    "href": "slides/lec-17.html#checking-constant-variance-1",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nThe vertical spread of the residuals is not constant across the plot.\nThe constant variance condition is not satisfied."
  },
  {
    "objectID": "slides/lec-17.html#checking-normality",
    "href": "slides/lec-17.html#checking-normality",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking normality",
    "text": "Checking normality"
  },
  {
    "objectID": "slides/lec-17.html#overlaying-a-density-plot-on-a-histogram",
    "href": "slides/lec-17.html#overlaying-a-density-plot-on-a-histogram",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Overlaying a density plot on a histogram",
    "text": "Overlaying a density plot on a histogram\n\nOverlay density plot on a histogram +geom_density()\nOverlay a normal density plot on a histogram to compare whether normal assumption is reasonable (adjust bin width)\n\n\nggplot(rt_full_aug, aes(.resid)) +\n  geom_histogram(aes(y = after_stat(density)), binwidth = 50) +\n  stat_function(\n    fun = dnorm, \n    args = list(mean = mean(rt_full_aug$.resid), sd = sd(rt_full_aug$.resid)), \n    lwd = 2, \n    color = \"red\"\n  )"
  },
  {
    "objectID": "slides/lec-17.html#checking-independence",
    "href": "slides/lec-17.html#checking-independence",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can often check the independence condition based on the context of the data and how the observations were collected.\nIf the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected.\nIf there is a grouping variable lurking in the background, check the residuals based on that grouping variable."
  },
  {
    "objectID": "slides/lec-17.html#checking-independence-1",
    "href": "slides/lec-17.html#checking-independence-1",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs.¬†order of data collection:\n\nggplot(rt_full_aug, aes(y = .resid, x = 1:nrow(rt_full_aug))) +\n  geom_point() +\n  labs(x = \"Order of data collection\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/lec-17.html#checking-independence-2",
    "href": "slides/lec-17.html#checking-independence-2",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs.¬†predicted values by season: overlap a lot"
  },
  {
    "objectID": "slides/lec-17.html#checking-independence-3",
    "href": "slides/lec-17.html#checking-independence-3",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs.¬†predicted values by day_type: overlap a lot"
  },
  {
    "objectID": "slides/lec-17.html#checking-independence-4",
    "href": "slides/lec-17.html#checking-independence-4",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\nNo clear pattern in the residuals vs.¬†order of data collection plot and the model predicts similarly for seasons and day types. Independence condition appears to be satisfied, as far as we can evaluate it."
  },
  {
    "objectID": "slides/lec-17.html#why-multicollinearity-is-a-problem",
    "href": "slides/lec-17.html#why-multicollinearity-is-a-problem",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWe can‚Äôt include two variables that have a perfect linear association with each other\nMathematically, cannot find unique estimates for the model coefficients - - non-identifiability"
  },
  {
    "objectID": "slides/lec-17.html#example",
    "href": "slides/lec-17.html#example",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Example",
    "text": "Example\nSuppose the true population regression equation is \\(y = 3 + 4x\\)\n\nSuppose we try estimating that equation using a model with variables \\(x\\) and \\(z = x/10\\)\n\n\\[\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-17.html#example-1",
    "href": "slides/lec-17.html#example-1",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Example",
    "text": "Example\n\\[\\hat{y} = \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\\]\n\nWe can set \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) to any two numbers such that \\(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4\\)\nTherefore, we are unable to choose the ‚Äúbest‚Äù combination of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)"
  },
  {
    "objectID": "slides/lec-17.html#why-multicollinearity-is-a-problem-1",
    "href": "slides/lec-17.html#why-multicollinearity-is-a-problem-1",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWhen we have almost perfect collinearities (i.e.¬†highly correlated predictor variables), the standard errors for our regression coefficients inflate (uncertainty on identification)\nLose precision in our estimates of the regression coefficients\nImpedes model inference or prediction"
  },
  {
    "objectID": "slides/lec-17.html#detecting-multicollinearity",
    "href": "slides/lec-17.html#detecting-multicollinearity",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Detecting Multicollinearity",
    "text": "Detecting Multicollinearity\nMulticollinearity may occur when‚Ä¶\n\nThere are very high correlations \\((r > 0.9)\\) among two or more predictor variables, especially when the sample size is small\nOne (or more) predictor variables is an almost perfect linear combination of the others\nInclude a quadratic in the model mean-centering the variable first\nIncluding interactions between two or more continuous variables"
  },
  {
    "objectID": "slides/lec-17.html#detecting-multicollinearity-in-the-eda",
    "href": "slides/lec-17.html#detecting-multicollinearity-in-the-eda",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Detecting multicollinearity in the EDA",
    "text": "Detecting multicollinearity in the EDA\n\nLook at a correlation matrix of the predictor variables, including all indicator variables\n\nLook out for values close to 1 or -1\n\nLook at a scatterplot matrix of the predictor variables\n\nLook out for plots that show a relatively linear relationship\n\nLook at variables that have unreasonable coefficients (based on context)"
  },
  {
    "objectID": "slides/lec-17.html#detecting-multicollinearity-vif",
    "href": "slides/lec-17.html#detecting-multicollinearity-vif",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nVariance Inflation Factor (VIF): Measure of multicollinearity in the regression model\n\\[VIF(\\hat{\\beta}_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the proportion of variation \\(X\\) that is explained by the linear combination of the other explanatory variables in the model."
  },
  {
    "objectID": "slides/lec-17.html#detecting-multicollinearity-vif-1",
    "href": "slides/lec-17.html#detecting-multicollinearity-vif-1",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nTypically \\(VIF > 10\\) indicates concerning multicollinearity\n\nVariables with similar values of VIF are typically the ones correlated with each other\n\n\nUse the vif() function in the rms R package to calculate VIF"
  },
  {
    "objectID": "slides/lec-17.html#vif-for-sat-model",
    "href": "slides/lec-17.html#vif-for-sat-model",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "VIF For SAT Model",
    "text": "VIF For SAT Model\n\nvif(rt_full_fit$fit)\n\n       hightemp         avgtemp    seasonSpring    seasonSummer      cloudcover \n      10.259978       13.086175        2.751577        5.841985        1.587485 \n         precip day_typeWeekend \n       1.295352        1.125741 \n\n\n\nhightemp and avgtemp are correlated. We need to remove one of these variables and refit the model."
  },
  {
    "objectID": "slides/lec-17.html#model-without-hightemp",
    "href": "slides/lec-17.html#model-without-hightemp",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Model without hightemp",
    "text": "Model without hightemp\n\nm1 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(volume ~ . - hightemp, data = rail_trail)\n  \nm1 %>% tidy() %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n76.071\n77.204\n0.985\n0.327\n\n\navgtemp\n6.003\n1.583\n3.792\n0.000\n\n\nseasonSpring\n34.555\n34.454\n1.003\n0.319\n\n\nseasonSummer\n13.531\n55.024\n0.246\n0.806\n\n\ncloudcover\n-12.807\n3.488\n-3.672\n0.000\n\n\nprecip\n-110.736\n44.137\n-2.509\n0.014\n\n\nday_typeWeekend\n48.420\n22.993\n2.106\n0.038\n\n\n\n\nglance(m1) %>%\n  select(adj.r.squared, AIC, BIC)\n\n# A tibble: 1 √ó 3\n  adj.r.squared   AIC   BIC\n          <dbl> <dbl> <dbl>\n1         0.421 1088. 1108."
  },
  {
    "objectID": "slides/lec-17.html#model-without-avgtemp",
    "href": "slides/lec-17.html#model-without-avgtemp",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Model without avgtemp",
    "text": "Model without avgtemp\n\nm2 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(volume ~ . - avgtemp, data = rail_trail)\n  \nm2 %>% tidy() %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100\n\n\n\n\nglance(m2) %>%\n  select(adj.r.squared, AIC, BIC)\n\n# A tibble: 1 √ó 3\n  adj.r.squared   AIC   BIC\n          <dbl> <dbl> <dbl>\n1         0.473 1079. 1099."
  },
  {
    "objectID": "slides/lec-17.html#choosing-a-model",
    "href": "slides/lec-17.html#choosing-a-model",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Choosing a model",
    "text": "Choosing a model\nModel with hightemp removed:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.42\n1087.5\n1107.5\n\n\n\n\n\nModel with avgtemp removed:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.47\n1079.05\n1099.05\n\n\n\n\n\nBased on Adjusted \\(R^2\\), AIC, and BIC, the model with avgtemp removed is a better fit. Therefore, we choose to remove avgtemp from the model and leave hightemp in the model to deal with the multicollinearity."
  },
  {
    "objectID": "slides/lec-17.html#recap",
    "href": "slides/lec-17.html#recap",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Recap",
    "text": "Recap\n\nConditions for inference\nMulticollinearity\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-15.html#announcements",
    "href": "slides/lec-15.html#announcements",
    "title": "Exam 2 review",
    "section": "Announcements",
    "text": "Announcements\n\n\nExam 2 is released on Friday at 1pm, due at 11:59pm on Monday\nKeys for HW and labs posted"
  },
  {
    "objectID": "slides/lec-15.html#application-exercise",
    "href": "slides/lec-15.html#application-exercise",
    "title": "Exam 2 review",
    "section": "Application exercise",
    "text": "Application exercise\n\nüìã ae-7-exam-2-review\n\n\n\n\nyunranchen.github.io/STA210Summer/"
  },
  {
    "objectID": "slides/lec-14.html#review-on-workflow-of-building-a-model-1",
    "href": "slides/lec-14.html#review-on-workflow-of-building-a-model-1",
    "title": "Cross validation",
    "section": "Review on workflow of building a model",
    "text": "Review on workflow of building a model\n\nSpending data: Split data into training and test sets\nSpecify question: association between y and x(s)\nFeature engineering\nmodel fitting, condition check and evaluation\nmodel comparison\nmake inference (HT + CI)\nmodel prediction\nmake conclusion"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Yunran Chen (she/her) is a third-year Ph.D.¬†student in Department of Statistical Science at Duke University. She is interested in Bayesian factor model, nonparametric Bayes, and applications in neuroscience. Her goal is to serve as a bridge to connect various disciplines through developing statistical methods and models, and spread the concepts of statistics to the general public.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMondays 7:00 pm - 8:00 pm\nZoom\n\n\nThursdays 2:00 pm - 3:00 pm\nOld Chem 025\n\n\n\nIf these times don‚Äôt work for you or you‚Äôd like to schedule a one-on-one meeting, you can email me at yunran.chen@duke.edu."
  },
  {
    "objectID": "course-team.html#teaching-assistant",
    "href": "course-team.html#teaching-assistant",
    "title": "Teaching team",
    "section": "Teaching assistant",
    "text": "Teaching assistant\n\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\n\nJoseph Ekpenyong\nTuesdays and Thursdays 8:00pm - 9:00pm\n| Zoom\n\n\n\nJoseph will also responsible for gradings for homework and labs. If you have any questions about coursework and gradings, you can email Joseph at joseph.ekpenyong@duke.edu"
  },
  {
    "objectID": "slides/lec-16.html#annoucement",
    "href": "slides/lec-16.html#annoucement",
    "title": "MLR: Inference",
    "section": "Annoucement",
    "text": "Annoucement\n\nClass observation on June 1 by Ed and June 8 by Ben\nCertificate in College Teaching (CCT program): Observation Requirement"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html",
    "href": "ae/ae-8-rail-trail.html",
    "title": "AE 8: Rail Trail",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-8-rail-trail-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#packages-and-data",
    "href": "ae/ae-8-rail-trail.html#packages-and-data",
    "title": "AE 8: Rail Trail",
    "section": "Packages and data",
    "text": "Packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nrail_trail <- read_csv(\"data/rail_trail.csv\")"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-1",
    "href": "ae/ae-8-rail-trail.html#exercise-1",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a model predicting volume from hightemp and season.\n\nrt_mlr_main_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit)\n\n# A tibble: 4 √ó 5\n  term         estimate std.error statistic       p.value\n  <chr>           <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)   -125.       71.7     -1.75  0.0841       \n2 hightemp         7.54      1.17     6.43  0.00000000692\n3 seasonSpring     5.13     34.3      0.150 0.881        \n4 seasonSummer   -76.8      47.7     -1.61  0.111        \n\n\nRecreate the following visualization which displays the three regression lines we can draw based on the results of this model.\n\n\n\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-2",
    "href": "ae/ae-8-rail-trail.html#exercise-2",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 2",
    "text": "Exercise 2\nAdd an interaction effect between hightemp and season and comment on the significance of the interaction predictors. Time permitting, visualize the interaction model as well.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-3",
    "href": "ae/ae-8-rail-trail.html#exercise-3",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit a model predicting volume from all available predictors.\n\nrt_full_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(volume ~ ., data = rail_trail)\n\ntidy(rt_full_fit)\n\n# A tibble: 8 √ó 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\nRecreate the following visualization which displays a histogram of residuals (y-axis should be frequency) and a density curve overlaid."
  },
  {
    "objectID": "slides/lec-17.html#will-not-influence-the-point-estimate",
    "href": "slides/lec-17.html#will-not-influence-the-point-estimate",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Will not influence the point estimate",
    "text": "Will not influence the point estimate\nGeometric explanation for multiple regression (for model with 2 predictors):\n\nData points locate in a 3D space expanded by (y,x1,x2).\nMLR is a line go through the center of the data ‚Äúcloud‚Äù\nIf we slice the space with constant \\(x_1\\), we obtain projection of the regression line to the plane \\(x_2=c\\). The projection is a line with the slope \\(\\beta_1\\). So \\(\\beta_1\\) is interpreted as: if \\(x_1\\) increases by 1 unit, we expect \\(y\\) increases by \\(\\beta_1\\), on average, hold other constant.\nIf \\(x_1\\) and \\(x_2\\) linear related, it is like rotate the line on \\((y,x_1)\\) space to \\((y,x_1,x_2)\\) space. So add \\(x_2\\) will not change the point estimate of \\(\\beta_1\\) much. But due to the nonidentifiability, the se will inflate."
  }
]