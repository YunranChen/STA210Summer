---
title: "Feature engineering"
subtitle: "STA 210 - Summer 2022"
author: "Yunran Chen"
footer:  "[yunranchen.github.io/STA210Summer/](https://yunranchen.github.io/STA210Summer/)"
logo: "images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    incremental: true 
    chalkboard: true
editor: visual
execute:
  freeze: auto
---

```{r}
#| include: false

# figure options
knitr::opts_chunk$set(
  fig.width = 10, fig.asp = 0.618, out.width = "90%",
  fig.retina = 3, dpi = 300, fig.align = "center"
)
```

```{r}

# load packages
library(tidyverse)
library(tidymodels)
library(gghighlight)
library(knitr)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))
```

# Welcome

## Announcements

::: nonincremental
-   Check your grades for exam-1
-   Go through conceptual questions in homework 2
:::

## HW 2 Part 1

## Categorical predictors

-   Set it to be factor `as.factor`
-   For variable only contains two levels, taking values 0 or 1. No actions needed. Can treat as continuous variable in R, since 1 unit increase equals to difference brought by level 1 compared to baseline level
-   Okay to take correlation to depict association

## High correlation

-   High correlation between predictors: bad
-   multicollinearity:  large variance -> unreasonable estimator

```{r}
#| echo: true
set.seed(1)
x=rnorm(100,1,1)
epsilon=rnorm(100,0,0.01)
y=2*x+epsilon
dat=tibble(x=x,y=y)
x1=0.8*x+rnorm(100,0.1,0.01)
dat=dat%>%mutate(x1=x1)
lm1=lm(y~x,data = dat)
lm2=lm(y~x+x1,data=dat)
lm3=lm(y~x1+x,data=dat)

```

## Multicollinearity

-   No change on point estimator. (rotation)
-   Increase se

```{r}
#| echo: true

summary(lm1)
summary(lm2)
summary(lm3)
```

## Interpretation on coefficient

-   we expect = model predicts = $\hat{y}$ 
-   on average = expectation of y
-   hold other constant

Cold call since today ! Be prepared ! 

## Topics

::: nonincremental
[recipes](https://recipes.tidymodels.org/articles/recipes.html):

-   Splitting training set and testing set
-   Specify the `recipes` (predictors, outcome)
-   `steps` within `recipes` (feature engineering)
-   `Workflows` to bring together models and recipes
-   Cook: Fit model and prediction
-   Taste it : RMSE and $R^2$ for model evaluation
-   Improve it: Cross validation
:::


# Introduction

## The Office

![](images/lec-12/the-office.jpeg)

## Data

The data come from [data.world](https://data.world/anujjain7/the-office-imdb-ratings-dataset), by way of [TidyTuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-03-17/readme.md)

```{r}
#| echo: true

office_ratings <- read_csv(here::here("slides", "data/office_ratings.csv"))
office_ratings
```

## IMDB ratings

```{r}
ggplot(office_ratings, aes(x = imdb_rating)) +
  geom_histogram(binwidth = 0.25) +
  labs(
    title = "The Office ratings",
    x = "IMDB rating"
  )
```

## IMDB ratings vs. number of votes

```{r}
#| fig.asp: 0.5

office_ratings %>%
  mutate(season = as_factor(season)) %>%
  ggplot(aes(x = total_votes, y = imdb_rating, color = season)) +
  geom_jitter(alpha = 0.7) +
  labs(
    title = "The Office ratings",
    x = "Total votes",
    y = "IMDB rating",
    color = "Season"
  ) +
  theme(legend.position = c(0.9, 0.5)) +
  scale_color_viridis_d()
```

## Outliers

```{r}
#| fig.asp: 0.5

ggplot(office_ratings, aes(x = total_votes, y = imdb_rating)) +
  geom_jitter() +
  gghighlight(total_votes > 4000, label_key = title) +
  labs(
    title = "The Office ratings",
    x = "Total votes",
    y = "IMDB rating"
  )
```


## Rating vs. air date

```{r}
office_ratings %>%
  mutate(season = as_factor(season)) %>%
  ggplot(aes(x = air_date, y = imdb_rating, 
             color = season, size = total_votes)) +
  geom_point() +
  labs(x = "Air date", y = "IMDB rating",
       title = "The Office Ratings") +
  scale_color_viridis_d()
```

## IMDB ratings vs. seasons

```{r}
office_ratings %>%
  mutate(season = as_factor(season)) %>%
  ggplot(aes(x = season, y = imdb_rating, color = season)) +
  geom_boxplot() +
  geom_jitter() +
  guides(color = FALSE) +
  labs(
    title = "The Office ratings",
    x = "Season",
    y = "IMDB rating"
  ) +
  scale_color_viridis_d()
```

# Modeling

## Train / test

**Step 1:** Create an initial split:

```{r}
#| echo: true

set.seed(123)
office_split <- initial_split(office_ratings) # prop = 3/4 by default
```

. . .

<br>

**Step 2:** Save training data

```{r}
#| echo: true

office_train <- training(office_split)
dim(office_train)
```

. . .

<br>

**Step 3:** Save testing data

```{r}
#| echo: true

office_test  <- testing(office_split)
dim(office_test)
```

## Training data

```{r}
#| echo: true

office_train
```

## Feature engineering

-   We prefer simple models when possible, but **parsimony** does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity

-   Variables that go into the model and how they are represented are just as critical to success of the model

-   **Feature engineering** allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)

-   Variable selection (parsimony) + Variable representation (feature engineering)

## Feature engineering : date

Periodic: month or weekday

```{r}
office_ratings %>%
  mutate(season = as_factor(season)) %>%
  ggplot(aes(x = air_date, y = imdb_rating, 
             color = season, size = total_votes)) +
  geom_point() +
  labs(x = "Air date", y = "IMDB rating",
       title = "The Office Ratings") +
  scale_color_viridis_d()
```

## Feature engineering with dplyr

```{r}
options(dplyr.print_max = 6, dplyr.print_min = 6)
```

```{r}
#| echo: true

office_train %>%
  mutate(
    season = as_factor(season),
    month = lubridate::month(air_date),
    wday = lubridate::wday(air_date)
  )
```

. . .

::: question
Can you identify any potential problems with this approach?
:::

Holidays

```{r}
options(dplyr.print_max = 10, dplyr.print_min = 10)
```

## Modeling workflow

-   Create a **recipe** for feature engineering steps to be applied to the training data

-   Fit the model to the training data after these steps have been applied

-   Using the model estimates from the training data, predict outcomes for the test data

-   Evaluate the performance of the model on the test data

# Building recipes

## Initiate a recipe

```{r}
#| echo: true
#| code-line-numbers: "|2|3"

office_rec <- recipe(
  imdb_rating ~ .,    # formula
  data = office_train # data for cataloguing names and types of variables
  )

office_rec
```

## Step 1: Alter roles

`title` isn't a predictor, but we might want to keep it around as an ID

```{r}
#| echo: true
#| code-line-numbers: "|2"

office_rec <- office_rec %>%
  update_role(title, new_role = "ID")

office_rec
```

## Step 2: Add features

New features for day of week and month. `step_date` creates a specification of a recipe step that will convert date data into one or more factor or numeric variables.

```{r}
#| echo: true
#| code-line-numbers: "|2"

office_rec <- office_rec %>%
  step_date(air_date, features = c("dow", "month"))

office_rec

```

## Step 3: Add more features {.smaller}

Identify holidays in `air_date`, then remove `air_date`

```{r}
#| echo: true
#| code-line-numbers: "|2,3,4,5,6"

office_rec <- office_rec %>%
  step_holiday(
    air_date, 
    holidays = c("USThanksgivingDay", "USChristmasDay", "USNewYearsDay", "USIndependenceDay"), 
    keep_original_cols = FALSE
  )

office_rec
```

## Step 4: Convert numbers to factors {.smaller}

Convert `season` to factor

```{r}
#| echo: true
#| code-line-numbers: "|2"

office_rec <- office_rec %>%
  step_num2factor(season, levels = as.character(1:9))

office_rec
```

## Step 5: Make dummy variables {.smaller}

Convert all nominal (categorical) predictors to factors

```{r}
#| echo: true
#| code-line-numbers: "|2"

office_rec <- office_rec %>%
  step_dummy(all_nominal_predictors())

office_rec
```

## Step 6: Remove zero variance pred.s {.smaller}

Remove all predictors that contain only a single value

```{r}
#| echo: true
#| code-line-numbers: "|2"

office_rec <- office_rec %>%
  step_zv(all_predictors())

office_rec
```

## Steps that may be useful

-   Impute data if there is missing data in predictors:

`step_impute_knn(all_predictors())`

-   If intercept is not meaningful and want to center predictors:

`step_center(all_numeric_predictors())`

-   Check other possible steps:  [recipes](https://recipes.tidymodels.org/articles/recipes.html)



## Putting it altogether {.smaller}

```{r}
#| label: recipe-altogether
#| echo: true
#| results: hide

office_rec <- recipe(imdb_rating ~ ., data = office_train) %>%
  # make title's role ID
  update_role(title, new_role = "ID") %>%
  # extract day of week and month of air_date
  step_date(air_date, features = c("dow", "month")) %>%
  # identify holidays and add indicators
  step_holiday(
    air_date, 
    holidays = c("USThanksgivingDay", "USChristmasDay", "USNewYearsDay", "USIndependenceDay"), 
    keep_original_cols = FALSE
  ) %>%
  # turn season into factor
  step_num2factor(season, levels = as.character(1:9)) %>%
  # make dummy variables
  step_dummy(all_nominal_predictors()) %>%
  # remove zero variance predictors
  step_zv(all_predictors())
```

## Putting it altogether

```{r}
#| echo: true

office_rec
```


# Building workflows

## Specify model

```{r}
#| echo: true

office_spec <- linear_reg() %>%
  set_engine("lm")

office_spec
```

## Build workflow

**Workflows** bring together models and recipes so that they can be easily applied to both the training and test data.

```{r}
#| echo: true

office_wflow <- workflow() %>%
  add_model(office_spec) %>%
  add_recipe(office_rec)
```


## View workflow

```{r}
#| echo: true

office_wflow
```

# Fit model 

## Fit model to training data {.smaller}

**Cold Call !**

```{r}
#| echo: true

office_fit <- office_wflow %>%
  fit(data = office_train)

tidy(office_fit)
```

<br>

. . .

*So many predictors!*

## Model fit summary

Print all the predictors w/o folding any 

```{r}
#| echo: true

tidy(office_fit) %>% print(n = 21)
```

# Evaluate model

## Make predictions for training data

```{r}
#| echo: true

office_train_pred <- predict(office_fit, office_train) %>%
  bind_cols(office_train %>% select(imdb_rating, title))

office_train_pred
```

## R-squared

Percentage of variability in the IMDB ratings explained by the model.

. . .

```{r}
#| echo: true

rsq(office_train_pred, truth = imdb_rating, estimate = .pred)
```

. . .

::: question
Are models with high or low $R^2$ more preferable?
:::

## RMSE

An alternative model performance statistic: **root mean square error**.

$$ RMSE = \sqrt{\frac{\sum_{i = 1}^n (y_i - \hat{y}_i)^2}{n}} $$

. . .

```{r}
#| label: rmse-train
#| echo: true

rmse(office_train_pred, truth = imdb_rating, estimate = .pred)
```

. . .

::: question
Are models with high or low RMSE are more preferable?
:::

## Interpreting RMSE

::: question
Is this RMSE considered low or high?
:::

```{r}
#| ref.label: "rmse-train"
#| echo: true
```

<br>

. . .

Depends...

```{r}
#| echo: true

office_train %>%
  summarise(min = min(imdb_rating), max = max(imdb_rating))
```

## But, really...

*who cares about predictions on **training** data?*

## Make predictions for testing data

```{r}
#| echo: true

office_test_pred <- predict(office_fit, office_test) %>%
  bind_cols(office_test %>% select(imdb_rating, title))

office_test_pred
```

## Evaluate performance for testing data

RMSE of model fit to **testing** data

```{r}
#| echo: true

rmse(office_test_pred, truth = imdb_rating, estimate = .pred)
```

R-sq of model fit to **testing** data

```{r}
#| echo: true

rsq(office_test_pred, truth = imdb_rating, estimate = .pred)
```

## Training vs. testing

```{r}
rmse_train <- rmse(office_train_pred, truth = imdb_rating, estimate = .pred) %>%
  pull(.estimate) %>%
  round(3)

rsq_train <- rsq(office_train_pred, truth = imdb_rating, estimate = .pred) %>%
  pull(.estimate) %>%
  round(3)

rmse_test <- rmse(office_test_pred, truth = imdb_rating, estimate = .pred) %>%
  pull(.estimate) %>%
  round(3)

rsq_test <- rsq(office_test_pred, truth = imdb_rating, estimate = .pred) %>%
  pull(.estimate) %>%
  round(3)
```

| metric    |          train |          test | comparison                    |
|:----------|---------------:|--------------:|:------------------------------|
| RMSE      | `r rmse_train` | `r rmse_test` | RMSE lower for training       |
| R-squared |  `r rsq_train` |  `r rsq_test` | R-squared higher for training |

## Evaluating performance on training data {.smaller}

-   The training set does not have the capacity to be a good arbiter of performance.

-   It is not an independent piece of information; predicting the training set can only reflect what the model already knows.

-   Suppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test.
