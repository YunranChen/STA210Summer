---
title: "Cross validation"
subtitle: "STA 210 - Summer 2022"
author: "Yunran Chen"
footer:  "[yunranchen.github.io/STA210Summer/](https://yunranchen.github.io/STA210Summer/)"
logo: "images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    incremental: true 
    chalkboard: true
editor: visual
execute:
  freeze: auto
  echo: true
---

```{r}
#| include: false

# figure options
knitr::opts_chunk$set(
  fig.width = 10, fig.asp = 0.618, out.width = "90%",
  fig.retina = 3, dpi = 300, fig.align = "center"
)
```

# Welcome

## Annoucement

-   Class observation on June 1 by Ed and June 8 by Ben
-   Certificate in College Teaching ([CCT program](https://gradschool.duke.edu/professional-development/programs/certificate-college-teaching)): [Observation Requirement](https://gradschool.duke.edu/professional-development/programs/certificate-college-teaching/teaching-and-observation-requirements)
-   Feedbacks on final project topics
## Topics

::: nonincremental
-   Cross validation for model evaluation
-   Cross validation for model comparison
:::

## Computational setup

```{r}
#| echo: true

# load packages
library(tidyverse)
library(tidymodels)
library(knitr)
library(schrute)
```

## Data & goal {.smaller}

::: nonincremental
-   Data: The data come from the [**shrute**](https://bradlindblad.github.io/schrute/) package, and has been transformed using instructions from [Lab 4](/labs/lab-4.html)(released on Wednesday)
-   Goal: Predict `imdb_rating` from other variables in the dataset
:::

```{r}
#| echo: true

office_episodes <- read_csv(here::here("slides", "data/office_episodes.csv"))
office_episodes
```

# Review on workflow of building a model

## Review on workflow of building a model

-   Obtain data
-   Split data into training and test sets
-   Specify question: association between y and x(s)
-   Feature engineering 
-   model fitting
-   model comparison
-   condition check and evaluation
-   make inference (HT + CI)
-   model prediction
-   make conclusion

# Modeling prep

## Split data into training and testing

```{r}
set.seed(123)
office_split <- initial_split(office_episodes)
office_train <- training(office_split)
office_test <- testing(office_split)
```

## Specify model

```{r}
office_spec <- linear_reg() %>%
  set_engine("lm")

office_spec
```

# Model 1

## One possible recipe

-   Create a recipe that uses the new variables we generated
-   Denotes `episode_name` as an ID variable and doesn't use `air_date` as a predictor
-   Create dummy variables for all nominal predictors
-   Remove all zero variance predictors

## Create recipe

```{r}
office_rec1 <- recipe(imdb_rating ~ ., data = office_train) %>%
  update_role(episode_name, new_role = "id") %>%
  step_rm(air_date) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

office_rec1
```

## Create workflow

```{r}
office_wflow1 <- workflow() %>%
  add_model(office_spec) %>%
  add_recipe(office_rec1)

office_wflow1
```

## Build model

. . .

-   Fit model to training data
-   Make predictions on testing data
-   Evaluate model

. . .

Data Splitting is random ! May vary from time to time.

Try multiple times and take average to make the conclusion robust.

# Cross validation

## Spending our data

-   Idea of data spending: test set was recommended for performance evaluation.
-   Training data (Model fitting) + Test data (Model prediction)
-   *before using the test set*: assure effectiveness of the model
-   How to decide on *which* final model to take to the test set
-   Treat training set as your dataset do data splitting. Repeat and take average.

## Resampling for model assessment

**Resampling is only conducted on the training set**. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples:

-   The model is fit with the **analysis set** (training).
-   The model is evaluated with the **assessment set**(test).

## Resampling for model assessment

![](images/lec-14/resampling.svg){fig-align="center"}

<br>

Source: Kuhn and Silge. [Tidy modeling with R](https://www.tmwr.org/).

## Analysis and assessment sets

-   Analysis set is analogous to training set.
-   Assessment set is analogous to test set.
-   The terms *analysis* and *assessment* avoids confusion with initial split of the data.
-   These data sets are mutually exclusive.

## Cross validation

More specifically, **v-fold cross validation** -- commonly used resampling technique:

-   Randomly split your **training** **data** into v partitions
-   Use 1 partition for assessment, and the remaining v-1 partitions for analysis
-   Repeat v times, updating which partition is used for assessment each time

. . .

Let's give an example where `v = 3`...

## Cross validation, step 1

Randomly split your **training** **data** into 3 partitions:

<br>

![](images/lec-14/three-CV.svg){fig-align="center"}

## Split data

```{r}
#| echo: true

set.seed(345)
folds <- vfold_cv(office_train, v = 3)
folds
```

## Cross validation, steps 2 and 3

::: nonincremental
-   Use 1 partition for assessment, and the remaining v-1 partitions for analysis
-   Repeat v times, updating which partition is used for assessment each time
:::

![](images/lec-14/three-CV-iter.svg){fig-align="center"}

## Fit resamples

```{r}
#| echo: true

set.seed(456)

office_fit_rs1 <- office_wflow1 %>%
  fit_resamples(folds)

office_fit_rs1
```

## Cross validation, now what?

-   We've fit a bunch of models
-   Now it's time to use them to collect metrics (e.g., R-squared, RMSE) on each model and use them to evaluate model fit and how it varies across folds

## Collect CV metrics

```{r}
collect_metrics(office_fit_rs1)
```

## Deeper look into CV metrics

```{r}
cv_metrics1 <- collect_metrics(office_fit_rs1, summarize = FALSE) 

cv_metrics1
```

## Better tabulation of CV metrics

```{r}
cv_metrics1 %>%
  mutate(.estimate = round(.estimate, 3)) %>%
  pivot_wider(id_cols = id, names_from = .metric, values_from = .estimate) %>%
  kable(col.names = c("Fold", "RMSE", "R-squared"))
```

## How does RMSE compare to y? {.smaller}

Cross validation RMSE stats:

```{r}
cv_metrics1 %>%
  filter(.metric == "rmse") %>%
  summarise(
    min = min(.estimate),
    max = max(.estimate),
    mean = mean(.estimate),
    sd = sd(.estimate)
  )
```

Training data IMDB score stats:

```{r}
office_episodes %>%
  summarise(
    min = min(imdb_rating),
    max = max(imdb_rating),
    mean = mean(imdb_rating),
    sd = sd(imdb_rating)
  )
```

## Cross validation jargon

-   Referred to as v-fold or k-fold cross validation
-   Also commonly abbreviated as CV

## Cross validation, for reals

-   To illustrate how CV works, we used `v = 3`:

    ::: nonincremental
    -   Analysis sets are 2/3 of the training set
    -   Each assessment set is a distinct 1/3
    -   The final resampling estimate of performance averages each of the 3 replicates
    :::

-   This was useful for illustrative purposes, but `v = 3` is a poor choice in practice

-   Values of `v` are most often 5 or 10; we generally prefer 10-fold cross-validation as a default

# Application exercise

::: appex
ðŸ“‹ [ae-6-the-office-cv](https://yunranchen.github.io/STA210Summer/ae/ae-6-the-office-cv.html)
:::

## Recap

::: nonincremental
-   Cross validation for model evaluation
-   Cross validation for model comparison
:::
